26-Feb-25 13:30:58 - agent.DQN.DQN - INFO - device is cuda
26-Feb-25 13:30:58 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
26-Feb-25 13:30:59 - __main__ - INFO - Libraries imported
26-Feb-25 13:30:59 - __main__ - INFO - Device: cuda
26-Feb-25 13:30:59 - __main__ - INFO - Starting training of DQN2 agent
26-Feb-25 13:30:59 - __main__ - INFO - Environment initialized
26-Feb-25 13:30:59 - agent.DQN.DQN - INFO - QNetwork initialized with 10 observations and 9 actions
26-Feb-25 13:30:59 - agent.DQN.DQN - INFO - QNetwork initialized with 10 observations and 9 actions
26-Feb-25 13:30:59 - __main__ - INFO - Q-Networks initialized and synchronized
26-Feb-25 13:31:01 - __main__ - INFO - Optimizer, LR scheduler, and loss function initialized
26-Feb-25 13:31:01 - __main__ - INFO - Epsilon-greedy strategy initialized
26-Feb-25 13:31:01 - __main__ - INFO - Replay buffer initialized
26-Feb-25 13:31:01 - __main__ - INFO - Training DQN2 agent
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 128 reward tensor([-1], device='cuda:0') loss 700.323486328125 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 129 reward tensor([-1], device='cuda:0') loss 136.1041717529297 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 130 reward tensor([-1], device='cuda:0') loss 184.09609985351562 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 131 reward tensor([-1], device='cuda:0') loss 171.28256225585938 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 132 reward tensor([-1], device='cuda:0') loss 104.40234375 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 133 reward tensor([-1], device='cuda:0') loss 60.452754974365234 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 134 reward tensor([-1], device='cuda:0') loss 33.59992218017578 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 135 reward tensor([-1], device='cuda:0') loss 28.11156463623047 epsilon 0.82
26-Feb-25 13:31:02 - agent.DQN.DQN - INFO - episode 1 step 136 reward tensor([-1], device='cuda:0') loss 32.52388000488281 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 137 reward tensor([-1], device='cuda:0') loss 34.82038879394531 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 138 reward tensor([-1], device='cuda:0') loss 32.0045166015625 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 139 reward tensor([-1], device='cuda:0') loss 22.48996353149414 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 140 reward tensor([-1], device='cuda:0') loss 18.674274444580078 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 141 reward tensor([-1], device='cuda:0') loss 13.782665252685547 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 142 reward tensor([-1], device='cuda:0') loss 10.441534042358398 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 143 reward tensor([-1], device='cuda:0') loss 7.68147087097168 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 144 reward tensor([-1], device='cuda:0') loss 7.06318998336792 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 145 reward tensor([-1], device='cuda:0') loss 7.078463077545166 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 146 reward tensor([-1], device='cuda:0') loss 7.932999610900879 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 147 reward tensor([-1], device='cuda:0') loss 8.075895309448242 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 148 reward tensor([-1], device='cuda:0') loss 7.152827739715576 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 149 reward tensor([-1], device='cuda:0') loss 7.381847858428955 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 150 reward tensor([-1], device='cuda:0') loss 5.969348907470703 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 151 reward tensor([-1], device='cuda:0') loss 7.1575212478637695 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 152 reward tensor([-1], device='cuda:0') loss 6.645340442657471 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 153 reward tensor([-1], device='cuda:0') loss 5.862112998962402 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 154 reward tensor([-1], device='cuda:0') loss 5.259294509887695 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 155 reward tensor([-1], device='cuda:0') loss 4.200623989105225 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 156 reward tensor([-1], device='cuda:0') loss 3.9647293090820312 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 157 reward tensor([-1], device='cuda:0') loss 3.615097761154175 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 158 reward tensor([-1], device='cuda:0') loss 3.911938190460205 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 159 reward tensor([-1], device='cuda:0') loss 3.608818292617798 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 160 reward tensor([-1], device='cuda:0') loss 3.7781853675842285 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 161 reward tensor([-1], device='cuda:0') loss 3.2581048011779785 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 162 reward tensor([-1], device='cuda:0') loss 3.476229190826416 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 163 reward tensor([-1], device='cuda:0') loss 3.283640146255493 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 164 reward tensor([-1], device='cuda:0') loss 3.1808290481567383 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 165 reward tensor([-1], device='cuda:0') loss 2.3901805877685547 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 166 reward tensor([-1], device='cuda:0') loss 2.320039749145508 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 167 reward tensor([-1], device='cuda:0') loss 2.096644401550293 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 168 reward tensor([-1], device='cuda:0') loss 1.864485502243042 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 169 reward tensor([-1], device='cuda:0') loss 1.927964210510254 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 170 reward tensor([-1], device='cuda:0') loss 1.865178108215332 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 171 reward tensor([-1], device='cuda:0') loss 1.838193416595459 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 172 reward tensor([-1], device='cuda:0') loss 2.01863956451416 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 173 reward tensor([-1], device='cuda:0') loss 1.9910590648651123 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 174 reward tensor([-1], device='cuda:0') loss 1.9929661750793457 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 175 reward tensor([-1], device='cuda:0') loss 1.8649994134902954 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 176 reward tensor([-1], device='cuda:0') loss 1.5936161279678345 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 177 reward tensor([-1], device='cuda:0') loss 1.4908342361450195 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 178 reward tensor([-1], device='cuda:0') loss 1.4096064567565918 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 179 reward tensor([-1], device='cuda:0') loss 1.499576210975647 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 180 reward tensor([-1], device='cuda:0') loss 1.4922831058502197 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 181 reward tensor([-1], device='cuda:0') loss 2.3658690452575684 epsilon 0.82
26-Feb-25 13:31:03 - agent.DQN.DQN - INFO - episode 1 step 182 reward tensor([-1], device='cuda:0') loss 1.6852984428405762 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 183 reward tensor([-1], device='cuda:0') loss 1.1812512874603271 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 184 reward tensor([-1], device='cuda:0') loss 0.7779037952423096 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 185 reward tensor([-1], device='cuda:0') loss 0.8542954921722412 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 186 reward tensor([-1], device='cuda:0') loss 0.8379067778587341 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 187 reward tensor([-1], device='cuda:0') loss 1.0623016357421875 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 188 reward tensor([-1], device='cuda:0') loss 0.7581039071083069 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 189 reward tensor([-1], device='cuda:0') loss 0.7558245062828064 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 190 reward tensor([-1], device='cuda:0') loss 0.5022790431976318 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 191 reward tensor([-1], device='cuda:0') loss 0.44960302114486694 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 192 reward tensor([-1], device='cuda:0') loss 0.5093472599983215 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 193 reward tensor([-1], device='cuda:0') loss 0.45902031660079956 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 194 reward tensor([-1], device='cuda:0') loss 0.5330265760421753 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 195 reward tensor([-1], device='cuda:0') loss 0.5657249093055725 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 196 reward tensor([-1], device='cuda:0') loss 0.42548882961273193 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 197 reward tensor([-1], device='cuda:0') loss 0.49724912643432617 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 198 reward tensor([-1], device='cuda:0') loss 0.4669816195964813 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 199 reward tensor([-1], device='cuda:0') loss 0.3851682245731354 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 200 reward tensor([-1], device='cuda:0') loss 0.336081862449646 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 201 reward tensor([-1], device='cuda:0') loss 0.3005818724632263 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 202 reward tensor([-1], device='cuda:0') loss 0.3503350615501404 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 203 reward tensor([-1], device='cuda:0') loss 0.2845481038093567 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 204 reward tensor([-1], device='cuda:0') loss 0.2926322817802429 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 205 reward tensor([-1], device='cuda:0') loss 0.31917309761047363 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 206 reward tensor([-1], device='cuda:0') loss 0.2985255718231201 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 207 reward tensor([-1], device='cuda:0') loss 0.25256794691085815 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 208 reward tensor([-1], device='cuda:0') loss 0.2406465858221054 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 209 reward tensor([-1], device='cuda:0') loss 0.2539170980453491 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 210 reward tensor([-1], device='cuda:0') loss 0.1944909393787384 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 211 reward tensor([-1], device='cuda:0') loss 3.874396800994873 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 212 reward tensor([-1], device='cuda:0') loss 3.5654220581054688 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 213 reward tensor([-1], device='cuda:0') loss 2.979566812515259 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 214 reward tensor([-1], device='cuda:0') loss 2.1826066970825195 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 215 reward tensor([-1], device='cuda:0') loss 1.3962912559509277 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 216 reward tensor([-1], device='cuda:0') loss 0.9300612211227417 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 217 reward tensor([-1], device='cuda:0') loss 0.766997754573822 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 218 reward tensor([-1], device='cuda:0') loss 0.8105893135070801 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 219 reward tensor([-1], device='cuda:0') loss 0.9423242211341858 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 220 reward tensor([-1], device='cuda:0') loss 0.9699907898902893 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 221 reward tensor([-1], device='cuda:0') loss 1.0222280025482178 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 222 reward tensor([-1], device='cuda:0') loss 0.9278779029846191 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 223 reward tensor([-1], device='cuda:0') loss 0.9706804752349854 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 224 reward tensor([-1], device='cuda:0') loss 0.7284902334213257 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 225 reward tensor([-1], device='cuda:0') loss 0.6985989212989807 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 226 reward tensor([-1], device='cuda:0') loss 0.680931806564331 epsilon 0.82
26-Feb-25 13:31:04 - agent.DQN.DQN - INFO - episode 1 step 227 reward tensor([-1], device='cuda:0') loss 0.4456055164337158 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 228 reward tensor([-1], device='cuda:0') loss 0.4423850476741791 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 229 reward tensor([-1], device='cuda:0') loss 0.6836704015731812 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 230 reward tensor([-1], device='cuda:0') loss 0.6807582378387451 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 231 reward tensor([-1], device='cuda:0') loss 0.6387971639633179 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 232 reward tensor([-1], device='cuda:0') loss 0.6498541831970215 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 233 reward tensor([-1], device='cuda:0') loss 0.5265606641769409 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 234 reward tensor([-1], device='cuda:0') loss 0.5315985083580017 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 235 reward tensor([-1], device='cuda:0') loss 0.5103070735931396 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 236 reward tensor([-1], device='cuda:0') loss 0.42968618869781494 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 237 reward tensor([-1], device='cuda:0') loss 0.3615589737892151 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 238 reward tensor([-1], device='cuda:0') loss 0.4074316620826721 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 239 reward tensor([-1], device='cuda:0') loss 0.3914950489997864 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 240 reward tensor([-1], device='cuda:0') loss 0.6059352159500122 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 241 reward tensor([-1], device='cuda:0') loss 4.081999778747559 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 242 reward tensor([-1], device='cuda:0') loss 3.9523847103118896 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 243 reward tensor([-1], device='cuda:0') loss 2.830453872680664 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 244 reward tensor([-1], device='cuda:0') loss 2.625600814819336 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 245 reward tensor([-1], device='cuda:0') loss 2.260692596435547 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 246 reward tensor([-1], device='cuda:0') loss 1.8028595447540283 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 247 reward tensor([-1], device='cuda:0') loss 1.7157115936279297 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 248 reward tensor([-1], device='cuda:0') loss 1.3624974489212036 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 249 reward tensor([-1], device='cuda:0') loss 1.0766913890838623 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 250 reward tensor([-1], device='cuda:0') loss 0.8988293409347534 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 251 reward tensor([-1], device='cuda:0') loss 1.35410737991333 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 252 reward tensor([-1], device='cuda:0') loss 0.7469507455825806 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 253 reward tensor([-1], device='cuda:0') loss 1.2396845817565918 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 254 reward tensor([-1], device='cuda:0') loss 1.0039803981781006 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 255 reward tensor([-1], device='cuda:0') loss 1.1242029666900635 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 256 reward tensor([-1], device='cuda:0') loss 1.3919910192489624 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 257 reward tensor([-1], device='cuda:0') loss 0.7399989366531372 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 258 reward tensor([-1], device='cuda:0') loss 1.2091374397277832 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 259 reward tensor([-1], device='cuda:0') loss 1.1133800745010376 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 260 reward tensor([-1], device='cuda:0') loss 1.2217950820922852 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 261 reward tensor([-1], device='cuda:0') loss 1.4349371194839478 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 262 reward tensor([-1], device='cuda:0') loss 1.2697031497955322 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 263 reward tensor([-1], device='cuda:0') loss 1.003943681716919 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 264 reward tensor([-1], device='cuda:0') loss 1.092379093170166 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 265 reward tensor([-1], device='cuda:0') loss 0.9400501847267151 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 266 reward tensor([-1], device='cuda:0') loss 1.143884539604187 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 267 reward tensor([-1], device='cuda:0') loss 1.3112201690673828 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 268 reward tensor([-1], device='cuda:0') loss 1.4849289655685425 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 269 reward tensor([-1], device='cuda:0') loss 1.4417989253997803 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 270 reward tensor([-1], device='cuda:0') loss 1.5769147872924805 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 271 reward tensor([-1], device='cuda:0') loss 3.136148691177368 epsilon 0.82
26-Feb-25 13:31:05 - agent.DQN.DQN - INFO - episode 1 step 272 reward tensor([-1], device='cuda:0') loss 2.8531291484832764 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 273 reward tensor([-1], device='cuda:0') loss 2.200087070465088 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 274 reward tensor([-1], device='cuda:0') loss 1.7880594730377197 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 275 reward tensor([-1], device='cuda:0') loss 1.7768993377685547 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 276 reward tensor([-1], device='cuda:0') loss 1.7269580364227295 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 277 reward tensor([-1], device='cuda:0') loss 1.025301218032837 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 278 reward tensor([-1], device='cuda:0') loss 1.2739880084991455 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 279 reward tensor([-1], device='cuda:0') loss 1.8253145217895508 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 280 reward tensor([-1], device='cuda:0') loss 1.5409892797470093 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 281 reward tensor([-1], device='cuda:0') loss 0.9803462028503418 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 282 reward tensor([-1], device='cuda:0') loss 1.277299165725708 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 283 reward tensor([-1], device='cuda:0') loss 1.3187408447265625 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 284 reward tensor([-1], device='cuda:0') loss 1.4460699558258057 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 285 reward tensor([-1], device='cuda:0') loss 1.4418022632598877 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 286 reward tensor([-1], device='cuda:0') loss 1.2104809284210205 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 287 reward tensor([-1], device='cuda:0') loss 1.3380217552185059 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 288 reward tensor([-1], device='cuda:0') loss 1.4321351051330566 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 289 reward tensor([-1], device='cuda:0') loss 1.0584139823913574 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 290 reward tensor([-1], device='cuda:0') loss 0.9215375185012817 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 291 reward tensor([-1], device='cuda:0') loss 1.2141687870025635 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 292 reward tensor([-1], device='cuda:0') loss 1.0351717472076416 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 293 reward tensor([-1], device='cuda:0') loss 1.0533548593521118 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 294 reward tensor([-1], device='cuda:0') loss 0.8630852699279785 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 295 reward tensor([-1], device='cuda:0') loss 1.0764573812484741 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 296 reward tensor([-1], device='cuda:0') loss 1.0851867198944092 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 297 reward tensor([-1], device='cuda:0') loss 0.9490202069282532 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 298 reward tensor([-1], device='cuda:0') loss 1.021657943725586 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 299 reward tensor([-1], device='cuda:0') loss 0.9968574643135071 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 300 reward tensor([-1], device='cuda:0') loss 0.8880075216293335 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 1 step 301 reward tensor([-1], device='cuda:0') loss 4.184123992919922 epsilon 0.82
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 0 reward tensor([-1], device='cuda:0') loss 3.947310447692871 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 1 reward tensor([-1], device='cuda:0') loss 15.56712818145752 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 2 reward tensor([-1], device='cuda:0') loss 14.185009956359863 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 3 reward tensor([-1], device='cuda:0') loss 1.7948062419891357 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 4 reward tensor([-1], device='cuda:0') loss 12.612540245056152 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 5 reward tensor([-1], device='cuda:0') loss 1.2904164791107178 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 6 reward tensor([-1], device='cuda:0') loss 11.258252143859863 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 7 reward tensor([-1], device='cuda:0') loss 0.8694638013839722 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 8 reward tensor([-1], device='cuda:0') loss 0.7767502069473267 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 9 reward tensor([-1], device='cuda:0') loss 0.9613729119300842 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 10 reward tensor([-1], device='cuda:0') loss 0.937893271446228 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 11 reward tensor([-1], device='cuda:0') loss 9.605581283569336 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 12 reward tensor([-1], device='cuda:0') loss 1.322505235671997 epsilon 0.79335
26-Feb-25 13:31:06 - agent.DQN.DQN - INFO - episode 2 step 13 reward tensor([-1], device='cuda:0') loss 1.127131462097168 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 14 reward tensor([-1], device='cuda:0') loss 1.2843466997146606 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 15 reward tensor([-1], device='cuda:0') loss 1.331571102142334 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 16 reward tensor([-1], device='cuda:0') loss 9.07160472869873 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 17 reward tensor([-1], device='cuda:0') loss 1.5267879962921143 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 18 reward tensor([-1], device='cuda:0') loss 1.3735394477844238 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 19 reward tensor([-1], device='cuda:0') loss 8.89681625366211 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 20 reward tensor([-1], device='cuda:0') loss 8.55370807647705 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 21 reward tensor([-1], device='cuda:0') loss 1.1851658821105957 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 22 reward tensor([-1], device='cuda:0') loss 8.431381225585938 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 23 reward tensor([-1], device='cuda:0') loss 0.9260852336883545 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 24 reward tensor([-1], device='cuda:0') loss 1.336308479309082 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 25 reward tensor([-1], device='cuda:0') loss 8.280278205871582 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 26 reward tensor([-1], device='cuda:0') loss 8.36027717590332 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 27 reward tensor([-1], device='cuda:0') loss 1.2852022647857666 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 28 reward tensor([-1], device='cuda:0') loss 8.175808906555176 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 29 reward tensor([-1], device='cuda:0') loss 3.326101303100586 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 30 reward tensor([-1], device='cuda:0') loss 3.4344916343688965 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 31 reward tensor([-1], device='cuda:0') loss 10.286972999572754 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 32 reward tensor([-1], device='cuda:0') loss 10.158126831054688 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 33 reward tensor([-1], device='cuda:0') loss 8.959144592285156 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 34 reward tensor([-1], device='cuda:0') loss 1.5349111557006836 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 35 reward tensor([-1], device='cuda:0') loss 8.106522560119629 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 36 reward tensor([-1], device='cuda:0') loss 0.9941491484642029 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 37 reward tensor([-1], device='cuda:0') loss 0.8415975570678711 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 38 reward tensor([-1], device='cuda:0') loss 0.8612444400787354 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 39 reward tensor([-1], device='cuda:0') loss 0.8159070014953613 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 40 reward tensor([-1], device='cuda:0') loss 7.357492923736572 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 41 reward tensor([-1], device='cuda:0') loss 0.9594429731369019 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 42 reward tensor([-1], device='cuda:0') loss 0.9418056607246399 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 43 reward tensor([-1], device='cuda:0') loss 1.1514519453048706 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 44 reward tensor([-1], device='cuda:0') loss 1.022275686264038 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 45 reward tensor([-1], device='cuda:0') loss 7.524410247802734 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 46 reward tensor([-1], device='cuda:0') loss 0.9394916892051697 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 47 reward tensor([-1], device='cuda:0') loss 7.414737224578857 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 48 reward tensor([-1], device='cuda:0') loss 1.0751744508743286 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 49 reward tensor([-1], device='cuda:0') loss 6.857558250427246 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 50 reward tensor([-1], device='cuda:0') loss 1.1597232818603516 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 51 reward tensor([-1], device='cuda:0') loss 1.4224815368652344 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 52 reward tensor([-1], device='cuda:0') loss 6.941736221313477 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 53 reward tensor([-1], device='cuda:0') loss 1.4406245946884155 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 54 reward tensor([-1], device='cuda:0') loss 7.056003093719482 epsilon 0.79335
26-Feb-25 13:31:07 - agent.DQN.DQN - INFO - episode 2 step 55 reward tensor([-1], device='cuda:0') loss 1.4985183477401733 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 56 reward tensor([-1], device='cuda:0') loss 6.293849468231201 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 57 reward tensor([-1], device='cuda:0') loss 0.8335845470428467 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 58 reward tensor([-1], device='cuda:0') loss 1.4520015716552734 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 59 reward tensor([-1], device='cuda:0') loss 3.3464760780334473 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 60 reward tensor([-1], device='cuda:0') loss 8.030025482177734 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 61 reward tensor([-1], device='cuda:0') loss 3.055501937866211 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 62 reward tensor([-1], device='cuda:0') loss 7.832719802856445 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 63 reward tensor([-1], device='cuda:0') loss 2.8976874351501465 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 64 reward tensor([-1], device='cuda:0') loss 6.902805328369141 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 65 reward tensor([-1], device='cuda:0') loss 2.6167778968811035 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 66 reward tensor([-1], device='cuda:0') loss 1.4514598846435547 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 67 reward tensor([-1], device='cuda:0') loss 1.7504338026046753 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 68 reward tensor([-1], device='cuda:0') loss 6.701709270477295 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 69 reward tensor([-1], device='cuda:0') loss 3.9725985527038574 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 70 reward tensor([-1], device='cuda:0') loss 7.0729827880859375 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 71 reward tensor([-1], device='cuda:0') loss 3.5564780235290527 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 72 reward tensor([-1], device='cuda:0') loss 1.4654247760772705 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 73 reward tensor([-1], device='cuda:0') loss 2.160038709640503 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 74 reward tensor([-1], device='cuda:0') loss 5.914124011993408 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 75 reward tensor([-1], device='cuda:0') loss 4.0097479820251465 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 76 reward tensor([-1], device='cuda:0') loss 8.618729591369629 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 77 reward tensor([-1], device='cuda:0') loss 6.66331672668457 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 78 reward tensor([-1], device='cuda:0') loss 6.450124740600586 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 79 reward tensor([-1], device='cuda:0') loss 7.4740705490112305 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 80 reward tensor([-1], device='cuda:0') loss 12.301259994506836 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 81 reward tensor([-1], device='cuda:0') loss 4.69536828994751 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 82 reward tensor([-1], device='cuda:0') loss 4.978558540344238 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 83 reward tensor([-1], device='cuda:0') loss 12.413683891296387 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 84 reward tensor([-1], device='cuda:0') loss 10.639501571655273 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 85 reward tensor([-1], device='cuda:0') loss 5.740399360656738 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 86 reward tensor([-1], device='cuda:0') loss 6.651888847351074 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 87 reward tensor([-1], device='cuda:0') loss 9.421469688415527 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 88 reward tensor([-1], device='cuda:0') loss 16.437429428100586 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 89 reward tensor([-1], device='cuda:0') loss 5.825370788574219 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 90 reward tensor([-1], device='cuda:0') loss 7.514664649963379 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 91 reward tensor([-1], device='cuda:0') loss 9.762370109558105 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 92 reward tensor([-1], device='cuda:0') loss 9.709587097167969 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 93 reward tensor([-1], device='cuda:0') loss 7.8553643226623535 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 94 reward tensor([-1], device='cuda:0') loss 8.152963638305664 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 95 reward tensor([-1], device='cuda:0') loss 5.284089088439941 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 96 reward tensor([-1], device='cuda:0') loss 3.7264456748962402 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 97 reward tensor([-1], device='cuda:0') loss 7.324141979217529 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 98 reward tensor([-1], device='cuda:0') loss 8.544578552246094 epsilon 0.79335
26-Feb-25 13:31:08 - agent.DQN.DQN - INFO - episode 2 step 99 reward tensor([-1], device='cuda:0') loss 2.3886818885803223 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 100 reward tensor([-1], device='cuda:0') loss 11.73901081085205 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 101 reward tensor([-1], device='cuda:0') loss 6.794618129730225 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 102 reward tensor([-1], device='cuda:0') loss 6.17253303527832 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 103 reward tensor([-1], device='cuda:0') loss 2.7288708686828613 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 104 reward tensor([-1], device='cuda:0') loss 7.549251079559326 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 105 reward tensor([-1], device='cuda:0') loss 12.55506706237793 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 106 reward tensor([-1], device='cuda:0') loss 3.52976131439209 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 107 reward tensor([-1], device='cuda:0') loss 5.865720272064209 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 108 reward tensor([-1], device='cuda:0') loss 7.453505516052246 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 109 reward tensor([-1], device='cuda:0') loss 12.520840644836426 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 110 reward tensor([-1], device='cuda:0') loss 12.028253555297852 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 111 reward tensor([-1], device='cuda:0') loss 7.179618835449219 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 112 reward tensor([-1], device='cuda:0') loss 7.712701320648193 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 113 reward tensor([-1], device='cuda:0') loss 5.572999954223633 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 114 reward tensor([-1], device='cuda:0') loss 5.795278072357178 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 115 reward tensor([-1], device='cuda:0') loss 8.885973930358887 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 116 reward tensor([-1], device='cuda:0') loss 6.344152927398682 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 117 reward tensor([-1], device='cuda:0') loss 6.986631393432617 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 118 reward tensor([-1], device='cuda:0') loss 3.2322702407836914 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 119 reward tensor([-1], device='cuda:0') loss 3.20979642868042 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 120 reward tensor([-1], device='cuda:0') loss 4.13728141784668 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 121 reward tensor([-1], device='cuda:0') loss 2.7524256706237793 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 122 reward tensor([-1], device='cuda:0') loss 3.531620979309082 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 123 reward tensor([-1], device='cuda:0') loss 2.36566162109375 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 124 reward tensor([-1], device='cuda:0') loss 2.8403255939483643 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 125 reward tensor([-1], device='cuda:0') loss 2.621647357940674 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 126 reward tensor([-1], device='cuda:0') loss 7.166093349456787 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 127 reward tensor([-1], device='cuda:0') loss 2.3748598098754883 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 128 reward tensor([-1], device='cuda:0') loss 2.0370311737060547 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 129 reward tensor([-1], device='cuda:0') loss 1.581053376197815 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 130 reward tensor([-1], device='cuda:0') loss 1.1013003587722778 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 131 reward tensor([-1], device='cuda:0') loss 1.6444817781448364 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 132 reward tensor([-1], device='cuda:0') loss 2.621570587158203 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 133 reward tensor([-1], device='cuda:0') loss 1.1211206912994385 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 134 reward tensor([-1], device='cuda:0') loss 1.8418684005737305 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 135 reward tensor([-1], device='cuda:0') loss 1.5857460498809814 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 136 reward tensor([-1], device='cuda:0') loss 1.572862148284912 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 137 reward tensor([-1], device='cuda:0') loss 1.7879889011383057 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 138 reward tensor([-1], device='cuda:0') loss 1.582248330116272 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 139 reward tensor([-1], device='cuda:0') loss 1.1818554401397705 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 140 reward tensor([-1], device='cuda:0') loss 2.461895704269409 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 141 reward tensor([-1], device='cuda:0') loss 2.812577724456787 epsilon 0.79335
26-Feb-25 13:31:09 - agent.DQN.DQN - INFO - episode 2 step 142 reward tensor([-1], device='cuda:0') loss 4.767877578735352 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 143 reward tensor([-1], device='cuda:0') loss 2.30216908454895 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 144 reward tensor([-1], device='cuda:0') loss 4.83568811416626 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 145 reward tensor([-1], device='cuda:0') loss 4.9169793128967285 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 146 reward tensor([-1], device='cuda:0') loss 4.449569225311279 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 147 reward tensor([-1], device='cuda:0') loss 4.532611846923828 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 148 reward tensor([-1], device='cuda:0') loss 1.0223608016967773 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 149 reward tensor([-1], device='cuda:0') loss 2.778271436691284 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 150 reward tensor([-1], device='cuda:0') loss 5.7790422439575195 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 151 reward tensor([-1], device='cuda:0') loss 2.818716526031494 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 152 reward tensor([-1], device='cuda:0') loss 1.7018988132476807 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 153 reward tensor([-1], device='cuda:0') loss 1.9092650413513184 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 154 reward tensor([-1], device='cuda:0') loss 1.7458536624908447 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 155 reward tensor([-1], device='cuda:0') loss 1.746034860610962 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 156 reward tensor([-1], device='cuda:0') loss 4.4738359451293945 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 157 reward tensor([-1], device='cuda:0') loss 1.3544567823410034 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 158 reward tensor([-1], device='cuda:0') loss 1.2856864929199219 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 159 reward tensor([-1], device='cuda:0') loss 4.276138782501221 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 160 reward tensor([-1], device='cuda:0') loss 1.0499095916748047 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 161 reward tensor([-1], device='cuda:0') loss 1.0667577981948853 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 162 reward tensor([-1], device='cuda:0') loss 3.8978025913238525 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 163 reward tensor([-1], device='cuda:0') loss 1.4454190731048584 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 164 reward tensor([-1], device='cuda:0') loss 3.772512197494507 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 165 reward tensor([-1], device='cuda:0') loss 1.0768396854400635 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 166 reward tensor([-1], device='cuda:0') loss 3.989933967590332 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 167 reward tensor([-1], device='cuda:0') loss 1.0948731899261475 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 168 reward tensor([-1], device='cuda:0') loss 3.853100299835205 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 169 reward tensor([-1], device='cuda:0') loss 1.086017370223999 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 170 reward tensor([-1], device='cuda:0') loss 1.1384605169296265 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 171 reward tensor([-1], device='cuda:0') loss 0.9797979593276978 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 172 reward tensor([-1], device='cuda:0') loss 1.130357027053833 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 173 reward tensor([-1], device='cuda:0') loss 3.534304141998291 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 174 reward tensor([-1], device='cuda:0') loss 3.5593955516815186 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 175 reward tensor([-1], device='cuda:0') loss 3.5250043869018555 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 176 reward tensor([-1], device='cuda:0') loss 1.0295536518096924 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 177 reward tensor([-1], device='cuda:0') loss 0.9698820114135742 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 178 reward tensor([-1], device='cuda:0') loss 1.5101131200790405 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 179 reward tensor([-1], device='cuda:0') loss 2.9586896896362305 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 180 reward tensor([-1], device='cuda:0') loss 3.557253837585449 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 181 reward tensor([-1], device='cuda:0') loss 2.9809296131134033 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 182 reward tensor([-1], device='cuda:0') loss 1.9679945707321167 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 183 reward tensor([-1], device='cuda:0') loss 1.7759431600570679 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 184 reward tensor([-1], device='cuda:0') loss 1.743045449256897 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 185 reward tensor([-1], device='cuda:0') loss 1.3425877094268799 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 186 reward tensor([-1], device='cuda:0') loss 4.291874408721924 epsilon 0.79335
26-Feb-25 13:31:10 - agent.DQN.DQN - INFO - episode 2 step 187 reward tensor([-1], device='cuda:0') loss 3.959714412689209 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 188 reward tensor([-1], device='cuda:0') loss 1.2499430179595947 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 189 reward tensor([-1], device='cuda:0') loss 1.126625895500183 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 190 reward tensor([-1], device='cuda:0') loss 3.7016091346740723 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 191 reward tensor([-1], device='cuda:0') loss 0.9507215619087219 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 192 reward tensor([-1], device='cuda:0') loss 0.9036493897438049 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 193 reward tensor([-1], device='cuda:0') loss 1.013156771659851 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 194 reward tensor([-1], device='cuda:0') loss 3.3971903324127197 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 195 reward tensor([-1], device='cuda:0') loss 3.092212438583374 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 196 reward tensor([-1], device='cuda:0') loss 3.1930150985717773 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 197 reward tensor([-1], device='cuda:0') loss 1.1136949062347412 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 198 reward tensor([-1], device='cuda:0') loss 2.998026132583618 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 199 reward tensor([-1], device='cuda:0') loss 1.056010365486145 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 200 reward tensor([-1], device='cuda:0') loss 3.157900094985962 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 201 reward tensor([-1], device='cuda:0') loss 0.930208683013916 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 202 reward tensor([-1], device='cuda:0') loss 1.1959633827209473 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 203 reward tensor([-1], device='cuda:0') loss 1.3193402290344238 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 204 reward tensor([-1], device='cuda:0') loss 3.262936592102051 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 205 reward tensor([-1], device='cuda:0') loss 2.4003548622131348 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 206 reward tensor([-1], device='cuda:0') loss 0.8655338287353516 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 207 reward tensor([-1], device='cuda:0') loss 0.7668706178665161 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 208 reward tensor([-1], device='cuda:0') loss 0.5628458261489868 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 209 reward tensor([-1], device='cuda:0') loss 6.052434921264648 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 210 reward tensor([-1], device='cuda:0') loss 3.0218887329101562 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 211 reward tensor([-1], device='cuda:0') loss 2.732588768005371 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 212 reward tensor([-1], device='cuda:0') loss 2.1826956272125244 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 213 reward tensor([-1], device='cuda:0') loss 1.0781216621398926 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 214 reward tensor([-1], device='cuda:0') loss 1.3093881607055664 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 215 reward tensor([-1], device='cuda:0') loss 1.238999605178833 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 216 reward tensor([-1], device='cuda:0') loss 1.9012680053710938 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 217 reward tensor([-1], device='cuda:0') loss 1.3563716411590576 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 218 reward tensor([-1], device='cuda:0') loss 0.9059120416641235 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 219 reward tensor([-1], device='cuda:0') loss 0.8974021077156067 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 220 reward tensor([-1], device='cuda:0') loss 0.9526904821395874 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 221 reward tensor([-1], device='cuda:0') loss 0.6889168620109558 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 222 reward tensor([-1], device='cuda:0') loss 0.8714514374732971 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 223 reward tensor([-1], device='cuda:0') loss 2.9723715782165527 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 224 reward tensor([-1], device='cuda:0') loss 1.0364923477172852 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 225 reward tensor([-1], device='cuda:0') loss 1.1050302982330322 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 226 reward tensor([-1], device='cuda:0') loss 1.1763025522232056 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 227 reward tensor([-1], device='cuda:0') loss 0.9940170049667358 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 228 reward tensor([-1], device='cuda:0') loss 0.7139148712158203 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 229 reward tensor([-1], device='cuda:0') loss 0.5458070635795593 epsilon 0.79335
26-Feb-25 13:31:11 - agent.DQN.DQN - INFO - episode 2 step 230 reward tensor([-1], device='cuda:0') loss 2.886739730834961 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 231 reward tensor([-1], device='cuda:0') loss 0.5318444967269897 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 232 reward tensor([-1], device='cuda:0') loss 0.5343049168586731 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 233 reward tensor([-1], device='cuda:0') loss 0.6596490144729614 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 234 reward tensor([-1], device='cuda:0') loss 2.821765899658203 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 235 reward tensor([-1], device='cuda:0') loss 0.6222414970397949 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 236 reward tensor([-1], device='cuda:0') loss 0.46528103947639465 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 237 reward tensor([-1], device='cuda:0') loss 0.5566304922103882 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 238 reward tensor([-1], device='cuda:0') loss 0.6201393604278564 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 239 reward tensor([-1], device='cuda:0') loss 4.195355415344238 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 240 reward tensor([-1], device='cuda:0') loss 2.3138110637664795 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 241 reward tensor([-1], device='cuda:0') loss 5.603169918060303 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 242 reward tensor([-1], device='cuda:0') loss 2.387805461883545 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 243 reward tensor([-1], device='cuda:0') loss 1.8986116647720337 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 244 reward tensor([-1], device='cuda:0') loss 1.5176342725753784 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 245 reward tensor([-1], device='cuda:0') loss 3.244706630706787 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 246 reward tensor([-1], device='cuda:0') loss 0.9402636289596558 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 247 reward tensor([-1], device='cuda:0') loss 1.2156403064727783 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 248 reward tensor([-1], device='cuda:0') loss 2.727874279022217 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 249 reward tensor([-1], device='cuda:0') loss 0.8970053195953369 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 250 reward tensor([-1], device='cuda:0') loss 1.017927646636963 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 251 reward tensor([-1], device='cuda:0') loss 2.4620723724365234 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 252 reward tensor([-1], device='cuda:0') loss 0.9829925298690796 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 253 reward tensor([-1], device='cuda:0') loss 0.7901030778884888 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 254 reward tensor([-1], device='cuda:0') loss 0.8747212886810303 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 255 reward tensor([-1], device='cuda:0') loss 0.7785782217979431 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 256 reward tensor([-1], device='cuda:0') loss 2.4397881031036377 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 257 reward tensor([-1], device='cuda:0') loss 2.677736520767212 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 258 reward tensor([-1], device='cuda:0') loss 2.0435357093811035 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 259 reward tensor([-1], device='cuda:0') loss 0.9673373103141785 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 260 reward tensor([-1], device='cuda:0') loss 0.6988010406494141 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 261 reward tensor([-1], device='cuda:0') loss 0.6525990962982178 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 262 reward tensor([-1], device='cuda:0') loss 0.4266297221183777 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 263 reward tensor([-1], device='cuda:0') loss 0.7248183488845825 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 264 reward tensor([-1], device='cuda:0') loss 0.5029821991920471 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 265 reward tensor([-1], device='cuda:0') loss 1.9463145732879639 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 266 reward tensor([-1], device='cuda:0') loss 1.8582627773284912 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 267 reward tensor([-1], device='cuda:0') loss 0.719406008720398 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 268 reward tensor([-1], device='cuda:0') loss 0.9707794189453125 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 269 reward tensor([-1], device='cuda:0') loss 4.494926452636719 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 270 reward tensor([-1], device='cuda:0') loss 3.6577346324920654 epsilon 0.79335
26-Feb-25 13:31:12 - agent.DQN.DQN - INFO - episode 2 step 271 reward tensor([-1], device='cuda:0') loss 4.188697814941406 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 272 reward tensor([-1], device='cuda:0') loss 2.049029588699341 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 273 reward tensor([-1], device='cuda:0') loss 1.1312007904052734 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 274 reward tensor([-1], device='cuda:0') loss 0.874488890171051 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 275 reward tensor([-1], device='cuda:0') loss 1.9481374025344849 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 276 reward tensor([-1], device='cuda:0') loss 0.8280363082885742 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 277 reward tensor([-1], device='cuda:0') loss 1.1747767925262451 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 278 reward tensor([-1], device='cuda:0') loss 0.9886218309402466 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 279 reward tensor([-1], device='cuda:0') loss 1.1874141693115234 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 280 reward tensor([-1], device='cuda:0') loss 0.9441588521003723 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 281 reward tensor([-1], device='cuda:0') loss 2.420109510421753 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 282 reward tensor([-1], device='cuda:0') loss 1.270626187324524 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 283 reward tensor([-1], device='cuda:0') loss 0.8272015452384949 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 284 reward tensor([-1], device='cuda:0') loss 0.8369150161743164 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 285 reward tensor([-1], device='cuda:0') loss 0.8146740198135376 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 286 reward tensor([-1], device='cuda:0') loss 0.6456924080848694 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 287 reward tensor([-1], device='cuda:0') loss 0.677396297454834 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 288 reward tensor([-1], device='cuda:0') loss 0.5510503053665161 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 289 reward tensor([-1], device='cuda:0') loss 0.7091466784477234 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 290 reward tensor([-1], device='cuda:0') loss 1.901597499847412 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 291 reward tensor([-1], device='cuda:0') loss 0.7677018642425537 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 292 reward tensor([-1], device='cuda:0') loss 1.9764623641967773 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 293 reward tensor([-1], device='cuda:0') loss 1.856827974319458 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 294 reward tensor([-1], device='cuda:0') loss 1.8583570718765259 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 295 reward tensor([-1], device='cuda:0') loss 0.5046119689941406 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 296 reward tensor([-1], device='cuda:0') loss 1.6447007656097412 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 297 reward tensor([-1], device='cuda:0') loss 0.4648195505142212 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 298 reward tensor([-1], device='cuda:0') loss 1.4937801361083984 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 299 reward tensor([-1], device='cuda:0') loss 4.456830978393555 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 300 reward tensor([-1], device='cuda:0') loss 4.347900390625 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 2 step 301 reward tensor([-1], device='cuda:0') loss 3.453916549682617 epsilon 0.79335
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 0 reward tensor([-1], device='cuda:0') loss 3.144326686859131 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 1 reward tensor([-1], device='cuda:0') loss 1.5443134307861328 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 2 reward tensor([-1], device='cuda:0') loss 1.0307830572128296 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 3 reward tensor([-1], device='cuda:0') loss 12.871252059936523 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 4 reward tensor([-1], device='cuda:0') loss 1.4272935390472412 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 5 reward tensor([-1], device='cuda:0') loss 2.3542003631591797 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 6 reward tensor([-1], device='cuda:0') loss 1.327299952507019 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 7 reward tensor([-1], device='cuda:0') loss 2.491145610809326 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 8 reward tensor([-1], device='cuda:0') loss 1.4557092189788818 epsilon 0.767566125
26-Feb-25 13:31:13 - agent.DQN.DQN - INFO - episode 3 step 9 reward tensor([-1], device='cuda:0') loss 1.4688892364501953 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 10 reward tensor([-1], device='cuda:0') loss 1.2842196226119995 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 11 reward tensor([-1], device='cuda:0') loss 15.480114936828613 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 12 reward tensor([-1], device='cuda:0') loss 1.4243961572647095 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 13 reward tensor([-1], device='cuda:0') loss 1.7445495128631592 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 14 reward tensor([-1], device='cuda:0') loss 1.4798166751861572 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 15 reward tensor([-1], device='cuda:0') loss 1.4865435361862183 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 16 reward tensor([-1], device='cuda:0') loss 0.9518604874610901 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 17 reward tensor([-1], device='cuda:0') loss 12.627001762390137 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 18 reward tensor([-1], device='cuda:0') loss 0.7901268005371094 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 19 reward tensor([-1], device='cuda:0') loss 0.9997506141662598 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 20 reward tensor([-1], device='cuda:0') loss 1.9343125820159912 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 21 reward tensor([-1], device='cuda:0') loss 11.165481567382812 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 22 reward tensor([-1], device='cuda:0') loss 1.078700304031372 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 23 reward tensor([-1], device='cuda:0') loss 11.051885604858398 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 24 reward tensor([-1], device='cuda:0') loss 1.3240621089935303 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 25 reward tensor([-1], device='cuda:0') loss 1.5169060230255127 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 26 reward tensor([-1], device='cuda:0') loss 0.9673287272453308 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 27 reward tensor([-1], device='cuda:0') loss 14.099493026733398 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 28 reward tensor([-1], device='cuda:0') loss 12.389520645141602 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 29 reward tensor([-1], device='cuda:0') loss 4.07594108581543 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 30 reward tensor([-1], device='cuda:0') loss 2.3342485427856445 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 31 reward tensor([-1], device='cuda:0') loss 1.8650972843170166 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 32 reward tensor([-1], device='cuda:0') loss 1.7669702768325806 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 33 reward tensor([-1], device='cuda:0') loss 0.8968336582183838 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 34 reward tensor([-1], device='cuda:0') loss 0.9045102596282959 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 35 reward tensor([-1], device='cuda:0') loss 0.7928799986839294 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 36 reward tensor([-1], device='cuda:0') loss 0.9102106690406799 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 37 reward tensor([-1], device='cuda:0') loss 0.7145620584487915 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 38 reward tensor([-1], device='cuda:0') loss 0.9822671413421631 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 39 reward tensor([-1], device='cuda:0') loss 1.2009081840515137 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 40 reward tensor([-1], device='cuda:0') loss 1.2067804336547852 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 41 reward tensor([-1], device='cuda:0') loss 11.732599258422852 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 42 reward tensor([-1], device='cuda:0') loss 1.8572807312011719 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 43 reward tensor([-1], device='cuda:0') loss 0.9107617139816284 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 44 reward tensor([-1], device='cuda:0') loss 0.8565963506698608 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 45 reward tensor([-1], device='cuda:0') loss 1.6383709907531738 epsilon 0.767566125
26-Feb-25 13:31:14 - agent.DQN.DQN - INFO - episode 3 step 46 reward tensor([-1], device='cuda:0') loss 1.292519211769104 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 47 reward tensor([-1], device='cuda:0') loss 10.340673446655273 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 48 reward tensor([-1], device='cuda:0') loss 1.3712791204452515 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 49 reward tensor([-1], device='cuda:0') loss 1.4283511638641357 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 50 reward tensor([-1], device='cuda:0') loss 9.327963829040527 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 51 reward tensor([-1], device='cuda:0') loss 0.7274218797683716 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 52 reward tensor([-1], device='cuda:0') loss 0.8665146827697754 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 53 reward tensor([-1], device='cuda:0') loss 1.2854732275009155 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 54 reward tensor([-1], device='cuda:0') loss 0.8283369541168213 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 55 reward tensor([-1], device='cuda:0') loss 1.0938535928726196 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 56 reward tensor([-1], device='cuda:0') loss 0.7401161789894104 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 57 reward tensor([-1], device='cuda:0') loss 3.8695595264434814 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 58 reward tensor([-1], device='cuda:0') loss 3.318070650100708 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 59 reward tensor([-1], device='cuda:0') loss 2.7181456089019775 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 60 reward tensor([-1], device='cuda:0') loss 2.1896181106567383 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 61 reward tensor([-1], device='cuda:0') loss 1.3556933403015137 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 62 reward tensor([-1], device='cuda:0') loss 0.9691585302352905 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 63 reward tensor([-1], device='cuda:0') loss 0.747085690498352 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 64 reward tensor([-1], device='cuda:0') loss 0.6034848093986511 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 65 reward tensor([-1], device='cuda:0') loss 0.6783382296562195 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 66 reward tensor([-1], device='cuda:0') loss 0.6809260845184326 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 67 reward tensor([-1], device='cuda:0') loss 8.939363479614258 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 68 reward tensor([-1], device='cuda:0') loss 1.4490671157836914 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 69 reward tensor([-1], device='cuda:0') loss 1.1312464475631714 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 70 reward tensor([-1], device='cuda:0') loss 0.7688090205192566 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 71 reward tensor([-1], device='cuda:0') loss 1.3780665397644043 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 72 reward tensor([-1], device='cuda:0') loss 1.2815358638763428 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 73 reward tensor([-1], device='cuda:0') loss 8.181886672973633 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 74 reward tensor([-1], device='cuda:0') loss 1.1415719985961914 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 75 reward tensor([-1], device='cuda:0') loss 7.9212751388549805 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 76 reward tensor([-1], device='cuda:0') loss 0.83213871717453 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 77 reward tensor([-1], device='cuda:0') loss 7.302491188049316 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 78 reward tensor([-1], device='cuda:0') loss 7.183168411254883 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 79 reward tensor([-1], device='cuda:0') loss 7.038567066192627 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 80 reward tensor([-1], device='cuda:0') loss 1.0491567850112915 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 81 reward tensor([-1], device='cuda:0') loss 1.003448486328125 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 82 reward tensor([-1], device='cuda:0') loss 6.9398064613342285 epsilon 0.767566125
26-Feb-25 13:31:15 - agent.DQN.DQN - INFO - episode 3 step 83 reward tensor([-1], device='cuda:0') loss 6.217146396636963 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 84 reward tensor([-1], device='cuda:0') loss 2.7472753524780273 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 85 reward tensor([-1], device='cuda:0') loss 2.1688599586486816 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 86 reward tensor([-1], device='cuda:0') loss 2.3098671436309814 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 87 reward tensor([-1], device='cuda:0') loss 3.378737449645996 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 88 reward tensor([-1], device='cuda:0') loss 3.325887680053711 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 89 reward tensor([-1], device='cuda:0') loss 2.240906238555908 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 90 reward tensor([-1], device='cuda:0') loss 1.7877120971679688 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 91 reward tensor([-1], device='cuda:0') loss 1.4384007453918457 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 92 reward tensor([-1], device='cuda:0') loss 1.5212714672088623 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 93 reward tensor([-1], device='cuda:0') loss 1.123335838317871 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 94 reward tensor([-1], device='cuda:0') loss 1.1377224922180176 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 95 reward tensor([-1], device='cuda:0') loss 0.9890040159225464 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 96 reward tensor([-1], device='cuda:0') loss 0.8256920576095581 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 97 reward tensor([-1], device='cuda:0') loss 0.551862895488739 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 98 reward tensor([-1], device='cuda:0') loss 0.7623695135116577 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 99 reward tensor([-1], device='cuda:0') loss 7.851649284362793 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 100 reward tensor([-1], device='cuda:0') loss 0.9223372936248779 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 101 reward tensor([-1], device='cuda:0') loss 0.9195601940155029 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 102 reward tensor([-1], device='cuda:0') loss 1.0759434700012207 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 103 reward tensor([-1], device='cuda:0') loss 6.562051773071289 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 104 reward tensor([-1], device='cuda:0') loss 6.281142234802246 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 105 reward tensor([-1], device='cuda:0') loss 1.6977910995483398 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 106 reward tensor([-1], device='cuda:0') loss 1.4499322175979614 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 107 reward tensor([-1], device='cuda:0') loss 1.8541018962860107 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 108 reward tensor([-1], device='cuda:0') loss 0.9817253351211548 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 109 reward tensor([-1], device='cuda:0') loss 6.421050548553467 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 110 reward tensor([-1], device='cuda:0') loss 1.6099796295166016 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 111 reward tensor([-1], device='cuda:0') loss 0.9440062046051025 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 112 reward tensor([-1], device='cuda:0') loss 0.8324840068817139 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 113 reward tensor([-1], device='cuda:0') loss 6.858305931091309 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 114 reward tensor([-1], device='cuda:0') loss 0.5429852604866028 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 115 reward tensor([-1], device='cuda:0') loss 0.9849653840065002 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 116 reward tensor([-1], device='cuda:0') loss 0.6021421551704407 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 117 reward tensor([-1], device='cuda:0') loss 8.077122688293457 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 118 reward tensor([-1], device='cuda:0') loss 2.1690382957458496 epsilon 0.767566125
26-Feb-25 13:31:16 - agent.DQN.DQN - INFO - episode 3 step 119 reward tensor([-1], device='cuda:0') loss 2.017164707183838 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 120 reward tensor([-1], device='cuda:0') loss 1.6556634902954102 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 121 reward tensor([-1], device='cuda:0') loss 0.9288551211357117 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 122 reward tensor([-1], device='cuda:0') loss 6.8199896812438965 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 123 reward tensor([-1], device='cuda:0') loss 5.916615009307861 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 124 reward tensor([-1], device='cuda:0') loss 5.51269006729126 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 125 reward tensor([-1], device='cuda:0') loss 5.891526222229004 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 126 reward tensor([-1], device='cuda:0') loss 1.3268780708312988 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 127 reward tensor([-1], device='cuda:0') loss 2.6101205348968506 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 128 reward tensor([-1], device='cuda:0') loss 1.3739333152770996 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 129 reward tensor([-1], device='cuda:0') loss 1.9900623559951782 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 130 reward tensor([-1], device='cuda:0') loss 1.1394551992416382 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 131 reward tensor([-1], device='cuda:0') loss 2.0198113918304443 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 132 reward tensor([-1], device='cuda:0') loss 1.0757449865341187 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 133 reward tensor([-1], device='cuda:0') loss 6.216674327850342 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 134 reward tensor([-1], device='cuda:0') loss 1.8041322231292725 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 135 reward tensor([-1], device='cuda:0') loss 1.5814943313598633 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 136 reward tensor([-1], device='cuda:0') loss 1.0681425333023071 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 137 reward tensor([-1], device='cuda:0') loss 1.1334292888641357 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 138 reward tensor([-1], device='cuda:0') loss 0.9848024845123291 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 139 reward tensor([-1], device='cuda:0') loss 1.2363276481628418 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 140 reward tensor([-1], device='cuda:0') loss 6.538258075714111 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 141 reward tensor([-1], device='cuda:0') loss 5.950448989868164 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 142 reward tensor([-1], device='cuda:0') loss 5.625864028930664 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 143 reward tensor([-1], device='cuda:0') loss 1.638343334197998 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 144 reward tensor([-1], device='cuda:0') loss 0.804193913936615 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 145 reward tensor([-1], device='cuda:0') loss 1.633472204208374 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 146 reward tensor([-1], device='cuda:0') loss 1.6786186695098877 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 147 reward tensor([-1], device='cuda:0') loss 6.713451385498047 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 148 reward tensor([-1], device='cuda:0') loss 2.5777838230133057 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 149 reward tensor([-1], device='cuda:0') loss 1.6727042198181152 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 150 reward tensor([-1], device='cuda:0') loss 1.349027395248413 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 151 reward tensor([-1], device='cuda:0') loss 1.2295055389404297 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 152 reward tensor([-1], device='cuda:0') loss 1.2222720384597778 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 153 reward tensor([-1], device='cuda:0') loss 0.9155644774436951 epsilon 0.767566125
26-Feb-25 13:31:17 - agent.DQN.DQN - INFO - episode 3 step 154 reward tensor([-1], device='cuda:0') loss 0.7826220989227295 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 155 reward tensor([-1], device='cuda:0') loss 1.0273070335388184 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 156 reward tensor([-1], device='cuda:0') loss 0.7331128120422363 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 157 reward tensor([-1], device='cuda:0') loss 0.9601144790649414 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 158 reward tensor([-1], device='cuda:0') loss 0.9206505417823792 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 159 reward tensor([-1], device='cuda:0') loss 6.765292167663574 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 160 reward tensor([-1], device='cuda:0') loss 0.9712638854980469 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 161 reward tensor([-1], device='cuda:0') loss 1.0906544923782349 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 162 reward tensor([-1], device='cuda:0') loss 1.6341333389282227 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 163 reward tensor([-1], device='cuda:0') loss 1.5595662593841553 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 164 reward tensor([-1], device='cuda:0') loss 1.6685746908187866 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 165 reward tensor([-1], device='cuda:0') loss 1.1335951089859009 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 166 reward tensor([-1], device='cuda:0') loss 1.7912698984146118 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 167 reward tensor([-1], device='cuda:0') loss 0.7908599376678467 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 168 reward tensor([-1], device='cuda:0') loss 4.512236595153809 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 169 reward tensor([-1], device='cuda:0') loss 0.46139851212501526 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 170 reward tensor([-1], device='cuda:0') loss 0.7000771760940552 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 171 reward tensor([-1], device='cuda:0') loss 1.4287251234054565 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 172 reward tensor([-1], device='cuda:0') loss 0.7717574238777161 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 173 reward tensor([-1], device='cuda:0') loss 1.3602991104125977 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 174 reward tensor([-1], device='cuda:0') loss 2.2162561416625977 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 175 reward tensor([-1], device='cuda:0') loss 1.1786994934082031 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 176 reward tensor([-1], device='cuda:0') loss 1.6318044662475586 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 177 reward tensor([-1], device='cuda:0') loss 2.409085512161255 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 178 reward tensor([-1], device='cuda:0') loss 1.3909687995910645 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 179 reward tensor([-1], device='cuda:0') loss 1.3778067827224731 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 180 reward tensor([-1], device='cuda:0') loss 1.6421726942062378 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 181 reward tensor([-1], device='cuda:0') loss 1.1356697082519531 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 182 reward tensor([-1], device='cuda:0') loss 1.2999470233917236 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 183 reward tensor([-1], device='cuda:0') loss 1.0614192485809326 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 184 reward tensor([-1], device='cuda:0') loss 0.7105242609977722 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 185 reward tensor([-1], device='cuda:0') loss 4.71628475189209 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 186 reward tensor([-1], device='cuda:0') loss 0.6925363540649414 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 187 reward tensor([-1], device='cuda:0') loss 0.6137216091156006 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 188 reward tensor([-1], device='cuda:0') loss 0.7570885419845581 epsilon 0.767566125
26-Feb-25 13:31:18 - agent.DQN.DQN - INFO - episode 3 step 189 reward tensor([-1], device='cuda:0') loss 0.5765784978866577 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 190 reward tensor([-1], device='cuda:0') loss 0.8706930875778198 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 191 reward tensor([-1], device='cuda:0') loss 3.872357130050659 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 192 reward tensor([-1], device='cuda:0') loss 3.504006862640381 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 193 reward tensor([-1], device='cuda:0') loss 3.549520492553711 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 194 reward tensor([-1], device='cuda:0') loss 1.0161758661270142 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 195 reward tensor([-1], device='cuda:0') loss 0.8020491600036621 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 196 reward tensor([-1], device='cuda:0') loss 3.2981276512145996 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 197 reward tensor([-1], device='cuda:0') loss 0.9556595087051392 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 198 reward tensor([-1], device='cuda:0') loss 1.4531121253967285 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 199 reward tensor([-1], device='cuda:0') loss 1.3796781301498413 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 200 reward tensor([-1], device='cuda:0') loss 0.8221882581710815 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 201 reward tensor([-1], device='cuda:0') loss 1.4948939085006714 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 202 reward tensor([-1], device='cuda:0') loss 0.7113179564476013 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 203 reward tensor([-1], device='cuda:0') loss 3.757803201675415 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 204 reward tensor([-1], device='cuda:0') loss 3.9523074626922607 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 205 reward tensor([-1], device='cuda:0') loss 0.580748438835144 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 206 reward tensor([-1], device='cuda:0') loss 0.6898549795150757 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 207 reward tensor([-1], device='cuda:0') loss 5.141014099121094 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 208 reward tensor([-1], device='cuda:0') loss 1.505932092666626 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 209 reward tensor([-1], device='cuda:0') loss 1.1640387773513794 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 210 reward tensor([-1], device='cuda:0') loss 0.9009914994239807 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 211 reward tensor([-1], device='cuda:0') loss 0.9717617034912109 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 212 reward tensor([-1], device='cuda:0') loss 0.6558822393417358 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 213 reward tensor([-1], device='cuda:0') loss 0.912810206413269 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 214 reward tensor([-1], device='cuda:0') loss 0.7433340549468994 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 215 reward tensor([-1], device='cuda:0') loss 0.7953722476959229 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 216 reward tensor([-1], device='cuda:0') loss 0.5651437044143677 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 217 reward tensor([-1], device='cuda:0') loss 0.9329150915145874 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 218 reward tensor([-1], device='cuda:0') loss 0.5795121788978577 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 219 reward tensor([-1], device='cuda:0') loss 3.348414421081543 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 220 reward tensor([-1], device='cuda:0') loss 0.45543578267097473 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 221 reward tensor([-1], device='cuda:0') loss 0.5548526048660278 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 222 reward tensor([-1], device='cuda:0') loss 0.7716401815414429 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 223 reward tensor([-1], device='cuda:0') loss 3.273545742034912 epsilon 0.767566125
26-Feb-25 13:31:19 - agent.DQN.DQN - INFO - episode 3 step 224 reward tensor([-1], device='cuda:0') loss 0.48689621686935425 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 225 reward tensor([-1], device='cuda:0') loss 0.6220484972000122 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 226 reward tensor([-1], device='cuda:0') loss 0.3619764447212219 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 227 reward tensor([-1], device='cuda:0') loss 0.5438928604125977 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 228 reward tensor([-1], device='cuda:0') loss 0.5188246965408325 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 229 reward tensor([-1], device='cuda:0') loss 0.36564508080482483 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 230 reward tensor([-1], device='cuda:0') loss 0.39664676785469055 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 231 reward tensor([-1], device='cuda:0') loss 3.191026449203491 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 232 reward tensor([-1], device='cuda:0') loss 0.45192083716392517 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 233 reward tensor([-1], device='cuda:0') loss 0.5384201407432556 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 234 reward tensor([-1], device='cuda:0') loss 0.3401910662651062 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 235 reward tensor([-1], device='cuda:0') loss 0.5772678256034851 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 236 reward tensor([-1], device='cuda:0') loss 0.4501189887523651 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 237 reward tensor([-1], device='cuda:0') loss 1.8412370681762695 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 238 reward tensor([-1], device='cuda:0') loss 2.2666964530944824 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 239 reward tensor([-1], device='cuda:0') loss 4.189823150634766 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 240 reward tensor([-1], device='cuda:0') loss 1.2850124835968018 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 241 reward tensor([-1], device='cuda:0') loss 1.058327555656433 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 242 reward tensor([-1], device='cuda:0') loss 0.5756288766860962 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 243 reward tensor([-1], device='cuda:0') loss 0.5723142027854919 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 244 reward tensor([-1], device='cuda:0') loss 0.8690673112869263 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 245 reward tensor([-1], device='cuda:0') loss 2.550896406173706 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 246 reward tensor([-1], device='cuda:0') loss 2.298689365386963 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 247 reward tensor([-1], device='cuda:0') loss 0.8172385692596436 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 248 reward tensor([-1], device='cuda:0') loss 0.8055009841918945 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 249 reward tensor([-1], device='cuda:0') loss 1.1758447885513306 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 250 reward tensor([-1], device='cuda:0') loss 0.8145083785057068 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 251 reward tensor([-1], device='cuda:0') loss 0.6969127058982849 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 252 reward tensor([-1], device='cuda:0') loss 0.6068752408027649 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 253 reward tensor([-1], device='cuda:0') loss 0.5834280848503113 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 254 reward tensor([-1], device='cuda:0') loss 2.848755359649658 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 255 reward tensor([-1], device='cuda:0') loss 0.5224360227584839 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 256 reward tensor([-1], device='cuda:0') loss 0.37130221724510193 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 257 reward tensor([-1], device='cuda:0') loss 0.5203069448471069 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 258 reward tensor([-1], device='cuda:0') loss 0.44010889530181885 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 259 reward tensor([-1], device='cuda:0') loss 0.33501166105270386 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 260 reward tensor([-1], device='cuda:0') loss 0.4379393458366394 epsilon 0.767566125
26-Feb-25 13:31:20 - agent.DQN.DQN - INFO - episode 3 step 261 reward tensor([-1], device='cuda:0') loss 0.4607154130935669 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 262 reward tensor([-1], device='cuda:0') loss 2.4410037994384766 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 263 reward tensor([-1], device='cuda:0') loss 0.38576996326446533 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 264 reward tensor([-1], device='cuda:0') loss 0.4414471983909607 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 265 reward tensor([-1], device='cuda:0') loss 0.3363393545150757 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 266 reward tensor([-1], device='cuda:0') loss 0.39591121673583984 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 267 reward tensor([-1], device='cuda:0') loss 2.0239033699035645 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 268 reward tensor([-1], device='cuda:0') loss 1.5527878999710083 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 269 reward tensor([-1], device='cuda:0') loss 1.1578490734100342 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 270 reward tensor([-1], device='cuda:0') loss 1.1623103618621826 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 271 reward tensor([-1], device='cuda:0') loss 2.145832061767578 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 272 reward tensor([-1], device='cuda:0') loss 0.602604866027832 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 273 reward tensor([-1], device='cuda:0') loss 0.46349695324897766 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 274 reward tensor([-1], device='cuda:0') loss 0.5871533751487732 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 275 reward tensor([-1], device='cuda:0') loss 1.987350344657898 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 276 reward tensor([-1], device='cuda:0') loss 1.8501287698745728 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 277 reward tensor([-1], device='cuda:0') loss 0.8915721774101257 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 278 reward tensor([-1], device='cuda:0') loss 0.9512356519699097 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 279 reward tensor([-1], device='cuda:0') loss 1.8973126411437988 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 280 reward tensor([-1], device='cuda:0') loss 0.5687246918678284 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 281 reward tensor([-1], device='cuda:0') loss 0.8314437866210938 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 282 reward tensor([-1], device='cuda:0') loss 0.5280516147613525 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 283 reward tensor([-1], device='cuda:0') loss 0.593593955039978 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 284 reward tensor([-1], device='cuda:0') loss 0.3254780173301697 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 285 reward tensor([-1], device='cuda:0') loss 0.47616931796073914 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 286 reward tensor([-1], device='cuda:0') loss 0.4161235988140106 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 287 reward tensor([-1], device='cuda:0') loss 2.0353775024414062 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 288 reward tensor([-1], device='cuda:0') loss 0.37772685289382935 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 289 reward tensor([-1], device='cuda:0') loss 2.099351644515991 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 290 reward tensor([-1], device='cuda:0') loss 0.3539588749408722 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 291 reward tensor([-1], device='cuda:0') loss 0.6123877167701721 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 292 reward tensor([-1], device='cuda:0') loss 0.4940406382083893 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 293 reward tensor([-1], device='cuda:0') loss 1.891047716140747 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 294 reward tensor([-1], device='cuda:0') loss 0.6892070174217224 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 295 reward tensor([-1], device='cuda:0') loss 0.5454498529434204 epsilon 0.767566125
26-Feb-25 13:31:21 - agent.DQN.DQN - INFO - episode 3 step 296 reward tensor([-1], device='cuda:0') loss 0.6313539743423462 epsilon 0.767566125
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 3 step 297 reward tensor([-1], device='cuda:0') loss 2.0237646102905273 epsilon 0.767566125
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 3 step 298 reward tensor([-1], device='cuda:0') loss 1.8002591133117676 epsilon 0.767566125
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 3 step 299 reward tensor([-1], device='cuda:0') loss 1.346768856048584 epsilon 0.767566125
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 3 step 300 reward tensor([-1], device='cuda:0') loss 0.9920445680618286 epsilon 0.767566125
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 3 step 301 reward tensor([-1], device='cuda:0') loss 0.7931368350982666 epsilon 0.767566125
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 0 reward tensor([-1], device='cuda:0') loss 0.5753564834594727 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 1 reward tensor([-1], device='cuda:0') loss 0.4587630033493042 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 2 reward tensor([-1], device='cuda:0') loss 0.5730511546134949 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 3 reward tensor([-1], device='cuda:0') loss 0.404439240694046 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 4 reward tensor([-1], device='cuda:0') loss 0.6032207608222961 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 5 reward tensor([-1], device='cuda:0') loss 0.609107255935669 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 6 reward tensor([-1], device='cuda:0') loss 0.6045657396316528 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 7 reward tensor([-1], device='cuda:0') loss 0.5767165422439575 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 8 reward tensor([-1], device='cuda:0') loss 0.844170868396759 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 9 reward tensor([-1], device='cuda:0') loss 0.6756244897842407 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 10 reward tensor([-1], device='cuda:0') loss 0.5976394414901733 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 11 reward tensor([-1], device='cuda:0') loss 0.46572571992874146 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 12 reward tensor([-1], device='cuda:0') loss 2.236435890197754 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 13 reward tensor([-1], device='cuda:0') loss 0.35447821021080017 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 14 reward tensor([-1], device='cuda:0') loss 0.4121289849281311 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 15 reward tensor([-1], device='cuda:0') loss 0.3266671299934387 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 16 reward tensor([-1], device='cuda:0') loss 0.2846909165382385 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 17 reward tensor([-1], device='cuda:0') loss 0.29508882761001587 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 18 reward tensor([-1], device='cuda:0') loss 0.3639926314353943 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 19 reward tensor([-1], device='cuda:0') loss 1.860262393951416 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 20 reward tensor([-1], device='cuda:0') loss 0.34962913393974304 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 21 reward tensor([-1], device='cuda:0') loss 0.2971060574054718 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 22 reward tensor([-1], device='cuda:0') loss 1.8747100830078125 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 23 reward tensor([-1], device='cuda:0') loss 5.24204158782959 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 24 reward tensor([-1], device='cuda:0') loss 0.3048296570777893 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 25 reward tensor([-1], device='cuda:0') loss 1.6041089296340942 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 26 reward tensor([-1], device='cuda:0') loss 5.534626007080078 epsilon 0.7426202259375
26-Feb-25 13:31:22 - agent.DQN.DQN - INFO - episode 4 step 27 reward tensor([-1], device='cuda:0') loss 1.2335600852966309 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 28 reward tensor([-1], device='cuda:0') loss 0.9467045068740845 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 29 reward tensor([-1], device='cuda:0') loss 1.7599397897720337 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 30 reward tensor([-1], device='cuda:0') loss 0.6505509614944458 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 31 reward tensor([-1], device='cuda:0') loss 0.5490025877952576 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 32 reward tensor([-1], device='cuda:0') loss 0.5904474258422852 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 33 reward tensor([-1], device='cuda:0') loss 0.6462273597717285 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 34 reward tensor([-1], device='cuda:0') loss 0.8641481995582581 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 35 reward tensor([-1], device='cuda:0') loss 0.9166862964630127 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 36 reward tensor([-1], device='cuda:0') loss 0.9498510956764221 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 37 reward tensor([-1], device='cuda:0') loss 1.0060358047485352 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 38 reward tensor([-1], device='cuda:0') loss 0.5012500286102295 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 39 reward tensor([-1], device='cuda:0') loss 0.7154354453086853 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 40 reward tensor([-1], device='cuda:0') loss 0.682275652885437 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 41 reward tensor([-1], device='cuda:0') loss 0.5477732419967651 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 42 reward tensor([-1], device='cuda:0') loss 3.6457326412200928 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 43 reward tensor([-1], device='cuda:0') loss 3.7869763374328613 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 44 reward tensor([-1], device='cuda:0') loss 0.42494264245033264 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 45 reward tensor([-1], device='cuda:0') loss 0.4865124523639679 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 46 reward tensor([-1], device='cuda:0') loss 1.8482753038406372 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 47 reward tensor([-1], device='cuda:0') loss 0.35048359632492065 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 48 reward tensor([-1], device='cuda:0') loss 0.47754818201065063 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 49 reward tensor([-1], device='cuda:0') loss 0.47482359409332275 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 50 reward tensor([-1], device='cuda:0') loss 0.5375460982322693 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 51 reward tensor([-1], device='cuda:0') loss 4.270824432373047 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 52 reward tensor([-1], device='cuda:0') loss 0.5740227699279785 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 53 reward tensor([-1], device='cuda:0') loss 0.4594188630580902 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 54 reward tensor([-1], device='cuda:0') loss 0.46153557300567627 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 55 reward tensor([-1], device='cuda:0') loss 1.2739211320877075 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 56 reward tensor([-1], device='cuda:0') loss 2.0034711360931396 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 57 reward tensor([-1], device='cuda:0') loss 0.7806911468505859 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 58 reward tensor([-1], device='cuda:0') loss 0.6964543461799622 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 59 reward tensor([-1], device='cuda:0') loss 0.5946698188781738 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 60 reward tensor([-1], device='cuda:0') loss 1.3021502494812012 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 61 reward tensor([-1], device='cuda:0') loss 0.553860604763031 epsilon 0.7426202259375
26-Feb-25 13:31:23 - agent.DQN.DQN - INFO - episode 4 step 62 reward tensor([-1], device='cuda:0') loss 3.163132905960083 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 63 reward tensor([-1], device='cuda:0') loss 0.9750823974609375 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 64 reward tensor([-1], device='cuda:0') loss 0.902991533279419 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 65 reward tensor([-1], device='cuda:0') loss 1.1981215476989746 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 66 reward tensor([-1], device='cuda:0') loss 0.716291069984436 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 67 reward tensor([-1], device='cuda:0') loss 2.5588903427124023 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 68 reward tensor([-1], device='cuda:0') loss 0.5831908583641052 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 69 reward tensor([-1], device='cuda:0') loss 2.1658873558044434 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 70 reward tensor([-1], device='cuda:0') loss 0.521583080291748 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 71 reward tensor([-1], device='cuda:0') loss 1.4070554971694946 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 72 reward tensor([-1], device='cuda:0') loss 0.5751034021377563 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 73 reward tensor([-1], device='cuda:0') loss 1.9042017459869385 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 74 reward tensor([-1], device='cuda:0') loss 0.6492705345153809 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 75 reward tensor([-1], device='cuda:0') loss 0.8448540568351746 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 76 reward tensor([-1], device='cuda:0') loss 0.6015375256538391 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 77 reward tensor([-1], device='cuda:0') loss 1.9418370723724365 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 78 reward tensor([-1], device='cuda:0') loss 0.5264527797698975 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 79 reward tensor([-1], device='cuda:0') loss 0.45900675654411316 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 80 reward tensor([-1], device='cuda:0') loss 2.0875022411346436 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 81 reward tensor([-1], device='cuda:0') loss 2.1224610805511475 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 82 reward tensor([-1], device='cuda:0') loss 0.4378313422203064 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 83 reward tensor([-1], device='cuda:0') loss 0.5882078409194946 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 84 reward tensor([-1], device='cuda:0') loss 0.44129037857055664 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 85 reward tensor([-1], device='cuda:0') loss 0.9724207520484924 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 86 reward tensor([-1], device='cuda:0') loss 2.644881248474121 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 87 reward tensor([-1], device='cuda:0') loss 0.8984630107879639 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 88 reward tensor([-1], device='cuda:0') loss 0.7807775139808655 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 89 reward tensor([-1], device='cuda:0') loss 1.4377645254135132 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 90 reward tensor([-1], device='cuda:0') loss 0.4186200499534607 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 91 reward tensor([-1], device='cuda:0') loss 0.47508785128593445 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 92 reward tensor([-1], device='cuda:0') loss 0.3480052351951599 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 93 reward tensor([-1], device='cuda:0') loss 1.9659453630447388 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 94 reward tensor([-1], device='cuda:0') loss 0.4296428859233856 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 95 reward tensor([-1], device='cuda:0') loss 0.9887181520462036 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 96 reward tensor([-1], device='cuda:0') loss 2.2283761501312256 epsilon 0.7426202259375
26-Feb-25 13:31:24 - agent.DQN.DQN - INFO - episode 4 step 97 reward tensor([-1], device='cuda:0') loss 0.6518539190292358 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 98 reward tensor([-1], device='cuda:0') loss 1.0690361261367798 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 99 reward tensor([-1], device='cuda:0') loss 0.5326796770095825 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 100 reward tensor([-1], device='cuda:0') loss 0.5518261790275574 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 101 reward tensor([-1], device='cuda:0') loss 0.43076464533805847 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 102 reward tensor([-1], device='cuda:0') loss 0.4087529480457306 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 103 reward tensor([-1], device='cuda:0') loss 0.3370766043663025 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 104 reward tensor([-1], device='cuda:0') loss 1.1196271181106567 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 105 reward tensor([-1], device='cuda:0') loss 0.9969736337661743 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 106 reward tensor([-1], device='cuda:0') loss 0.30249300599098206 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 107 reward tensor([-1], device='cuda:0') loss 0.3326302170753479 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 108 reward tensor([-1], device='cuda:0') loss 0.29205915331840515 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 109 reward tensor([-1], device='cuda:0') loss 0.29775023460388184 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 110 reward tensor([-1], device='cuda:0') loss 0.3298138678073883 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 111 reward tensor([-1], device='cuda:0') loss 0.9464751482009888 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 112 reward tensor([-1], device='cuda:0') loss 0.9985603094100952 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 113 reward tensor([-1], device='cuda:0') loss 0.28205376863479614 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 114 reward tensor([-1], device='cuda:0') loss 0.3919067978858948 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 115 reward tensor([-1], device='cuda:0') loss 0.8143688440322876 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 116 reward tensor([-1], device='cuda:0') loss 0.8823571801185608 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 117 reward tensor([-1], device='cuda:0') loss 0.8185296058654785 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 118 reward tensor([-1], device='cuda:0') loss 0.5338406562805176 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 119 reward tensor([-1], device='cuda:0') loss 0.6206161379814148 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 120 reward tensor([-1], device='cuda:0') loss 0.7867387533187866 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 121 reward tensor([-1], device='cuda:0') loss 0.3839429020881653 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 122 reward tensor([-1], device='cuda:0') loss 0.41109853982925415 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 123 reward tensor([-1], device='cuda:0') loss 0.794750452041626 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 124 reward tensor([-1], device='cuda:0') loss 0.28360843658447266 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 125 reward tensor([-1], device='cuda:0') loss 0.35694706439971924 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 126 reward tensor([-1], device='cuda:0') loss 0.31488263607025146 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 127 reward tensor([-1], device='cuda:0') loss 0.5987370014190674 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 128 reward tensor([-1], device='cuda:0') loss 0.40077584981918335 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 129 reward tensor([-1], device='cuda:0') loss 0.41922134160995483 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 130 reward tensor([-1], device='cuda:0') loss 0.3007156550884247 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 131 reward tensor([-1], device='cuda:0') loss 0.9635944962501526 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 132 reward tensor([-1], device='cuda:0') loss 0.9543633460998535 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 133 reward tensor([-1], device='cuda:0') loss 0.28290843963623047 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 134 reward tensor([-1], device='cuda:0') loss 0.4020766615867615 epsilon 0.7426202259375
26-Feb-25 13:31:25 - agent.DQN.DQN - INFO - episode 4 step 135 reward tensor([-1], device='cuda:0') loss 0.41202399134635925 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 136 reward tensor([-1], device='cuda:0') loss 0.34937769174575806 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 137 reward tensor([-1], device='cuda:0') loss 0.3077065348625183 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 138 reward tensor([-1], device='cuda:0') loss 0.3447727560997009 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 139 reward tensor([-1], device='cuda:0') loss 0.31764742732048035 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 140 reward tensor([-1], device='cuda:0') loss 0.41023680567741394 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 141 reward tensor([-1], device='cuda:0') loss 0.30565059185028076 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 142 reward tensor([-1], device='cuda:0') loss 0.26654544472694397 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 143 reward tensor([-1], device='cuda:0') loss 0.28417715430259705 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 144 reward tensor([-1], device='cuda:0') loss 1.8242627382278442 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 145 reward tensor([-1], device='cuda:0') loss 0.9864131212234497 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 146 reward tensor([-1], device='cuda:0') loss 2.1746392250061035 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 147 reward tensor([-1], device='cuda:0') loss 0.9168392419815063 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 148 reward tensor([-1], device='cuda:0') loss 0.6708934307098389 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 149 reward tensor([-1], device='cuda:0') loss 0.5575202703475952 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 150 reward tensor([-1], device='cuda:0') loss 0.5822352170944214 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 151 reward tensor([-1], device='cuda:0') loss 0.45822224020957947 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 152 reward tensor([-1], device='cuda:0') loss 0.45661091804504395 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 153 reward tensor([-1], device='cuda:0') loss 0.3209124505519867 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 154 reward tensor([-1], device='cuda:0') loss 0.6505467891693115 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 155 reward tensor([-1], device='cuda:0') loss 0.963568389415741 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 156 reward tensor([-1], device='cuda:0') loss 0.514896035194397 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 157 reward tensor([-1], device='cuda:0') loss 0.5063295364379883 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 158 reward tensor([-1], device='cuda:0') loss 0.4971506893634796 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 159 reward tensor([-1], device='cuda:0') loss 0.41461509466171265 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 160 reward tensor([-1], device='cuda:0') loss 0.910912275314331 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 161 reward tensor([-1], device='cuda:0') loss 0.38160640001296997 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 162 reward tensor([-1], device='cuda:0') loss 0.5209254026412964 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 163 reward tensor([-1], device='cuda:0') loss 0.3180170953273773 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 164 reward tensor([-1], device='cuda:0') loss 0.570814311504364 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 165 reward tensor([-1], device='cuda:0') loss 0.5417659282684326 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 166 reward tensor([-1], device='cuda:0') loss 0.5897350311279297 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 167 reward tensor([-1], device='cuda:0') loss 0.47967979311943054 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 168 reward tensor([-1], device='cuda:0') loss 0.41710007190704346 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 169 reward tensor([-1], device='cuda:0') loss 0.8398118019104004 epsilon 0.7426202259375
26-Feb-25 13:31:26 - agent.DQN.DQN - INFO - episode 4 step 170 reward tensor([-1], device='cuda:0') loss 1.4340059757232666 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 171 reward tensor([-1], device='cuda:0') loss 0.3348304033279419 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 172 reward tensor([-1], device='cuda:0') loss 0.439823180437088 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 173 reward tensor([-1], device='cuda:0') loss 0.45638829469680786 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 174 reward tensor([-1], device='cuda:0') loss 0.39480888843536377 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 175 reward tensor([-1], device='cuda:0') loss 1.0122556686401367 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 176 reward tensor([-1], device='cuda:0') loss 1.1895254850387573 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 177 reward tensor([-1], device='cuda:0') loss 0.6838780045509338 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 178 reward tensor([-1], device='cuda:0') loss 0.618284285068512 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 179 reward tensor([-1], device='cuda:0') loss 0.6956058740615845 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 180 reward tensor([-1], device='cuda:0') loss 0.4384777843952179 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 181 reward tensor([-1], device='cuda:0') loss 0.623332679271698 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 182 reward tensor([-1], device='cuda:0') loss 1.3700991868972778 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 183 reward tensor([-1], device='cuda:0') loss 0.5271915197372437 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 184 reward tensor([-1], device='cuda:0') loss 0.6952959299087524 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 185 reward tensor([-1], device='cuda:0') loss 0.5228418707847595 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 186 reward tensor([-1], device='cuda:0') loss 0.5457195043563843 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 187 reward tensor([-1], device='cuda:0') loss 0.5845493078231812 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 188 reward tensor([-1], device='cuda:0') loss 0.6018478870391846 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 189 reward tensor([-1], device='cuda:0') loss 0.48654067516326904 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 190 reward tensor([-1], device='cuda:0') loss 0.4960908889770508 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 191 reward tensor([-1], device='cuda:0') loss 0.5818823575973511 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 192 reward tensor([-1], device='cuda:0') loss 1.604247808456421 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 193 reward tensor([-1], device='cuda:0') loss 0.41268783807754517 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 194 reward tensor([-1], device='cuda:0') loss 0.34417691826820374 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 195 reward tensor([-1], device='cuda:0') loss 0.35790032148361206 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 196 reward tensor([-1], device='cuda:0') loss 0.33298051357269287 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 197 reward tensor([-1], device='cuda:0') loss 0.5266749858856201 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 198 reward tensor([-1], device='cuda:0') loss 0.4455038011074066 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 199 reward tensor([-1], device='cuda:0') loss 1.188307762145996 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 200 reward tensor([-1], device='cuda:0') loss 0.3415347933769226 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 201 reward tensor([-1], device='cuda:0') loss 0.27609002590179443 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 202 reward tensor([-1], device='cuda:0') loss 0.6565754413604736 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 203 reward tensor([-1], device='cuda:0') loss 0.32567089796066284 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 204 reward tensor([-1], device='cuda:0') loss 0.558999240398407 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 205 reward tensor([-1], device='cuda:0') loss 0.6416724920272827 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 206 reward tensor([-1], device='cuda:0') loss 0.7195837497711182 epsilon 0.7426202259375
26-Feb-25 13:31:27 - agent.DQN.DQN - INFO - episode 4 step 207 reward tensor([-1], device='cuda:0') loss 0.584376335144043 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 208 reward tensor([-1], device='cuda:0') loss 0.6190913915634155 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 209 reward tensor([-1], device='cuda:0') loss 0.5267668962478638 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 210 reward tensor([-1], device='cuda:0') loss 0.4145662784576416 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 211 reward tensor([-1], device='cuda:0') loss 0.5601257085800171 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 212 reward tensor([-1], device='cuda:0') loss 0.49032062292099 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 213 reward tensor([-1], device='cuda:0') loss 1.0217030048370361 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 214 reward tensor([-1], device='cuda:0') loss 0.41369807720184326 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 215 reward tensor([-1], device='cuda:0') loss 0.31042689085006714 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 216 reward tensor([-1], device='cuda:0') loss 0.7791658043861389 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 217 reward tensor([-1], device='cuda:0') loss 0.3538050353527069 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 218 reward tensor([-1], device='cuda:0') loss 0.43783125281333923 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 219 reward tensor([-1], device='cuda:0') loss 0.3820236325263977 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 220 reward tensor([-1], device='cuda:0') loss 0.4709634780883789 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 221 reward tensor([-1], device='cuda:0') loss 1.0059289932250977 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 222 reward tensor([-1], device='cuda:0') loss 0.4138641655445099 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 223 reward tensor([-1], device='cuda:0') loss 0.5099418759346008 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 224 reward tensor([-1], device='cuda:0') loss 0.295873761177063 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 225 reward tensor([-1], device='cuda:0') loss 0.3270392417907715 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 226 reward tensor([-1], device='cuda:0') loss 0.3963214159011841 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 227 reward tensor([-1], device='cuda:0') loss 0.4880474805831909 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 228 reward tensor([-1], device='cuda:0') loss 0.6032774448394775 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 229 reward tensor([-1], device='cuda:0') loss 0.3199331760406494 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 230 reward tensor([-1], device='cuda:0') loss 0.3781746029853821 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 231 reward tensor([-1], device='cuda:0') loss 0.4359874129295349 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 232 reward tensor([-1], device='cuda:0') loss 0.3247244954109192 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 233 reward tensor([-1], device='cuda:0') loss 0.5010647773742676 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 234 reward tensor([-1], device='cuda:0') loss 0.3007862865924835 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 235 reward tensor([-1], device='cuda:0') loss 0.7540116310119629 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 236 reward tensor([-1], device='cuda:0') loss 0.6920979022979736 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 237 reward tensor([-1], device='cuda:0') loss 1.0884424448013306 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 238 reward tensor([-1], device='cuda:0') loss 0.40314266085624695 epsilon 0.7426202259375
26-Feb-25 13:31:28 - agent.DQN.DQN - INFO - episode 4 step 239 reward tensor([-1], device='cuda:0') loss 0.6969909071922302 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 240 reward tensor([-1], device='cuda:0') loss 0.5570352673530579 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 241 reward tensor([-1], device='cuda:0') loss 1.1234678030014038 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 242 reward tensor([-1], device='cuda:0') loss 0.9327130913734436 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 243 reward tensor([-1], device='cuda:0') loss 0.5083929300308228 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 244 reward tensor([-1], device='cuda:0') loss 0.39882904291152954 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 245 reward tensor([-1], device='cuda:0') loss 0.5087870359420776 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 246 reward tensor([-1], device='cuda:0') loss 0.4165949821472168 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 247 reward tensor([-1], device='cuda:0') loss 0.3934357166290283 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 248 reward tensor([-1], device='cuda:0') loss 0.41315680742263794 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 249 reward tensor([-1], device='cuda:0') loss 0.4456474781036377 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 250 reward tensor([-1], device='cuda:0') loss 0.5351419448852539 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 251 reward tensor([-1], device='cuda:0') loss 0.5693904757499695 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 252 reward tensor([-1], device='cuda:0') loss 0.5663644075393677 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 253 reward tensor([-1], device='cuda:0') loss 0.7172083854675293 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 254 reward tensor([-1], device='cuda:0') loss 0.37675195932388306 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 255 reward tensor([-1], device='cuda:0') loss 0.5343926548957825 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 256 reward tensor([-1], device='cuda:0') loss 0.30560773611068726 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 257 reward tensor([-1], device='cuda:0') loss 0.5403778553009033 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 258 reward tensor([-1], device='cuda:0') loss 0.3723841607570648 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 259 reward tensor([-1], device='cuda:0') loss 0.6440760493278503 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 260 reward tensor([-1], device='cuda:0') loss 0.3204362094402313 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 261 reward tensor([-1], device='cuda:0') loss 0.43266618251800537 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 262 reward tensor([-1], device='cuda:0') loss 0.39493316411972046 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 263 reward tensor([-1], device='cuda:0') loss 0.3647547662258148 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 264 reward tensor([-1], device='cuda:0') loss 0.4496338665485382 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 265 reward tensor([-1], device='cuda:0') loss 0.781329333782196 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 266 reward tensor([-1], device='cuda:0') loss 0.9586327075958252 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 267 reward tensor([-1], device='cuda:0') loss 0.47742393612861633 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 268 reward tensor([-1], device='cuda:0') loss 0.4952136278152466 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 269 reward tensor([-1], device='cuda:0') loss 0.521470308303833 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 270 reward tensor([-1], device='cuda:0') loss 0.5804640054702759 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 271 reward tensor([-1], device='cuda:0') loss 0.38920050859451294 epsilon 0.7426202259375
26-Feb-25 13:31:29 - agent.DQN.DQN - INFO - episode 4 step 272 reward tensor([-1], device='cuda:0') loss 0.3994016647338867 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 273 reward tensor([-1], device='cuda:0') loss 0.5435868501663208 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 274 reward tensor([-1], device='cuda:0') loss 0.3089061975479126 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 275 reward tensor([-1], device='cuda:0') loss 0.5654017329216003 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 276 reward tensor([-1], device='cuda:0') loss 0.6250183582305908 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 277 reward tensor([-1], device='cuda:0') loss 0.4978332221508026 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 278 reward tensor([-1], device='cuda:0') loss 0.4342125654220581 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 279 reward tensor([-1], device='cuda:0') loss 0.38361331820487976 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 280 reward tensor([-1], device='cuda:0') loss 0.6855015754699707 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 281 reward tensor([-1], device='cuda:0') loss 0.4315676689147949 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 282 reward tensor([-1], device='cuda:0') loss 0.9893139004707336 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 283 reward tensor([-1], device='cuda:0') loss 0.5282667875289917 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 284 reward tensor([-1], device='cuda:0') loss 0.513884425163269 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 285 reward tensor([-1], device='cuda:0') loss 0.34898531436920166 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 286 reward tensor([-1], device='cuda:0') loss 0.4597242772579193 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 287 reward tensor([-1], device='cuda:0') loss 0.3657207489013672 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 288 reward tensor([-1], device='cuda:0') loss 0.6081122756004333 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 289 reward tensor([-1], device='cuda:0') loss 0.368927538394928 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 290 reward tensor([-1], device='cuda:0') loss 0.5992652177810669 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 291 reward tensor([-1], device='cuda:0') loss 0.5125582218170166 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 292 reward tensor([-1], device='cuda:0') loss 0.4916074872016907 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 293 reward tensor([-1], device='cuda:0') loss 0.539871096611023 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 294 reward tensor([-1], device='cuda:0') loss 0.3833332061767578 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 295 reward tensor([-1], device='cuda:0') loss 0.8674010038375854 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 296 reward tensor([-1], device='cuda:0') loss 0.6642906069755554 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 297 reward tensor([-1], device='cuda:0') loss 0.48570528626441956 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 298 reward tensor([-1], device='cuda:0') loss 0.5252444744110107 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 299 reward tensor([-1], device='cuda:0') loss 0.4636971354484558 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 300 reward tensor([-1], device='cuda:0') loss 0.6757022142410278 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 4 step 301 reward tensor([-1], device='cuda:0') loss 0.4069565534591675 epsilon 0.7426202259375
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 5 step 0 reward tensor([-1], device='cuda:0') loss 0.31382322311401367 epsilon 0.7184850685945312
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 5 step 1 reward tensor([-1], device='cuda:0') loss 0.37617868185043335 epsilon 0.7184850685945312
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 5 step 2 reward tensor([-1], device='cuda:0') loss 0.41773831844329834 epsilon 0.7184850685945312
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 5 step 3 reward tensor([-1], device='cuda:0') loss 0.3888739049434662 epsilon 0.7184850685945312
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 5 step 4 reward tensor([-1], device='cuda:0') loss 0.45258617401123047 epsilon 0.7184850685945312
26-Feb-25 13:31:30 - agent.DQN.DQN - INFO - episode 5 step 5 reward tensor([-1], device='cuda:0') loss 0.4925474226474762 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 6 reward tensor([-1], device='cuda:0') loss 0.45827487111091614 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 7 reward tensor([-1], device='cuda:0') loss 0.40446287393569946 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 8 reward tensor([-1], device='cuda:0') loss 0.47461652755737305 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 9 reward tensor([-1], device='cuda:0') loss 0.5374658107757568 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 10 reward tensor([-1], device='cuda:0') loss 0.41461339592933655 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 11 reward tensor([-1], device='cuda:0') loss 0.37374481558799744 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 12 reward tensor([-1], device='cuda:0') loss 0.2981944680213928 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 13 reward tensor([-1], device='cuda:0') loss 0.4661083221435547 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 14 reward tensor([-1], device='cuda:0') loss 0.3165413737297058 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 15 reward tensor([-1], device='cuda:0') loss 0.34159523248672485 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 16 reward tensor([-1], device='cuda:0') loss 0.49727675318717957 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 17 reward tensor([-1], device='cuda:0') loss 0.3411758244037628 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 18 reward tensor([-1], device='cuda:0') loss 0.4003834128379822 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 19 reward tensor([-1], device='cuda:0') loss 0.39960533380508423 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 20 reward tensor([-1], device='cuda:0') loss 0.28764671087265015 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 21 reward tensor([-1], device='cuda:0') loss 0.4022170901298523 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 22 reward tensor([-1], device='cuda:0') loss 0.2961865961551666 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 23 reward tensor([-1], device='cuda:0') loss 0.6273277997970581 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 24 reward tensor([-1], device='cuda:0') loss 0.6293668746948242 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 25 reward tensor([-1], device='cuda:0') loss 0.47411084175109863 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 26 reward tensor([-1], device='cuda:0') loss 0.4246484637260437 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 27 reward tensor([-1], device='cuda:0') loss 0.4782567620277405 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 28 reward tensor([-1], device='cuda:0') loss 0.3724595904350281 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 29 reward tensor([-1], device='cuda:0') loss 0.34165000915527344 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 30 reward tensor([-1], device='cuda:0') loss 0.42577049136161804 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 31 reward tensor([-1], device='cuda:0') loss 0.2776263356208801 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 32 reward tensor([-1], device='cuda:0') loss 0.362108051776886 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 33 reward tensor([-1], device='cuda:0') loss 0.3002248704433441 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 34 reward tensor([-1], device='cuda:0') loss 0.3095560073852539 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 35 reward tensor([-1], device='cuda:0') loss 0.4139608144760132 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 36 reward tensor([-1], device='cuda:0') loss 0.4394172728061676 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 37 reward tensor([-1], device='cuda:0') loss 0.26615405082702637 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 38 reward tensor([-1], device='cuda:0') loss 0.23680365085601807 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 39 reward tensor([-1], device='cuda:0') loss 0.3213449716567993 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 40 reward tensor([-1], device='cuda:0') loss 0.40728724002838135 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 41 reward tensor([-1], device='cuda:0') loss 0.2904098629951477 epsilon 0.7184850685945312
26-Feb-25 13:31:31 - agent.DQN.DQN - INFO - episode 5 step 42 reward tensor([-1], device='cuda:0') loss 0.3205868899822235 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 43 reward tensor([-1], device='cuda:0') loss 0.3376792073249817 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 44 reward tensor([-1], device='cuda:0') loss 0.30423104763031006 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 45 reward tensor([-1], device='cuda:0') loss 0.3646301031112671 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 46 reward tensor([-1], device='cuda:0') loss 0.3375701606273651 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 47 reward tensor([-1], device='cuda:0') loss 0.25811541080474854 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 48 reward tensor([-1], device='cuda:0') loss 0.2883014678955078 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 49 reward tensor([-1], device='cuda:0') loss 0.2794860899448395 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 50 reward tensor([-1], device='cuda:0') loss 0.24006721377372742 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 51 reward tensor([-1], device='cuda:0') loss 0.36298081278800964 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 52 reward tensor([-1], device='cuda:0') loss 0.24105283617973328 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 53 reward tensor([-1], device='cuda:0') loss 0.5007171630859375 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 54 reward tensor([-1], device='cuda:0') loss 0.5337787866592407 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 55 reward tensor([-1], device='cuda:0') loss 0.4298633337020874 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 56 reward tensor([-1], device='cuda:0') loss 0.3546934723854065 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 57 reward tensor([-1], device='cuda:0') loss 0.4043996334075928 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 58 reward tensor([-1], device='cuda:0') loss 0.3288934826850891 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 59 reward tensor([-1], device='cuda:0') loss 0.32000482082366943 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 60 reward tensor([-1], device='cuda:0') loss 0.2941281199455261 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 61 reward tensor([-1], device='cuda:0') loss 0.25237351655960083 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 62 reward tensor([-1], device='cuda:0') loss 0.32508715987205505 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 63 reward tensor([-1], device='cuda:0') loss 0.384624719619751 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 64 reward tensor([-1], device='cuda:0') loss 0.3423398733139038 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 65 reward tensor([-1], device='cuda:0') loss 0.37296152114868164 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 66 reward tensor([-1], device='cuda:0') loss 0.31109511852264404 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 67 reward tensor([-1], device='cuda:0') loss 0.30371391773223877 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 68 reward tensor([-1], device='cuda:0') loss 0.3091128468513489 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 69 reward tensor([-1], device='cuda:0') loss 0.4423412084579468 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 70 reward tensor([-1], device='cuda:0') loss 0.28029316663742065 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 71 reward tensor([-1], device='cuda:0') loss 0.23197424411773682 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 72 reward tensor([-1], device='cuda:0') loss 0.31808575987815857 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 73 reward tensor([-1], device='cuda:0') loss 0.2959900200366974 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 74 reward tensor([-1], device='cuda:0') loss 0.28704211115837097 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 75 reward tensor([-1], device='cuda:0') loss 0.22849299013614655 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 76 reward tensor([-1], device='cuda:0') loss 0.2667127549648285 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 77 reward tensor([-1], device='cuda:0') loss 0.2947559952735901 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 78 reward tensor([-1], device='cuda:0') loss 0.2896074652671814 epsilon 0.7184850685945312
26-Feb-25 13:31:32 - agent.DQN.DQN - INFO - episode 5 step 79 reward tensor([-1], device='cuda:0') loss 0.27633458375930786 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 80 reward tensor([-1], device='cuda:0') loss 0.31507667899131775 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 81 reward tensor([-1], device='cuda:0') loss 0.2304757982492447 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 82 reward tensor([-1], device='cuda:0') loss 0.3695846199989319 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 83 reward tensor([-1], device='cuda:0') loss 0.37425804138183594 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 84 reward tensor([-1], device='cuda:0') loss 0.44178444147109985 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 85 reward tensor([-1], device='cuda:0') loss 0.3682132363319397 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 86 reward tensor([-1], device='cuda:0') loss 0.38539260625839233 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 87 reward tensor([-1], device='cuda:0') loss 0.3226515054702759 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 88 reward tensor([-1], device='cuda:0') loss 0.32441896200180054 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 89 reward tensor([-1], device='cuda:0') loss 0.22892288863658905 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 90 reward tensor([-1], device='cuda:0') loss 0.32432106137275696 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 91 reward tensor([-1], device='cuda:0') loss 0.30377888679504395 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 92 reward tensor([-1], device='cuda:0') loss 0.2578558921813965 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 93 reward tensor([-1], device='cuda:0') loss 0.31917065382003784 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 94 reward tensor([-1], device='cuda:0') loss 0.3629329800605774 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 95 reward tensor([-1], device='cuda:0') loss 0.3739786148071289 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 96 reward tensor([-1], device='cuda:0') loss 0.21814213693141937 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 97 reward tensor([-1], device='cuda:0') loss 0.44102713465690613 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 98 reward tensor([-1], device='cuda:0') loss 0.36771684885025024 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 99 reward tensor([-1], device='cuda:0') loss 0.33010995388031006 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 100 reward tensor([-1], device='cuda:0') loss 0.279366135597229 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 101 reward tensor([-1], device='cuda:0') loss 0.27792373299598694 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 102 reward tensor([-1], device='cuda:0') loss 0.3235316276550293 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 103 reward tensor([-1], device='cuda:0') loss 0.2730533182621002 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 104 reward tensor([-1], device='cuda:0') loss 0.36343929171562195 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 105 reward tensor([-1], device='cuda:0') loss 0.2829310894012451 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 106 reward tensor([-1], device='cuda:0') loss 0.33965620398521423 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 107 reward tensor([-1], device='cuda:0') loss 0.2621280550956726 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 108 reward tensor([-1], device='cuda:0') loss 0.37797826528549194 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 109 reward tensor([-1], device='cuda:0') loss 0.2350119650363922 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 110 reward tensor([-1], device='cuda:0') loss 0.3232882022857666 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 111 reward tensor([-1], device='cuda:0') loss 0.2973853051662445 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 112 reward tensor([-1], device='cuda:0') loss 0.2770370543003082 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 113 reward tensor([-1], device='cuda:0') loss 0.3872925043106079 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 114 reward tensor([-1], device='cuda:0') loss 0.4374890625476837 epsilon 0.7184850685945312
26-Feb-25 13:31:33 - agent.DQN.DQN - INFO - episode 5 step 115 reward tensor([-1], device='cuda:0') loss 0.37959951162338257 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 116 reward tensor([-1], device='cuda:0') loss 0.3908929228782654 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 117 reward tensor([-1], device='cuda:0') loss 0.3665202260017395 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 118 reward tensor([-1], device='cuda:0') loss 0.3019624948501587 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 119 reward tensor([-1], device='cuda:0') loss 0.2629513740539551 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 120 reward tensor([-1], device='cuda:0') loss 0.2709692120552063 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 121 reward tensor([-1], device='cuda:0') loss 0.2721596360206604 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 122 reward tensor([-1], device='cuda:0') loss 0.2758568227291107 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 123 reward tensor([-1], device='cuda:0') loss 0.24350789189338684 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 124 reward tensor([-1], device='cuda:0') loss 0.3282161355018616 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 125 reward tensor([-1], device='cuda:0') loss 0.26474928855895996 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 126 reward tensor([-1], device='cuda:0') loss 0.30962350964546204 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 127 reward tensor([-1], device='cuda:0') loss 0.2806542217731476 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 128 reward tensor([-1], device='cuda:0') loss 0.33524954319000244 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 129 reward tensor([-1], device='cuda:0') loss 0.28596246242523193 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 130 reward tensor([-1], device='cuda:0') loss 0.24604351818561554 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 131 reward tensor([-1], device='cuda:0') loss 0.29797863960266113 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 132 reward tensor([-1], device='cuda:0') loss 0.20816627144813538 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 133 reward tensor([-1], device='cuda:0') loss 0.3190951347351074 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 134 reward tensor([-1], device='cuda:0') loss 0.22988536953926086 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 135 reward tensor([-1], device='cuda:0') loss 0.21303221583366394 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 136 reward tensor([-1], device='cuda:0') loss 0.25824281573295593 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 137 reward tensor([-1], device='cuda:0') loss 0.3380717933177948 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 138 reward tensor([-1], device='cuda:0') loss 0.22664934396743774 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 139 reward tensor([-1], device='cuda:0') loss 0.3150976002216339 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 140 reward tensor([-1], device='cuda:0') loss 0.22554819285869598 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 141 reward tensor([-1], device='cuda:0') loss 0.27491244673728943 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 142 reward tensor([-1], device='cuda:0') loss 0.28461551666259766 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 143 reward tensor([-1], device='cuda:0') loss 0.42762571573257446 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 144 reward tensor([-1], device='cuda:0') loss 0.4091004431247711 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 145 reward tensor([-1], device='cuda:0') loss 0.3603407144546509 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 146 reward tensor([-1], device='cuda:0') loss 0.344015896320343 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 147 reward tensor([-1], device='cuda:0') loss 0.2716089189052582 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 148 reward tensor([-1], device='cuda:0') loss 0.3409377336502075 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 149 reward tensor([-1], device='cuda:0') loss 0.32338598370552063 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 150 reward tensor([-1], device='cuda:0') loss 0.258014440536499 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 151 reward tensor([-1], device='cuda:0') loss 0.33927100896835327 epsilon 0.7184850685945312
26-Feb-25 13:31:34 - agent.DQN.DQN - INFO - episode 5 step 152 reward tensor([-1], device='cuda:0') loss 0.34075427055358887 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 153 reward tensor([-1], device='cuda:0') loss 0.21865123510360718 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 154 reward tensor([-1], device='cuda:0') loss 0.31601518392562866 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 155 reward tensor([-1], device='cuda:0') loss 0.3160208463668823 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 156 reward tensor([-1], device='cuda:0') loss 0.25524502992630005 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 157 reward tensor([-1], device='cuda:0') loss 0.26314640045166016 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 158 reward tensor([-1], device='cuda:0') loss 0.36190980672836304 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 159 reward tensor([-1], device='cuda:0') loss 0.2505495250225067 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 160 reward tensor([-1], device='cuda:0') loss 0.2609630823135376 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 161 reward tensor([-1], device='cuda:0') loss 0.2787483334541321 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 162 reward tensor([-1], device='cuda:0') loss 0.2593429684638977 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 163 reward tensor([-1], device='cuda:0') loss 0.3124881684780121 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 164 reward tensor([-1], device='cuda:0') loss 0.3191283941268921 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 165 reward tensor([-1], device='cuda:0') loss 0.1934502124786377 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 166 reward tensor([-1], device='cuda:0') loss 0.2735898792743683 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 167 reward tensor([-1], device='cuda:0') loss 0.21023622155189514 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 168 reward tensor([-1], device='cuda:0') loss 0.2529206871986389 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 169 reward tensor([-1], device='cuda:0') loss 0.23779040575027466 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 170 reward tensor([-1], device='cuda:0') loss 0.29837292432785034 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 171 reward tensor([-1], device='cuda:0') loss 0.2675021290779114 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 172 reward tensor([-1], device='cuda:0') loss 0.24533173441886902 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 173 reward tensor([-1], device='cuda:0') loss 0.4518328905105591 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 174 reward tensor([-1], device='cuda:0') loss 0.4388461709022522 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 175 reward tensor([-1], device='cuda:0') loss 0.3251774311065674 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 176 reward tensor([-1], device='cuda:0') loss 0.3339208960533142 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 177 reward tensor([-1], device='cuda:0') loss 0.21493873000144958 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 178 reward tensor([-1], device='cuda:0') loss 0.32392871379852295 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 179 reward tensor([-1], device='cuda:0') loss 0.23433233797550201 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 180 reward tensor([-1], device='cuda:0') loss 0.3067605495452881 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 181 reward tensor([-1], device='cuda:0') loss 0.3009241223335266 epsilon 0.7184850685945312
26-Feb-25 13:31:35 - agent.DQN.DQN - INFO - episode 5 step 182 reward tensor([-1], device='cuda:0') loss 0.3246138095855713 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 183 reward tensor([-1], device='cuda:0') loss 0.22000259160995483 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 184 reward tensor([-1], device='cuda:0') loss 0.2648465931415558 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 185 reward tensor([-1], device='cuda:0') loss 0.29042431712150574 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 186 reward tensor([-1], device='cuda:0') loss 0.2891770601272583 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 187 reward tensor([-1], device='cuda:0') loss 0.337253212928772 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 188 reward tensor([-1], device='cuda:0') loss 0.2617121636867523 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 189 reward tensor([-1], device='cuda:0') loss 0.25686582922935486 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 190 reward tensor([-1], device='cuda:0') loss 0.2929043471813202 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 191 reward tensor([-1], device='cuda:0') loss 0.29004567861557007 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 192 reward tensor([-1], device='cuda:0') loss 0.28523653745651245 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 193 reward tensor([-1], device='cuda:0') loss 0.2366924285888672 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 194 reward tensor([-1], device='cuda:0') loss 0.25310713052749634 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 195 reward tensor([-1], device='cuda:0') loss 0.25909602642059326 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 196 reward tensor([-1], device='cuda:0') loss 0.3061179220676422 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 197 reward tensor([-1], device='cuda:0') loss 0.25411954522132874 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 198 reward tensor([-1], device='cuda:0') loss 0.2264944612979889 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 199 reward tensor([-1], device='cuda:0') loss 0.18985489010810852 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 200 reward tensor([-1], device='cuda:0') loss 0.31426721811294556 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 201 reward tensor([-1], device='cuda:0') loss 0.29991036653518677 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 202 reward tensor([-1], device='cuda:0') loss 0.33685147762298584 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 203 reward tensor([-1], device='cuda:0') loss 0.4168108105659485 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 204 reward tensor([-1], device='cuda:0') loss 0.3750978708267212 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 205 reward tensor([-1], device='cuda:0') loss 0.2958427667617798 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 206 reward tensor([-1], device='cuda:0') loss 0.3249940276145935 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 207 reward tensor([-1], device='cuda:0') loss 0.26847004890441895 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 208 reward tensor([-1], device='cuda:0') loss 0.26627078652381897 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 209 reward tensor([-1], device='cuda:0') loss 0.23322813212871552 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 210 reward tensor([-1], device='cuda:0') loss 0.28373628854751587 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 211 reward tensor([-1], device='cuda:0') loss 0.2994786500930786 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 212 reward tensor([-1], device='cuda:0') loss 0.24781519174575806 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 213 reward tensor([-1], device='cuda:0') loss 0.29014211893081665 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 214 reward tensor([-1], device='cuda:0') loss 0.22735361754894257 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 215 reward tensor([-1], device='cuda:0') loss 0.24455225467681885 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 216 reward tensor([-1], device='cuda:0') loss 0.32793235778808594 epsilon 0.7184850685945312
26-Feb-25 13:31:36 - agent.DQN.DQN - INFO - episode 5 step 217 reward tensor([-1], device='cuda:0') loss 0.2208607941865921 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 218 reward tensor([-1], device='cuda:0') loss 0.3174910545349121 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 219 reward tensor([-1], device='cuda:0') loss 0.24140438437461853 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 220 reward tensor([-1], device='cuda:0') loss 0.23322518169879913 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 221 reward tensor([-1], device='cuda:0') loss 0.264897882938385 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 222 reward tensor([-1], device='cuda:0') loss 0.21757836639881134 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 223 reward tensor([-1], device='cuda:0') loss 0.2719852328300476 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 224 reward tensor([-1], device='cuda:0') loss 0.2001154124736786 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 225 reward tensor([-1], device='cuda:0') loss 0.19573622941970825 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 226 reward tensor([-1], device='cuda:0') loss 0.19820886850357056 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 227 reward tensor([-1], device='cuda:0') loss 0.2637903392314911 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 228 reward tensor([-1], device='cuda:0') loss 0.25761592388153076 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 229 reward tensor([-1], device='cuda:0') loss 0.1862865537405014 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 230 reward tensor([-1], device='cuda:0') loss 0.25422409176826477 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 231 reward tensor([-1], device='cuda:0') loss 0.2175806313753128 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 232 reward tensor([-1], device='cuda:0') loss 0.189759761095047 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 233 reward tensor([-1], device='cuda:0') loss 0.35570812225341797 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 234 reward tensor([-1], device='cuda:0') loss 0.34289348125457764 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 235 reward tensor([-1], device='cuda:0') loss 0.32951268553733826 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 236 reward tensor([-1], device='cuda:0') loss 0.2678489685058594 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 237 reward tensor([-1], device='cuda:0') loss 0.2982185184955597 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 238 reward tensor([-1], device='cuda:0') loss 0.23557011783123016 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 239 reward tensor([-1], device='cuda:0') loss 0.3094898462295532 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 240 reward tensor([-1], device='cuda:0') loss 0.23519745469093323 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 241 reward tensor([-1], device='cuda:0') loss 0.432400643825531 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 242 reward tensor([-1], device='cuda:0') loss 0.25954562425613403 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 243 reward tensor([-1], device='cuda:0') loss 0.383965402841568 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 244 reward tensor([-1], device='cuda:0') loss 0.4211997985839844 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 245 reward tensor([-1], device='cuda:0') loss 0.3279764950275421 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 246 reward tensor([-1], device='cuda:0') loss 0.31140434741973877 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 247 reward tensor([-1], device='cuda:0') loss 0.2673145532608032 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 248 reward tensor([-1], device='cuda:0') loss 0.2523137927055359 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 249 reward tensor([-1], device='cuda:0') loss 0.3098597228527069 epsilon 0.7184850685945312
26-Feb-25 13:31:37 - agent.DQN.DQN - INFO - episode 5 step 250 reward tensor([-1], device='cuda:0') loss 0.4050542116165161 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 251 reward tensor([-1], device='cuda:0') loss 0.47094523906707764 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 252 reward tensor([-1], device='cuda:0') loss 0.3292927145957947 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 253 reward tensor([-1], device='cuda:0') loss 0.32214832305908203 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 254 reward tensor([-1], device='cuda:0') loss 0.3022092282772064 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 255 reward tensor([-1], device='cuda:0') loss 0.3956384062767029 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 256 reward tensor([-1], device='cuda:0') loss 0.2664816379547119 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 257 reward tensor([-1], device='cuda:0') loss 0.3931087255477905 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 258 reward tensor([-1], device='cuda:0') loss 0.32963114976882935 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 259 reward tensor([-1], device='cuda:0') loss 0.25985461473464966 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 260 reward tensor([-1], device='cuda:0') loss 0.32700854539871216 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 261 reward tensor([-1], device='cuda:0') loss 0.2906964421272278 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 262 reward tensor([-1], device='cuda:0') loss 0.2903937101364136 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 263 reward tensor([-1], device='cuda:0') loss 0.35782769322395325 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 264 reward tensor([-1], device='cuda:0') loss 0.3413529396057129 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 265 reward tensor([-1], device='cuda:0') loss 0.3131710886955261 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 266 reward tensor([-1], device='cuda:0') loss 0.26941919326782227 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 267 reward tensor([-1], device='cuda:0') loss 0.2178322821855545 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 268 reward tensor([-1], device='cuda:0') loss 0.28789544105529785 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 269 reward tensor([-1], device='cuda:0') loss 0.35644853115081787 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 270 reward tensor([-1], device='cuda:0') loss 0.24443133175373077 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 271 reward tensor([-1], device='cuda:0') loss 0.3119124174118042 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 272 reward tensor([-1], device='cuda:0') loss 0.29483506083488464 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 273 reward tensor([-1], device='cuda:0') loss 0.3489261865615845 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 274 reward tensor([-1], device='cuda:0') loss 0.2821027934551239 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 275 reward tensor([-1], device='cuda:0') loss 0.25079935789108276 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 276 reward tensor([-1], device='cuda:0') loss 0.2677750885486603 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 277 reward tensor([-1], device='cuda:0') loss 0.234486922621727 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 278 reward tensor([-1], device='cuda:0') loss 0.24932926893234253 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 279 reward tensor([-1], device='cuda:0') loss 0.3339667320251465 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 280 reward tensor([-1], device='cuda:0') loss 0.23175200819969177 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 281 reward tensor([-1], device='cuda:0') loss 0.3301599323749542 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 282 reward tensor([-1], device='cuda:0') loss 0.1844921112060547 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 283 reward tensor([-1], device='cuda:0') loss 0.24018725752830505 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 284 reward tensor([-1], device='cuda:0') loss 0.26319420337677 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 285 reward tensor([-1], device='cuda:0') loss 0.19460195302963257 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 286 reward tensor([-1], device='cuda:0') loss 0.21943780779838562 epsilon 0.7184850685945312
26-Feb-25 13:31:38 - agent.DQN.DQN - INFO - episode 5 step 287 reward tensor([-1], device='cuda:0') loss 0.22622407972812653 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 288 reward tensor([-1], device='cuda:0') loss 0.1668301522731781 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 289 reward tensor([-1], device='cuda:0') loss 0.22563210129737854 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 290 reward tensor([-1], device='cuda:0') loss 0.23633666336536407 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 291 reward tensor([-1], device='cuda:0') loss 0.2315692901611328 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 292 reward tensor([-1], device='cuda:0') loss 0.18560749292373657 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 293 reward tensor([-1], device='cuda:0') loss 0.2910846471786499 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 294 reward tensor([-1], device='cuda:0') loss 0.27103376388549805 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 295 reward tensor([-1], device='cuda:0') loss 0.4126106798648834 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 296 reward tensor([-1], device='cuda:0') loss 0.23850306868553162 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 297 reward tensor([-1], device='cuda:0') loss 0.3561851978302002 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 298 reward tensor([-1], device='cuda:0') loss 0.3166988492012024 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 299 reward tensor([-1], device='cuda:0') loss 0.23820725083351135 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 300 reward tensor([-1], device='cuda:0') loss 0.3681274652481079 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 5 step 301 reward tensor([-1], device='cuda:0') loss 0.22792449593544006 epsilon 0.7184850685945312
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 0 reward tensor([-1], device='cuda:0') loss 0.21107028424739838 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 1 reward tensor([-1], device='cuda:0') loss 0.24123670160770416 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 2 reward tensor([-1], device='cuda:0') loss 0.27659493684768677 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 3 reward tensor([-1], device='cuda:0') loss 0.24838075041770935 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 4 reward tensor([-1], device='cuda:0') loss 0.25526100397109985 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 5 reward tensor([-1], device='cuda:0') loss 0.22438926994800568 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 6 reward tensor([-1], device='cuda:0') loss 0.26321345567703247 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 7 reward tensor([-1], device='cuda:0') loss 0.31915244460105896 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 8 reward tensor([-1], device='cuda:0') loss 0.28332680463790894 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 9 reward tensor([-1], device='cuda:0') loss 0.22774359583854675 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 10 reward tensor([-1], device='cuda:0') loss 0.33949124813079834 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 11 reward tensor([-1], device='cuda:0') loss 0.17192620038986206 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 12 reward tensor([-1], device='cuda:0') loss 0.2094581425189972 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 13 reward tensor([-1], device='cuda:0') loss 0.22341260313987732 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 14 reward tensor([-1], device='cuda:0') loss 0.2447800189256668 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 15 reward tensor([-1], device='cuda:0') loss 0.21278829872608185 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 16 reward tensor([-1], device='cuda:0') loss 0.3272073566913605 epsilon 0.6951343038652089
26-Feb-25 13:31:39 - agent.DQN.DQN - INFO - episode 6 step 17 reward tensor([-1], device='cuda:0') loss 0.15576571226119995 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 18 reward tensor([-1], device='cuda:0') loss 0.208616703748703 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 19 reward tensor([-1], device='cuda:0') loss 0.2396194338798523 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 20 reward tensor([-1], device='cuda:0') loss 0.2213945984840393 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 21 reward tensor([-1], device='cuda:0') loss 0.35679155588150024 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 22 reward tensor([-1], device='cuda:0') loss 0.34030795097351074 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 23 reward tensor([-1], device='cuda:0') loss 0.30815422534942627 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 24 reward tensor([-1], device='cuda:0') loss 0.34189605712890625 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 25 reward tensor([-1], device='cuda:0') loss 0.23937612771987915 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 26 reward tensor([-1], device='cuda:0') loss 0.2176884114742279 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 27 reward tensor([-1], device='cuda:0') loss 0.24138876795768738 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 28 reward tensor([-1], device='cuda:0') loss 0.232021301984787 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 29 reward tensor([-1], device='cuda:0') loss 0.24333173036575317 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 30 reward tensor([-1], device='cuda:0') loss 0.2132664918899536 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 31 reward tensor([-1], device='cuda:0') loss 0.2767603397369385 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 32 reward tensor([-1], device='cuda:0') loss 0.3585866689682007 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 33 reward tensor([-1], device='cuda:0') loss 0.2218731939792633 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 34 reward tensor([-1], device='cuda:0') loss 0.20132559537887573 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 35 reward tensor([-1], device='cuda:0') loss 0.19939859211444855 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 36 reward tensor([-1], device='cuda:0') loss 0.1993331015110016 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 37 reward tensor([-1], device='cuda:0') loss 0.2730327248573303 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 38 reward tensor([-1], device='cuda:0') loss 0.25161224603652954 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 39 reward tensor([-1], device='cuda:0') loss 0.24972772598266602 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 40 reward tensor([-1], device='cuda:0') loss 0.36512428522109985 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 41 reward tensor([-1], device='cuda:0') loss 0.334791898727417 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 42 reward tensor([-1], device='cuda:0') loss 0.25995126366615295 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 43 reward tensor([-1], device='cuda:0') loss 0.21721993386745453 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 44 reward tensor([-1], device='cuda:0') loss 0.24241957068443298 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 45 reward tensor([-1], device='cuda:0') loss 0.19884786009788513 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 46 reward tensor([-1], device='cuda:0') loss 0.2454315721988678 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 47 reward tensor([-1], device='cuda:0') loss 0.19622525572776794 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 48 reward tensor([-1], device='cuda:0') loss 0.16206374764442444 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 49 reward tensor([-1], device='cuda:0') loss 0.21513700485229492 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 50 reward tensor([-1], device='cuda:0') loss 0.2835501432418823 epsilon 0.6951343038652089
26-Feb-25 13:31:40 - agent.DQN.DQN - INFO - episode 6 step 51 reward tensor([-1], device='cuda:0') loss 0.304136723279953 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 52 reward tensor([-1], device='cuda:0') loss 0.31420737504959106 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 53 reward tensor([-1], device='cuda:0') loss 0.2640852928161621 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 54 reward tensor([-1], device='cuda:0') loss 0.2488745152950287 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 55 reward tensor([-1], device='cuda:0') loss 0.2732102870941162 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 56 reward tensor([-1], device='cuda:0') loss 0.29063090682029724 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 57 reward tensor([-1], device='cuda:0') loss 0.26435360312461853 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 58 reward tensor([-1], device='cuda:0') loss 0.17644080519676208 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 59 reward tensor([-1], device='cuda:0') loss 0.2682729661464691 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 60 reward tensor([-1], device='cuda:0') loss 0.23175197839736938 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 61 reward tensor([-1], device='cuda:0') loss 0.21061250567436218 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 62 reward tensor([-1], device='cuda:0') loss 0.21387343108654022 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 63 reward tensor([-1], device='cuda:0') loss 0.32365530729293823 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 64 reward tensor([-1], device='cuda:0') loss 0.21402579545974731 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 65 reward tensor([-1], device='cuda:0') loss 0.2995751202106476 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 66 reward tensor([-1], device='cuda:0') loss 0.37379127740859985 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 67 reward tensor([-1], device='cuda:0') loss 0.3188244700431824 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 68 reward tensor([-1], device='cuda:0') loss 0.22760167717933655 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 69 reward tensor([-1], device='cuda:0') loss 0.3121001124382019 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 70 reward tensor([-1], device='cuda:0') loss 0.29346227645874023 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 71 reward tensor([-1], device='cuda:0') loss 0.2507058382034302 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 72 reward tensor([-1], device='cuda:0') loss 0.33903902769088745 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 73 reward tensor([-1], device='cuda:0') loss 0.27351802587509155 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 74 reward tensor([-1], device='cuda:0') loss 0.2548937201499939 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 75 reward tensor([-1], device='cuda:0') loss 0.2438529133796692 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 76 reward tensor([-1], device='cuda:0') loss 0.3613280653953552 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 77 reward tensor([-1], device='cuda:0') loss 0.2242256999015808 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 78 reward tensor([-1], device='cuda:0') loss 0.24183818697929382 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 79 reward tensor([-1], device='cuda:0') loss 0.24066999554634094 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 80 reward tensor([-1], device='cuda:0') loss 0.22180187702178955 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 81 reward tensor([-1], device='cuda:0') loss 0.41829878091812134 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 82 reward tensor([-1], device='cuda:0') loss 0.350457102060318 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 83 reward tensor([-1], device='cuda:0') loss 0.3455911874771118 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 84 reward tensor([-1], device='cuda:0') loss 0.23433056473731995 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 85 reward tensor([-1], device='cuda:0') loss 0.2530125379562378 epsilon 0.6951343038652089
26-Feb-25 13:31:41 - agent.DQN.DQN - INFO - episode 6 step 86 reward tensor([-1], device='cuda:0') loss 0.2978985905647278 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 87 reward tensor([-1], device='cuda:0') loss 0.23472242057323456 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 88 reward tensor([-1], device='cuda:0') loss 0.3800976872444153 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 89 reward tensor([-1], device='cuda:0') loss 0.2455582618713379 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 90 reward tensor([-1], device='cuda:0') loss 0.2917000651359558 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 91 reward tensor([-1], device='cuda:0') loss 0.3966711461544037 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 92 reward tensor([-1], device='cuda:0') loss 0.3654080033302307 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 93 reward tensor([-1], device='cuda:0') loss 0.16633284091949463 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 94 reward tensor([-1], device='cuda:0') loss 0.22875866293907166 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 95 reward tensor([-1], device='cuda:0') loss 0.27596086263656616 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 96 reward tensor([-1], device='cuda:0') loss 0.34368962049484253 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 97 reward tensor([-1], device='cuda:0') loss 0.24667680263519287 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 98 reward tensor([-1], device='cuda:0') loss 0.3159571886062622 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 99 reward tensor([-1], device='cuda:0') loss 0.35304149985313416 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 100 reward tensor([-1], device='cuda:0') loss 0.19522255659103394 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 101 reward tensor([-1], device='cuda:0') loss 0.216953843832016 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 102 reward tensor([-1], device='cuda:0') loss 0.2433963268995285 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 103 reward tensor([-1], device='cuda:0') loss 0.17336773872375488 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 104 reward tensor([-1], device='cuda:0') loss 0.24695616960525513 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 105 reward tensor([-1], device='cuda:0') loss 0.36625564098358154 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 106 reward tensor([-1], device='cuda:0') loss 0.3348180651664734 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 107 reward tensor([-1], device='cuda:0') loss 0.24162939190864563 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 108 reward tensor([-1], device='cuda:0') loss 0.18632405996322632 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 109 reward tensor([-1], device='cuda:0') loss 0.2696043848991394 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 110 reward tensor([-1], device='cuda:0') loss 0.18876652419567108 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 111 reward tensor([-1], device='cuda:0') loss 0.4132697880268097 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 112 reward tensor([-1], device='cuda:0') loss 0.3040883243083954 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 113 reward tensor([-1], device='cuda:0') loss 0.22735479474067688 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 114 reward tensor([-1], device='cuda:0') loss 0.22957678139209747 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 115 reward tensor([-1], device='cuda:0') loss 0.2382277250289917 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 116 reward tensor([-1], device='cuda:0') loss 0.3370596766471863 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 117 reward tensor([-1], device='cuda:0') loss 0.26036664843559265 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 118 reward tensor([-1], device='cuda:0') loss 0.2183210402727127 epsilon 0.6951343038652089
26-Feb-25 13:31:42 - agent.DQN.DQN - INFO - episode 6 step 119 reward tensor([-1], device='cuda:0') loss 0.27847933769226074 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 120 reward tensor([-1], device='cuda:0') loss 0.2000010460615158 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 121 reward tensor([-1], device='cuda:0') loss 0.21543458104133606 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 122 reward tensor([-1], device='cuda:0') loss 0.17988914251327515 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 123 reward tensor([-1], device='cuda:0') loss 0.1990545690059662 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 124 reward tensor([-1], device='cuda:0') loss 0.24982233345508575 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 125 reward tensor([-1], device='cuda:0') loss 0.21281762421131134 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 126 reward tensor([-1], device='cuda:0') loss 0.17562834918498993 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 127 reward tensor([-1], device='cuda:0') loss 0.2086779773235321 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 128 reward tensor([-1], device='cuda:0') loss 0.23013602197170258 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 129 reward tensor([-1], device='cuda:0') loss 0.3424462676048279 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 130 reward tensor([-1], device='cuda:0') loss 0.21180135011672974 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 131 reward tensor([-1], device='cuda:0') loss 0.19711962342262268 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 132 reward tensor([-1], device='cuda:0') loss 0.296944260597229 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 133 reward tensor([-1], device='cuda:0') loss 0.31896162033081055 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 134 reward tensor([-1], device='cuda:0') loss 0.43178147077560425 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 135 reward tensor([-1], device='cuda:0') loss 0.18573689460754395 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 136 reward tensor([-1], device='cuda:0') loss 0.22633114457130432 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 137 reward tensor([-1], device='cuda:0') loss 0.29431670904159546 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 138 reward tensor([-1], device='cuda:0') loss 0.35825932025909424 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 139 reward tensor([-1], device='cuda:0') loss 0.27028688788414 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 140 reward tensor([-1], device='cuda:0') loss 0.23122990131378174 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 141 reward tensor([-1], device='cuda:0') loss 0.3154953122138977 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 142 reward tensor([-1], device='cuda:0') loss 0.2635096609592438 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 143 reward tensor([-1], device='cuda:0') loss 0.37233418226242065 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 144 reward tensor([-1], device='cuda:0') loss 0.3072565793991089 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 145 reward tensor([-1], device='cuda:0') loss 0.2905217409133911 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 146 reward tensor([-1], device='cuda:0') loss 0.19861862063407898 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 147 reward tensor([-1], device='cuda:0') loss 0.2718474268913269 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 148 reward tensor([-1], device='cuda:0') loss 0.2248658835887909 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 149 reward tensor([-1], device='cuda:0') loss 0.2517954111099243 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 150 reward tensor([-1], device='cuda:0') loss 0.27339649200439453 epsilon 0.6951343038652089
26-Feb-25 13:31:43 - agent.DQN.DQN - INFO - episode 6 step 151 reward tensor([-1], device='cuda:0') loss 0.23724229633808136 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 152 reward tensor([-1], device='cuda:0') loss 0.2814177870750427 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 153 reward tensor([-1], device='cuda:0') loss 0.28376486897468567 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 154 reward tensor([-1], device='cuda:0') loss 0.1840723305940628 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 155 reward tensor([-1], device='cuda:0') loss 0.2525363266468048 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 156 reward tensor([-1], device='cuda:0') loss 0.34264320135116577 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 157 reward tensor([-1], device='cuda:0') loss 0.25653547048568726 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 158 reward tensor([-1], device='cuda:0') loss 0.21055498719215393 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 159 reward tensor([-1], device='cuda:0') loss 0.19582128524780273 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 160 reward tensor([-1], device='cuda:0') loss 0.20532730221748352 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 161 reward tensor([-1], device='cuda:0') loss 0.15212872624397278 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 162 reward tensor([-1], device='cuda:0') loss 0.3617042899131775 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 163 reward tensor([-1], device='cuda:0') loss 0.1848810911178589 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 164 reward tensor([-1], device='cuda:0') loss 0.1916176825761795 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 165 reward tensor([-1], device='cuda:0') loss 0.16242216527462006 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 166 reward tensor([-1], device='cuda:0') loss 0.33086922764778137 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 167 reward tensor([-1], device='cuda:0') loss 0.24008238315582275 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 168 reward tensor([-1], device='cuda:0') loss 0.2539152503013611 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 169 reward tensor([-1], device='cuda:0') loss 0.2791188359260559 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 170 reward tensor([-1], device='cuda:0') loss 0.19264796376228333 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 171 reward tensor([-1], device='cuda:0') loss 0.416736900806427 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 172 reward tensor([-1], device='cuda:0') loss 0.313242107629776 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 173 reward tensor([-1], device='cuda:0') loss 0.24807333946228027 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 174 reward tensor([-1], device='cuda:0') loss 0.2533828914165497 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 175 reward tensor([-1], device='cuda:0') loss 0.2187914252281189 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 176 reward tensor([-1], device='cuda:0') loss 0.2822123169898987 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 177 reward tensor([-1], device='cuda:0') loss 0.20564675331115723 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 178 reward tensor([-1], device='cuda:0') loss 0.377392053604126 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 179 reward tensor([-1], device='cuda:0') loss 0.18713602423667908 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 180 reward tensor([-1], device='cuda:0') loss 0.18165910243988037 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 181 reward tensor([-1], device='cuda:0') loss 0.22143080830574036 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 182 reward tensor([-1], device='cuda:0') loss 0.33077895641326904 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 183 reward tensor([-1], device='cuda:0') loss 0.14314281940460205 epsilon 0.6951343038652089
26-Feb-25 13:31:44 - agent.DQN.DQN - INFO - episode 6 step 184 reward tensor([-1], device='cuda:0') loss 0.22760099172592163 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 185 reward tensor([-1], device='cuda:0') loss 0.21734802424907684 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 186 reward tensor([-1], device='cuda:0') loss 0.2657855153083801 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 187 reward tensor([-1], device='cuda:0') loss 0.2493118941783905 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 188 reward tensor([-1], device='cuda:0') loss 0.19833000004291534 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 189 reward tensor([-1], device='cuda:0') loss 0.37692826986312866 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 190 reward tensor([-1], device='cuda:0') loss 0.19038903713226318 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 191 reward tensor([-1], device='cuda:0') loss 0.33432307839393616 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 192 reward tensor([-1], device='cuda:0') loss 0.20332494378089905 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 193 reward tensor([-1], device='cuda:0') loss 0.20170605182647705 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 194 reward tensor([-1], device='cuda:0') loss 0.22861525416374207 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 195 reward tensor([-1], device='cuda:0') loss 0.2268945574760437 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 196 reward tensor([-1], device='cuda:0') loss 0.254708856344223 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 197 reward tensor([-1], device='cuda:0') loss 0.21192222833633423 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 198 reward tensor([-1], device='cuda:0') loss 0.2618405222892761 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 199 reward tensor([-1], device='cuda:0') loss 0.19299255311489105 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 200 reward tensor([-1], device='cuda:0') loss 0.21288108825683594 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 201 reward tensor([-1], device='cuda:0') loss 0.2146144062280655 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 202 reward tensor([-1], device='cuda:0') loss 0.26368680596351624 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 203 reward tensor([-1], device='cuda:0') loss 0.2354307472705841 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 204 reward tensor([-1], device='cuda:0') loss 0.3716588020324707 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 205 reward tensor([-1], device='cuda:0') loss 0.2297787219285965 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 206 reward tensor([-1], device='cuda:0') loss 0.39340314269065857 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 207 reward tensor([-1], device='cuda:0') loss 0.1841740757226944 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 208 reward tensor([-1], device='cuda:0') loss 0.15286515653133392 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 209 reward tensor([-1], device='cuda:0') loss 0.41701745986938477 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 210 reward tensor([-1], device='cuda:0') loss 0.23586326837539673 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 211 reward tensor([-1], device='cuda:0') loss 0.2548452615737915 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 212 reward tensor([-1], device='cuda:0') loss 0.17214971780776978 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 213 reward tensor([-1], device='cuda:0') loss 0.19338563084602356 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 214 reward tensor([-1], device='cuda:0') loss 0.20668673515319824 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 215 reward tensor([-1], device='cuda:0') loss 0.1884637176990509 epsilon 0.6951343038652089
26-Feb-25 13:31:45 - agent.DQN.DQN - INFO - episode 6 step 216 reward tensor([-1], device='cuda:0') loss 0.2139853537082672 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 217 reward tensor([-1], device='cuda:0') loss 0.37615692615509033 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 218 reward tensor([-1], device='cuda:0') loss 0.20073774456977844 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 219 reward tensor([-1], device='cuda:0') loss 0.22412195801734924 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 220 reward tensor([-1], device='cuda:0') loss 0.3531992435455322 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 221 reward tensor([-1], device='cuda:0') loss 0.17192891240119934 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 222 reward tensor([-1], device='cuda:0') loss 0.1729976385831833 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 223 reward tensor([-1], device='cuda:0') loss 0.21420462429523468 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 224 reward tensor([-1], device='cuda:0') loss 0.3943714201450348 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 225 reward tensor([-1], device='cuda:0') loss 0.32321640849113464 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 226 reward tensor([-1], device='cuda:0') loss 0.2124849110841751 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 227 reward tensor([-1], device='cuda:0') loss 0.3077176809310913 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 228 reward tensor([-1], device='cuda:0') loss 0.1985883116722107 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 229 reward tensor([-1], device='cuda:0') loss 0.26005542278289795 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 230 reward tensor([-1], device='cuda:0') loss 0.28929346799850464 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 231 reward tensor([-1], device='cuda:0') loss 0.34552985429763794 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 232 reward tensor([-1], device='cuda:0') loss 0.2650120258331299 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 233 reward tensor([-1], device='cuda:0') loss 0.3109029531478882 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 234 reward tensor([-1], device='cuda:0') loss 0.25636646151542664 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 235 reward tensor([-1], device='cuda:0') loss 0.23079070448875427 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 236 reward tensor([-1], device='cuda:0') loss 0.3300999402999878 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 237 reward tensor([-1], device='cuda:0') loss 0.2371266484260559 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 238 reward tensor([-1], device='cuda:0') loss 0.21502180397510529 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 239 reward tensor([-1], device='cuda:0') loss 0.27399787306785583 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 240 reward tensor([-1], device='cuda:0') loss 0.40355974435806274 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 241 reward tensor([-1], device='cuda:0') loss 0.4787033796310425 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 242 reward tensor([-1], device='cuda:0') loss 0.23393994569778442 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 243 reward tensor([-1], device='cuda:0') loss 0.231857568025589 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 244 reward tensor([-1], device='cuda:0') loss 0.20583473145961761 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 245 reward tensor([-1], device='cuda:0') loss 0.20442411303520203 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 246 reward tensor([-1], device='cuda:0') loss 0.3124992251396179 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 247 reward tensor([-1], device='cuda:0') loss 0.16450843214988708 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 248 reward tensor([-1], device='cuda:0') loss 0.45771878957748413 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 249 reward tensor([-1], device='cuda:0') loss 0.19473138451576233 epsilon 0.6951343038652089
26-Feb-25 13:31:46 - agent.DQN.DQN - INFO - episode 6 step 250 reward tensor([-1], device='cuda:0') loss 0.4180002808570862 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 251 reward tensor([-1], device='cuda:0') loss 0.22904396057128906 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 252 reward tensor([-1], device='cuda:0') loss 0.2052610069513321 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 253 reward tensor([-1], device='cuda:0') loss 0.20202665030956268 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 254 reward tensor([-1], device='cuda:0') loss 0.28824350237846375 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 255 reward tensor([-1], device='cuda:0') loss 0.5304099917411804 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 256 reward tensor([-1], device='cuda:0') loss 0.3200216293334961 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 257 reward tensor([-1], device='cuda:0') loss 0.2260962575674057 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 258 reward tensor([-1], device='cuda:0') loss 0.22459347546100616 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 259 reward tensor([-1], device='cuda:0') loss 0.21082553267478943 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 260 reward tensor([-1], device='cuda:0') loss 0.24308964610099792 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 261 reward tensor([-1], device='cuda:0') loss 0.24566616117954254 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 262 reward tensor([-1], device='cuda:0') loss 0.20558568835258484 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 263 reward tensor([-1], device='cuda:0') loss 0.32913410663604736 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 264 reward tensor([-1], device='cuda:0') loss 0.29952600598335266 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 265 reward tensor([-1], device='cuda:0') loss 0.21543139219284058 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 266 reward tensor([-1], device='cuda:0') loss 0.261355996131897 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 267 reward tensor([-1], device='cuda:0') loss 0.4014781713485718 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 268 reward tensor([-1], device='cuda:0') loss 0.25628697872161865 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 269 reward tensor([-1], device='cuda:0') loss 0.2051418572664261 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 270 reward tensor([-1], device='cuda:0') loss 0.17303891479969025 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 271 reward tensor([-1], device='cuda:0') loss 0.2840568721294403 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 272 reward tensor([-1], device='cuda:0') loss 0.18031629920005798 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 273 reward tensor([-1], device='cuda:0') loss 0.21511846780776978 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 274 reward tensor([-1], device='cuda:0') loss 0.27927571535110474 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 275 reward tensor([-1], device='cuda:0') loss 0.27670571208000183 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 276 reward tensor([-1], device='cuda:0') loss 0.2327108085155487 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 277 reward tensor([-1], device='cuda:0') loss 0.4179045259952545 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 278 reward tensor([-1], device='cuda:0') loss 0.1844593584537506 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 279 reward tensor([-1], device='cuda:0') loss 0.18069235980510712 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 280 reward tensor([-1], device='cuda:0') loss 0.19655996561050415 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 281 reward tensor([-1], device='cuda:0') loss 0.18167580664157867 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 282 reward tensor([-1], device='cuda:0') loss 0.24148640036582947 epsilon 0.6951343038652089
26-Feb-25 13:31:47 - agent.DQN.DQN - INFO - episode 6 step 283 reward tensor([-1], device='cuda:0') loss 0.1743171513080597 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 284 reward tensor([-1], device='cuda:0') loss 0.19784381985664368 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 285 reward tensor([-1], device='cuda:0') loss 0.319644957780838 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 286 reward tensor([-1], device='cuda:0') loss 0.31242984533309937 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 287 reward tensor([-1], device='cuda:0') loss 0.1454688161611557 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 288 reward tensor([-1], device='cuda:0') loss 0.2185022532939911 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 289 reward tensor([-1], device='cuda:0') loss 0.19924646615982056 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 290 reward tensor([-1], device='cuda:0') loss 0.198390930891037 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 291 reward tensor([-1], device='cuda:0') loss 0.4601435959339142 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 292 reward tensor([-1], device='cuda:0') loss 0.29130053520202637 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 293 reward tensor([-1], device='cuda:0') loss 0.36749452352523804 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 294 reward tensor([-1], device='cuda:0') loss 0.26743027567863464 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 295 reward tensor([-1], device='cuda:0') loss 0.15129753947257996 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 296 reward tensor([-1], device='cuda:0') loss 0.25699788331985474 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 297 reward tensor([-1], device='cuda:0') loss 0.19823426008224487 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 298 reward tensor([-1], device='cuda:0') loss 0.23515000939369202 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 299 reward tensor([-1], device='cuda:0') loss 0.23121346533298492 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 300 reward tensor([-1], device='cuda:0') loss 0.21430036425590515 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 6 step 301 reward tensor([-1], device='cuda:0') loss 0.22727221250534058 epsilon 0.6951343038652089
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 0 reward tensor([-1], device='cuda:0') loss 0.33099910616874695 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 1 reward tensor([-1], device='cuda:0') loss 0.24435661733150482 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 2 reward tensor([-1], device='cuda:0') loss 0.23788480460643768 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 3 reward tensor([-1], device='cuda:0') loss 0.3945305049419403 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 4 reward tensor([-1], device='cuda:0') loss 0.20348849892616272 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 5 reward tensor([-1], device='cuda:0') loss 0.21214447915554047 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 6 reward tensor([-1], device='cuda:0') loss 0.35880279541015625 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 7 reward tensor([-1], device='cuda:0') loss 0.15346744656562805 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 8 reward tensor([-1], device='cuda:0') loss 0.18497200310230255 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 9 reward tensor([-1], device='cuda:0') loss 0.26638931035995483 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 10 reward tensor([-1], device='cuda:0') loss 0.28014251589775085 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 11 reward tensor([-1], device='cuda:0') loss 0.19400334358215332 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 12 reward tensor([-1], device='cuda:0') loss 0.27229276299476624 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 13 reward tensor([-1], device='cuda:0') loss 0.46323341131210327 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 14 reward tensor([-1], device='cuda:0') loss 0.18042004108428955 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 15 reward tensor([-1], device='cuda:0') loss 0.23030748963356018 epsilon 0.6725424389895897
26-Feb-25 13:31:48 - agent.DQN.DQN - INFO - episode 7 step 16 reward tensor([-1], device='cuda:0') loss 0.19424597918987274 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 17 reward tensor([-1], device='cuda:0') loss 0.33444979786872864 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 18 reward tensor([-1], device='cuda:0') loss 0.2078370451927185 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 19 reward tensor([-1], device='cuda:0') loss 0.23132865130901337 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 20 reward tensor([-1], device='cuda:0') loss 0.2287246286869049 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 21 reward tensor([-1], device='cuda:0') loss 0.2358095645904541 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 22 reward tensor([-1], device='cuda:0') loss 0.2233666628599167 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 23 reward tensor([-1], device='cuda:0') loss 0.2488078773021698 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 24 reward tensor([-1], device='cuda:0') loss 0.17340639233589172 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 25 reward tensor([-1], device='cuda:0') loss 0.21972396969795227 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 26 reward tensor([-1], device='cuda:0') loss 0.4328012466430664 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 27 reward tensor([-1], device='cuda:0') loss 0.1931086778640747 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 28 reward tensor([-1], device='cuda:0') loss 0.1881522834300995 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 29 reward tensor([-1], device='cuda:0') loss 0.34678468108177185 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 30 reward tensor([-1], device='cuda:0') loss 0.2248707413673401 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 31 reward tensor([-1], device='cuda:0') loss 0.3546767234802246 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 32 reward tensor([-1], device='cuda:0') loss 0.21465939283370972 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 33 reward tensor([-1], device='cuda:0') loss 0.22115802764892578 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 34 reward tensor([-1], device='cuda:0') loss 0.2446112334728241 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 35 reward tensor([-1], device='cuda:0') loss 0.22170580923557281 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 36 reward tensor([-1], device='cuda:0') loss 0.24311339855194092 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 37 reward tensor([-1], device='cuda:0') loss 0.5676204562187195 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 38 reward tensor([-1], device='cuda:0') loss 0.26447829604148865 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 39 reward tensor([-1], device='cuda:0') loss 0.20873740315437317 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 40 reward tensor([-1], device='cuda:0') loss 0.1566672921180725 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 41 reward tensor([-1], device='cuda:0') loss 0.3301064670085907 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 42 reward tensor([-1], device='cuda:0') loss 0.36579421162605286 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 43 reward tensor([-1], device='cuda:0') loss 0.21048565208911896 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 44 reward tensor([-1], device='cuda:0') loss 0.23241448402404785 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 45 reward tensor([-1], device='cuda:0') loss 0.19604741036891937 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 46 reward tensor([-1], device='cuda:0') loss 0.174537792801857 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 47 reward tensor([-1], device='cuda:0') loss 0.20483443140983582 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 48 reward tensor([-1], device='cuda:0') loss 0.168477863073349 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 49 reward tensor([-1], device='cuda:0') loss 0.2334052324295044 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 50 reward tensor([-1], device='cuda:0') loss 0.2384740114212036 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 51 reward tensor([-1], device='cuda:0') loss 0.17570841312408447 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 52 reward tensor([-1], device='cuda:0') loss 0.23865753412246704 epsilon 0.6725424389895897
26-Feb-25 13:31:49 - agent.DQN.DQN - INFO - episode 7 step 53 reward tensor([-1], device='cuda:0') loss 0.41402003169059753 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 54 reward tensor([-1], device='cuda:0') loss 0.206179678440094 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 55 reward tensor([-1], device='cuda:0') loss 0.165957510471344 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 56 reward tensor([-1], device='cuda:0') loss 0.29153963923454285 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 57 reward tensor([-1], device='cuda:0') loss 0.5839998722076416 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 58 reward tensor([-1], device='cuda:0') loss 0.26398199796676636 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 59 reward tensor([-1], device='cuda:0') loss 0.20540104806423187 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 60 reward tensor([-1], device='cuda:0') loss 0.47842538356781006 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 61 reward tensor([-1], device='cuda:0') loss 0.26241815090179443 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 62 reward tensor([-1], device='cuda:0') loss 0.20038695633411407 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 63 reward tensor([-1], device='cuda:0') loss 0.17638400197029114 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 64 reward tensor([-1], device='cuda:0') loss 0.1816781610250473 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 65 reward tensor([-1], device='cuda:0') loss 0.21580293774604797 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 66 reward tensor([-1], device='cuda:0') loss 0.25042444467544556 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 67 reward tensor([-1], device='cuda:0') loss 0.2692471444606781 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 68 reward tensor([-1], device='cuda:0') loss 0.2776126265525818 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 69 reward tensor([-1], device='cuda:0') loss 0.18215370178222656 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 70 reward tensor([-1], device='cuda:0') loss 0.15701159834861755 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 71 reward tensor([-1], device='cuda:0') loss 0.2235795259475708 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 72 reward tensor([-1], device='cuda:0') loss 0.16700918972492218 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 73 reward tensor([-1], device='cuda:0') loss 0.15277592837810516 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 74 reward tensor([-1], device='cuda:0') loss 0.20682156085968018 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 75 reward tensor([-1], device='cuda:0') loss 0.25200194120407104 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 76 reward tensor([-1], device='cuda:0') loss 0.15901504456996918 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 77 reward tensor([-1], device='cuda:0') loss 0.26744723320007324 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 78 reward tensor([-1], device='cuda:0') loss 0.21669748425483704 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 79 reward tensor([-1], device='cuda:0') loss 0.21906831860542297 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 80 reward tensor([-1], device='cuda:0') loss 0.20253396034240723 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 81 reward tensor([-1], device='cuda:0') loss 0.19794616103172302 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 82 reward tensor([-1], device='cuda:0') loss 0.17107641696929932 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 83 reward tensor([-1], device='cuda:0') loss 0.1724153757095337 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 84 reward tensor([-1], device='cuda:0') loss 0.2867446839809418 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 85 reward tensor([-1], device='cuda:0') loss 0.23412850499153137 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 86 reward tensor([-1], device='cuda:0') loss 0.3855494558811188 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 87 reward tensor([-1], device='cuda:0') loss 0.435260146856308 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 88 reward tensor([-1], device='cuda:0') loss 0.17125824093818665 epsilon 0.6725424389895897
26-Feb-25 13:31:50 - agent.DQN.DQN - INFO - episode 7 step 89 reward tensor([-1], device='cuda:0') loss 0.23143664002418518 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 90 reward tensor([-1], device='cuda:0') loss 0.1983950436115265 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 91 reward tensor([-1], device='cuda:0') loss 0.2787470519542694 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 92 reward tensor([-1], device='cuda:0') loss 0.18105560541152954 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 93 reward tensor([-1], device='cuda:0') loss 0.1884147822856903 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 94 reward tensor([-1], device='cuda:0') loss 0.1758398413658142 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 95 reward tensor([-1], device='cuda:0') loss 0.1685047149658203 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 96 reward tensor([-1], device='cuda:0') loss 0.20361480116844177 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 97 reward tensor([-1], device='cuda:0') loss 0.17412149906158447 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 98 reward tensor([-1], device='cuda:0') loss 0.44656264781951904 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 99 reward tensor([-1], device='cuda:0') loss 0.20725983381271362 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 100 reward tensor([-1], device='cuda:0') loss 0.20461466908454895 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 101 reward tensor([-1], device='cuda:0') loss 0.177042156457901 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 102 reward tensor([-1], device='cuda:0') loss 0.1546286940574646 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 103 reward tensor([-1], device='cuda:0') loss 0.34001049399375916 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 104 reward tensor([-1], device='cuda:0') loss 0.1745515763759613 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 105 reward tensor([-1], device='cuda:0') loss 0.19508731365203857 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 106 reward tensor([-1], device='cuda:0') loss 0.14738261699676514 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 107 reward tensor([-1], device='cuda:0') loss 0.154876708984375 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 108 reward tensor([-1], device='cuda:0') loss 0.21510237455368042 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 109 reward tensor([-1], device='cuda:0') loss 0.21136687695980072 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 110 reward tensor([-1], device='cuda:0') loss 0.20498105883598328 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 111 reward tensor([-1], device='cuda:0') loss 0.3501172959804535 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 112 reward tensor([-1], device='cuda:0') loss 0.237224742770195 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 113 reward tensor([-1], device='cuda:0') loss 0.19678592681884766 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 114 reward tensor([-1], device='cuda:0') loss 0.1989155113697052 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 115 reward tensor([-1], device='cuda:0') loss 0.1903027892112732 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 116 reward tensor([-1], device='cuda:0') loss 0.3160894513130188 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 117 reward tensor([-1], device='cuda:0') loss 0.163918137550354 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 118 reward tensor([-1], device='cuda:0') loss 0.5777038931846619 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 119 reward tensor([-1], device='cuda:0') loss 0.29519122838974 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 120 reward tensor([-1], device='cuda:0') loss 0.2802848219871521 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 121 reward tensor([-1], device='cuda:0') loss 0.23037013411521912 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 122 reward tensor([-1], device='cuda:0') loss 0.18464574217796326 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 123 reward tensor([-1], device='cuda:0') loss 0.4284626245498657 epsilon 0.6725424389895897
26-Feb-25 13:31:51 - agent.DQN.DQN - INFO - episode 7 step 124 reward tensor([-1], device='cuda:0') loss 0.20026065409183502 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 125 reward tensor([-1], device='cuda:0') loss 0.1899438500404358 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 126 reward tensor([-1], device='cuda:0') loss 0.338745653629303 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 127 reward tensor([-1], device='cuda:0') loss 0.20584288239479065 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 128 reward tensor([-1], device='cuda:0') loss 0.2222016453742981 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 129 reward tensor([-1], device='cuda:0') loss 0.20163944363594055 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 130 reward tensor([-1], device='cuda:0') loss 0.49347931146621704 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 131 reward tensor([-1], device='cuda:0') loss 0.2959786653518677 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 132 reward tensor([-1], device='cuda:0') loss 0.2279767096042633 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 133 reward tensor([-1], device='cuda:0') loss 0.1585415154695511 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 134 reward tensor([-1], device='cuda:0') loss 0.18259799480438232 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 135 reward tensor([-1], device='cuda:0') loss 0.2165069878101349 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 136 reward tensor([-1], device='cuda:0') loss 0.2197617143392563 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 137 reward tensor([-1], device='cuda:0') loss 0.2322307527065277 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 138 reward tensor([-1], device='cuda:0') loss 0.21697130799293518 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 139 reward tensor([-1], device='cuda:0') loss 0.20652715861797333 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 140 reward tensor([-1], device='cuda:0') loss 0.26432088017463684 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 141 reward tensor([-1], device='cuda:0') loss 0.18240462243556976 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 142 reward tensor([-1], device='cuda:0') loss 0.6222472190856934 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 143 reward tensor([-1], device='cuda:0') loss 0.18192173540592194 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 144 reward tensor([-1], device='cuda:0') loss 0.1852841079235077 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 145 reward tensor([-1], device='cuda:0') loss 0.5091263055801392 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 146 reward tensor([-1], device='cuda:0') loss 0.4892122745513916 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 147 reward tensor([-1], device='cuda:0') loss 0.19216199219226837 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 148 reward tensor([-1], device='cuda:0') loss 0.21679092943668365 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 149 reward tensor([-1], device='cuda:0') loss 0.3899696469306946 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 150 reward tensor([-1], device='cuda:0') loss 0.20291024446487427 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 151 reward tensor([-1], device='cuda:0') loss 0.45100510120391846 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 152 reward tensor([-1], device='cuda:0') loss 0.5283203125 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 153 reward tensor([-1], device='cuda:0') loss 0.20133544504642487 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 154 reward tensor([-1], device='cuda:0') loss 0.1912597119808197 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 155 reward tensor([-1], device='cuda:0') loss 0.26582372188568115 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 156 reward tensor([-1], device='cuda:0') loss 0.46950143575668335 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 157 reward tensor([-1], device='cuda:0') loss 0.45413070917129517 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 158 reward tensor([-1], device='cuda:0') loss 0.2259410172700882 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 159 reward tensor([-1], device='cuda:0') loss 0.23195567727088928 epsilon 0.6725424389895897
26-Feb-25 13:31:52 - agent.DQN.DQN - INFO - episode 7 step 160 reward tensor([-1], device='cuda:0') loss 0.3089989125728607 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 161 reward tensor([-1], device='cuda:0') loss 0.43480852246284485 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 162 reward tensor([-1], device='cuda:0') loss 0.19632968306541443 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 163 reward tensor([-1], device='cuda:0') loss 0.19004786014556885 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 164 reward tensor([-1], device='cuda:0') loss 0.16757234930992126 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 165 reward tensor([-1], device='cuda:0') loss 0.3290371298789978 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 166 reward tensor([-1], device='cuda:0') loss 0.31669023633003235 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 167 reward tensor([-1], device='cuda:0') loss 0.23010313510894775 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 168 reward tensor([-1], device='cuda:0') loss 0.2993394434452057 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 169 reward tensor([-1], device='cuda:0') loss 0.44506508111953735 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 170 reward tensor([-1], device='cuda:0') loss 0.2472376525402069 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 171 reward tensor([-1], device='cuda:0') loss 0.5315269827842712 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 172 reward tensor([-1], device='cuda:0') loss 0.2326788306236267 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 173 reward tensor([-1], device='cuda:0') loss 0.28147774934768677 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 174 reward tensor([-1], device='cuda:0') loss 0.31549179553985596 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 175 reward tensor([-1], device='cuda:0') loss 0.21490639448165894 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 176 reward tensor([-1], device='cuda:0') loss 0.3080928921699524 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 177 reward tensor([-1], device='cuda:0') loss 0.16834455728530884 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 178 reward tensor([-1], device='cuda:0') loss 0.21597552299499512 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 179 reward tensor([-1], device='cuda:0') loss 0.26168861985206604 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 180 reward tensor([-1], device='cuda:0') loss 0.25548914074897766 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 181 reward tensor([-1], device='cuda:0') loss 0.19347411394119263 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 182 reward tensor([-1], device='cuda:0') loss 0.38003993034362793 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 183 reward tensor([-1], device='cuda:0') loss 0.28338009119033813 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 184 reward tensor([-1], device='cuda:0') loss 0.2266393005847931 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 185 reward tensor([-1], device='cuda:0') loss 0.2987951636314392 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 186 reward tensor([-1], device='cuda:0') loss 0.25704556703567505 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 187 reward tensor([-1], device='cuda:0') loss 0.37033289670944214 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 188 reward tensor([-1], device='cuda:0') loss 0.7938082814216614 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 189 reward tensor([-1], device='cuda:0') loss 0.22974474728107452 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 190 reward tensor([-1], device='cuda:0') loss 0.20405170321464539 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 191 reward tensor([-1], device='cuda:0') loss 0.1802285611629486 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 192 reward tensor([-1], device='cuda:0') loss 0.3818785548210144 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 193 reward tensor([-1], device='cuda:0') loss 0.3743017911911011 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 194 reward tensor([-1], device='cuda:0') loss 0.22975312173366547 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 195 reward tensor([-1], device='cuda:0') loss 0.20479890704154968 epsilon 0.6725424389895897
26-Feb-25 13:31:53 - agent.DQN.DQN - INFO - episode 7 step 196 reward tensor([-1], device='cuda:0') loss 0.45324939489364624 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 197 reward tensor([-1], device='cuda:0') loss 0.3555961847305298 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 198 reward tensor([-1], device='cuda:0') loss 0.19259248673915863 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 199 reward tensor([-1], device='cuda:0') loss 0.5538065433502197 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 200 reward tensor([-1], device='cuda:0') loss 0.2466934323310852 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 201 reward tensor([-1], device='cuda:0') loss 0.4135403037071228 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 202 reward tensor([-1], device='cuda:0') loss 0.23609158396720886 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 203 reward tensor([-1], device='cuda:0') loss 0.4898650050163269 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 204 reward tensor([-1], device='cuda:0') loss 0.5150834321975708 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 205 reward tensor([-1], device='cuda:0') loss 0.24111990630626678 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 206 reward tensor([-1], device='cuda:0') loss 0.19417771697044373 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 207 reward tensor([-1], device='cuda:0') loss 0.42828020453453064 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 208 reward tensor([-1], device='cuda:0') loss 0.2125827819108963 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 209 reward tensor([-1], device='cuda:0') loss 0.30587074160575867 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 210 reward tensor([-1], device='cuda:0') loss 0.43416696786880493 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 211 reward tensor([-1], device='cuda:0') loss 0.29604095220565796 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 212 reward tensor([-1], device='cuda:0') loss 0.38292402029037476 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 213 reward tensor([-1], device='cuda:0') loss 0.7358976006507874 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 214 reward tensor([-1], device='cuda:0') loss 0.39066562056541443 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 215 reward tensor([-1], device='cuda:0') loss 0.4356011748313904 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 216 reward tensor([-1], device='cuda:0') loss 0.2533710300922394 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 217 reward tensor([-1], device='cuda:0') loss 0.31647810339927673 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 218 reward tensor([-1], device='cuda:0') loss 0.4936756491661072 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 219 reward tensor([-1], device='cuda:0') loss 0.19460038840770721 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 220 reward tensor([-1], device='cuda:0') loss 0.6799294352531433 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 221 reward tensor([-1], device='cuda:0') loss 0.5250017642974854 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 222 reward tensor([-1], device='cuda:0') loss 0.34280771017074585 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 223 reward tensor([-1], device='cuda:0') loss 0.33839815855026245 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 224 reward tensor([-1], device='cuda:0') loss 0.3235377073287964 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 225 reward tensor([-1], device='cuda:0') loss 0.47357508540153503 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 226 reward tensor([-1], device='cuda:0') loss 0.24640116095542908 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 227 reward tensor([-1], device='cuda:0') loss 0.3256259560585022 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 228 reward tensor([-1], device='cuda:0') loss 0.42221492528915405 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 229 reward tensor([-1], device='cuda:0') loss 0.27987465262413025 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 230 reward tensor([-1], device='cuda:0') loss 0.2507137656211853 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 231 reward tensor([-1], device='cuda:0') loss 0.5349206924438477 epsilon 0.6725424389895897
26-Feb-25 13:31:54 - agent.DQN.DQN - INFO - episode 7 step 232 reward tensor([-1], device='cuda:0') loss 0.36321237683296204 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 233 reward tensor([-1], device='cuda:0') loss 0.2454695850610733 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 234 reward tensor([-1], device='cuda:0') loss 0.35352623462677 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 235 reward tensor([-1], device='cuda:0') loss 0.34822261333465576 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 236 reward tensor([-1], device='cuda:0') loss 0.29276373982429504 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 237 reward tensor([-1], device='cuda:0') loss 0.2369510531425476 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 238 reward tensor([-1], device='cuda:0') loss 0.29389217495918274 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 239 reward tensor([-1], device='cuda:0') loss 0.34627074003219604 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 240 reward tensor([-1], device='cuda:0') loss 0.22346267104148865 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 241 reward tensor([-1], device='cuda:0') loss 0.21777832508087158 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 242 reward tensor([-1], device='cuda:0') loss 0.22439925372600555 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 243 reward tensor([-1], device='cuda:0') loss 0.39641210436820984 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 244 reward tensor([-1], device='cuda:0') loss 0.3912980854511261 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 245 reward tensor([-1], device='cuda:0') loss 0.29182320833206177 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 246 reward tensor([-1], device='cuda:0') loss 0.34153810143470764 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 247 reward tensor([-1], device='cuda:0') loss 0.4548298716545105 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 248 reward tensor([-1], device='cuda:0') loss 0.31288522481918335 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 249 reward tensor([-1], device='cuda:0') loss 0.2650885581970215 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 250 reward tensor([-1], device='cuda:0') loss 0.44812679290771484 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 251 reward tensor([-1], device='cuda:0') loss 0.5295215249061584 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 252 reward tensor([-1], device='cuda:0') loss 0.2003868967294693 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 253 reward tensor([-1], device='cuda:0') loss 0.3262782096862793 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 254 reward tensor([-1], device='cuda:0') loss 0.2361719012260437 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 255 reward tensor([-1], device='cuda:0') loss 0.5268237590789795 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 256 reward tensor([-1], device='cuda:0') loss 0.2694336771965027 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 257 reward tensor([-1], device='cuda:0') loss 0.30913209915161133 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 258 reward tensor([-1], device='cuda:0') loss 0.33562225103378296 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 259 reward tensor([-1], device='cuda:0') loss 0.5001747608184814 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 260 reward tensor([-1], device='cuda:0') loss 0.2545979619026184 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 261 reward tensor([-1], device='cuda:0') loss 0.47713524103164673 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 262 reward tensor([-1], device='cuda:0') loss 0.3069536089897156 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 263 reward tensor([-1], device='cuda:0') loss 0.30291858315467834 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 264 reward tensor([-1], device='cuda:0') loss 0.25572896003723145 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 265 reward tensor([-1], device='cuda:0') loss 0.284616619348526 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 266 reward tensor([-1], device='cuda:0') loss 0.30959445238113403 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 267 reward tensor([-1], device='cuda:0') loss 0.30735671520233154 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 268 reward tensor([-1], device='cuda:0') loss 0.461279034614563 epsilon 0.6725424389895897
26-Feb-25 13:31:55 - agent.DQN.DQN - INFO - episode 7 step 269 reward tensor([-1], device='cuda:0') loss 0.25952357053756714 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 270 reward tensor([-1], device='cuda:0') loss 0.23524872958660126 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 271 reward tensor([-1], device='cuda:0') loss 0.4279313087463379 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 272 reward tensor([-1], device='cuda:0') loss 0.2623516619205475 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 273 reward tensor([-1], device='cuda:0') loss 0.37157708406448364 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 274 reward tensor([-1], device='cuda:0') loss 0.23081445693969727 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 275 reward tensor([-1], device='cuda:0') loss 0.23575922846794128 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 276 reward tensor([-1], device='cuda:0') loss 0.27151018381118774 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 277 reward tensor([-1], device='cuda:0') loss 0.19177283346652985 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 278 reward tensor([-1], device='cuda:0') loss 0.2562071681022644 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 279 reward tensor([-1], device='cuda:0') loss 0.22018173336982727 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 280 reward tensor([-1], device='cuda:0') loss 0.31815093755722046 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 281 reward tensor([-1], device='cuda:0') loss 0.2067601978778839 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 282 reward tensor([-1], device='cuda:0') loss 0.2261648327112198 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 283 reward tensor([-1], device='cuda:0') loss 0.21764281392097473 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 284 reward tensor([-1], device='cuda:0') loss 0.2101181000471115 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 285 reward tensor([-1], device='cuda:0') loss 0.3790612518787384 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 286 reward tensor([-1], device='cuda:0') loss 0.17495709657669067 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 287 reward tensor([-1], device='cuda:0') loss 0.20871879160404205 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 288 reward tensor([-1], device='cuda:0') loss 0.223651722073555 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 289 reward tensor([-1], device='cuda:0') loss 0.19529446959495544 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 290 reward tensor([-1], device='cuda:0') loss 0.17783451080322266 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 291 reward tensor([-1], device='cuda:0') loss 0.22311939299106598 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 292 reward tensor([-1], device='cuda:0') loss 0.46181145310401917 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 293 reward tensor([-1], device='cuda:0') loss 0.19552575051784515 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 294 reward tensor([-1], device='cuda:0') loss 0.21049414575099945 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 295 reward tensor([-1], device='cuda:0') loss 0.1865679919719696 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 296 reward tensor([-1], device='cuda:0') loss 0.18127618730068207 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 297 reward tensor([-1], device='cuda:0') loss 0.33211079239845276 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 298 reward tensor([-1], device='cuda:0') loss 0.210252583026886 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 299 reward tensor([-1], device='cuda:0') loss 0.16599220037460327 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 300 reward tensor([-1], device='cuda:0') loss 0.20561127364635468 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 7 step 301 reward tensor([-1], device='cuda:0') loss 0.19906571507453918 epsilon 0.6725424389895897
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 8 step 0 reward tensor([-1], device='cuda:0') loss 0.20670250058174133 epsilon 0.650684809722428
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 8 step 1 reward tensor([-1], device='cuda:0') loss 0.39058953523635864 epsilon 0.650684809722428
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 8 step 2 reward tensor([-1], device='cuda:0') loss 0.19092980027198792 epsilon 0.650684809722428
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 8 step 3 reward tensor([-1], device='cuda:0') loss 0.232937753200531 epsilon 0.650684809722428
26-Feb-25 13:31:56 - agent.DQN.DQN - INFO - episode 8 step 4 reward tensor([-1], device='cuda:0') loss 0.19150540232658386 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 5 reward tensor([-1], device='cuda:0') loss 0.20754176378250122 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 6 reward tensor([-1], device='cuda:0') loss 0.2529074549674988 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 7 reward tensor([-1], device='cuda:0') loss 0.27731263637542725 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 8 reward tensor([-1], device='cuda:0') loss 0.3174792528152466 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 9 reward tensor([-1], device='cuda:0') loss 0.34220194816589355 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 10 reward tensor([-1], device='cuda:0') loss 0.422826886177063 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 11 reward tensor([-1], device='cuda:0') loss 0.3198534846305847 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 12 reward tensor([-1], device='cuda:0') loss 0.21938911080360413 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 13 reward tensor([-1], device='cuda:0') loss 0.19193801283836365 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 14 reward tensor([-1], device='cuda:0') loss 0.29331448674201965 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 15 reward tensor([-1], device='cuda:0') loss 0.16194115579128265 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 16 reward tensor([-1], device='cuda:0') loss 0.29476356506347656 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 17 reward tensor([-1], device='cuda:0') loss 0.19105297327041626 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 18 reward tensor([-1], device='cuda:0') loss 0.21253177523612976 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 19 reward tensor([-1], device='cuda:0') loss 0.4849189221858978 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 20 reward tensor([-1], device='cuda:0') loss 0.14886592328548431 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 21 reward tensor([-1], device='cuda:0') loss 0.26974058151245117 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 22 reward tensor([-1], device='cuda:0') loss 0.19557683169841766 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 23 reward tensor([-1], device='cuda:0') loss 0.17805683612823486 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 24 reward tensor([-1], device='cuda:0') loss 0.20371690392494202 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 25 reward tensor([-1], device='cuda:0') loss 0.16410869359970093 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 26 reward tensor([-1], device='cuda:0') loss 0.2888321876525879 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 27 reward tensor([-1], device='cuda:0') loss 0.16417385637760162 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 28 reward tensor([-1], device='cuda:0') loss 0.6789292097091675 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 29 reward tensor([-1], device='cuda:0') loss 0.15108837187290192 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 30 reward tensor([-1], device='cuda:0') loss 0.5024068355560303 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 31 reward tensor([-1], device='cuda:0') loss 0.16059178113937378 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 32 reward tensor([-1], device='cuda:0') loss 0.26358285546302795 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 33 reward tensor([-1], device='cuda:0') loss 0.49264273047447205 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 34 reward tensor([-1], device='cuda:0') loss 0.15992772579193115 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 35 reward tensor([-1], device='cuda:0') loss 0.1712733954191208 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 36 reward tensor([-1], device='cuda:0') loss 0.3099491596221924 epsilon 0.650684809722428
26-Feb-25 13:31:57 - agent.DQN.DQN - INFO - episode 8 step 37 reward tensor([-1], device='cuda:0') loss 0.3222200870513916 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 38 reward tensor([-1], device='cuda:0') loss 0.2815602421760559 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 39 reward tensor([-1], device='cuda:0') loss 0.21448570489883423 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 40 reward tensor([-1], device='cuda:0') loss 0.2981618642807007 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 41 reward tensor([-1], device='cuda:0') loss 0.15870243310928345 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 42 reward tensor([-1], device='cuda:0') loss 0.19911620020866394 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 43 reward tensor([-1], device='cuda:0') loss 0.9734286069869995 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 44 reward tensor([-1], device='cuda:0') loss 0.17458096146583557 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 45 reward tensor([-1], device='cuda:0') loss 0.19289630651474 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 46 reward tensor([-1], device='cuda:0') loss 0.19509641826152802 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 47 reward tensor([-1], device='cuda:0') loss 0.17146600782871246 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 48 reward tensor([-1], device='cuda:0') loss 0.18874995410442352 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 49 reward tensor([-1], device='cuda:0') loss 0.2821137309074402 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 50 reward tensor([-1], device='cuda:0') loss 1.3682441711425781 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 51 reward tensor([-1], device='cuda:0') loss 0.4189737141132355 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 52 reward tensor([-1], device='cuda:0') loss 0.3769710063934326 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 53 reward tensor([-1], device='cuda:0') loss 0.3115179240703583 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 54 reward tensor([-1], device='cuda:0') loss 0.8959357738494873 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 55 reward tensor([-1], device='cuda:0') loss 0.30811232328414917 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 56 reward tensor([-1], device='cuda:0') loss 0.3447645902633667 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 57 reward tensor([-1], device='cuda:0') loss 0.411201536655426 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 58 reward tensor([-1], device='cuda:0') loss 0.24204763770103455 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 59 reward tensor([-1], device='cuda:0') loss 0.21158623695373535 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 60 reward tensor([-1], device='cuda:0') loss 0.48575741052627563 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 61 reward tensor([-1], device='cuda:0') loss 0.3223154842853546 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 62 reward tensor([-1], device='cuda:0') loss 0.2795896828174591 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 63 reward tensor([-1], device='cuda:0') loss 0.2639046907424927 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 64 reward tensor([-1], device='cuda:0') loss 0.18161842226982117 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 65 reward tensor([-1], device='cuda:0') loss 0.5691663026809692 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 66 reward tensor([-1], device='cuda:0') loss 0.1893036961555481 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 67 reward tensor([-1], device='cuda:0') loss 0.6539698243141174 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 68 reward tensor([-1], device='cuda:0') loss 0.18232637643814087 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 69 reward tensor([-1], device='cuda:0') loss 0.461181104183197 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 70 reward tensor([-1], device='cuda:0') loss 0.2042597532272339 epsilon 0.650684809722428
26-Feb-25 13:31:58 - agent.DQN.DQN - INFO - episode 8 step 71 reward tensor([-1], device='cuda:0') loss 0.2095281183719635 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 72 reward tensor([-1], device='cuda:0') loss 1.0193251371383667 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 73 reward tensor([-1], device='cuda:0') loss 0.9317402839660645 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 74 reward tensor([-1], device='cuda:0') loss 0.2623097896575928 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 75 reward tensor([-1], device='cuda:0') loss 0.21577097475528717 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 76 reward tensor([-1], device='cuda:0') loss 0.24814867973327637 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 77 reward tensor([-1], device='cuda:0') loss 0.24198780953884125 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 78 reward tensor([-1], device='cuda:0') loss 0.2502174973487854 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 79 reward tensor([-1], device='cuda:0') loss 0.30308467149734497 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 80 reward tensor([-1], device='cuda:0') loss 0.470678448677063 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 81 reward tensor([-1], device='cuda:0') loss 0.24912269413471222 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 82 reward tensor([-1], device='cuda:0') loss 0.373121976852417 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 83 reward tensor([-1], device='cuda:0') loss 0.4142601788043976 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 84 reward tensor([-1], device='cuda:0') loss 0.2270296812057495 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 85 reward tensor([-1], device='cuda:0') loss 0.41423696279525757 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 86 reward tensor([-1], device='cuda:0') loss 0.20196682214736938 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 87 reward tensor([-1], device='cuda:0') loss 0.5339269042015076 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 88 reward tensor([-1], device='cuda:0') loss 0.3593181371688843 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 89 reward tensor([-1], device='cuda:0') loss 0.5361016392707825 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 90 reward tensor([-1], device='cuda:0') loss 0.22350412607192993 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 91 reward tensor([-1], device='cuda:0') loss 0.19940492510795593 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 92 reward tensor([-1], device='cuda:0') loss 0.4221467971801758 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 93 reward tensor([-1], device='cuda:0') loss 0.3106551170349121 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 94 reward tensor([-1], device='cuda:0') loss 0.19660040736198425 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 95 reward tensor([-1], device='cuda:0') loss 0.20807194709777832 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 96 reward tensor([-1], device='cuda:0') loss 0.22495365142822266 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 97 reward tensor([-1], device='cuda:0') loss 0.36581629514694214 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 98 reward tensor([-1], device='cuda:0') loss 0.2771799564361572 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 99 reward tensor([-1], device='cuda:0') loss 0.27860692143440247 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 100 reward tensor([-1], device='cuda:0') loss 0.3869938850402832 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 101 reward tensor([-1], device='cuda:0') loss 0.2558055818080902 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 102 reward tensor([-1], device='cuda:0') loss 0.42603787779808044 epsilon 0.650684809722428
26-Feb-25 13:31:59 - agent.DQN.DQN - INFO - episode 8 step 103 reward tensor([-1], device='cuda:0') loss 0.1890118271112442 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 104 reward tensor([-1], device='cuda:0') loss 0.20349858701229095 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 105 reward tensor([-1], device='cuda:0') loss 0.34724268317222595 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 106 reward tensor([-1], device='cuda:0') loss 0.2514978051185608 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 107 reward tensor([-1], device='cuda:0') loss 0.35700535774230957 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 108 reward tensor([-1], device='cuda:0') loss 0.28138792514801025 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 109 reward tensor([-1], device='cuda:0') loss 0.2127106934785843 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 110 reward tensor([-1], device='cuda:0') loss 0.20321379601955414 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 111 reward tensor([-1], device='cuda:0') loss 0.19058948755264282 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 112 reward tensor([-1], device='cuda:0') loss 0.22917082905769348 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 113 reward tensor([-1], device='cuda:0') loss 0.21489214897155762 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 114 reward tensor([-1], device='cuda:0') loss 0.22979901731014252 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 115 reward tensor([-1], device='cuda:0') loss 0.3600010275840759 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 116 reward tensor([-1], device='cuda:0') loss 0.5529545545578003 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 117 reward tensor([-1], device='cuda:0') loss 0.18603113293647766 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 118 reward tensor([-1], device='cuda:0') loss 0.2995951175689697 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 119 reward tensor([-1], device='cuda:0') loss 0.5537891387939453 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 120 reward tensor([-1], device='cuda:0') loss 0.20497380197048187 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 121 reward tensor([-1], device='cuda:0') loss 0.6432340741157532 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 122 reward tensor([-1], device='cuda:0') loss 0.24205735325813293 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 123 reward tensor([-1], device='cuda:0') loss 0.2678300738334656 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 124 reward tensor([-1], device='cuda:0') loss 0.24743404984474182 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 125 reward tensor([-1], device='cuda:0') loss 0.41695111989974976 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 126 reward tensor([-1], device='cuda:0') loss 0.1844930350780487 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 127 reward tensor([-1], device='cuda:0') loss 0.496049165725708 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 128 reward tensor([-1], device='cuda:0') loss 0.20269706845283508 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 129 reward tensor([-1], device='cuda:0') loss 0.319325715303421 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 130 reward tensor([-1], device='cuda:0') loss 0.18950939178466797 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 131 reward tensor([-1], device='cuda:0') loss 0.27269798517227173 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 132 reward tensor([-1], device='cuda:0') loss 0.3333697021007538 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 133 reward tensor([-1], device='cuda:0') loss 0.25237488746643066 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 134 reward tensor([-1], device='cuda:0') loss 0.24147038161754608 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 135 reward tensor([-1], device='cuda:0') loss 0.2645447850227356 epsilon 0.650684809722428
26-Feb-25 13:32:00 - agent.DQN.DQN - INFO - episode 8 step 136 reward tensor([-1], device='cuda:0') loss 0.30206236243247986 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 137 reward tensor([-1], device='cuda:0') loss 0.48142874240875244 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 138 reward tensor([-1], device='cuda:0') loss 0.3353649973869324 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 139 reward tensor([-1], device='cuda:0') loss 0.20864294469356537 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 140 reward tensor([-1], device='cuda:0') loss 0.3370361924171448 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 141 reward tensor([-1], device='cuda:0') loss 0.1984492540359497 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 142 reward tensor([-1], device='cuda:0') loss 0.26861709356307983 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 143 reward tensor([-1], device='cuda:0') loss 0.2502976357936859 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 144 reward tensor([-1], device='cuda:0') loss 0.2807410955429077 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 145 reward tensor([-1], device='cuda:0') loss 0.22397713363170624 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 146 reward tensor([-1], device='cuda:0') loss 0.1836870312690735 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 147 reward tensor([-1], device='cuda:0') loss 0.1605994552373886 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 148 reward tensor([-1], device='cuda:0') loss 0.17659926414489746 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 149 reward tensor([-1], device='cuda:0') loss 0.48683762550354004 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 150 reward tensor([-1], device='cuda:0') loss 0.32468175888061523 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 151 reward tensor([-1], device='cuda:0') loss 0.30848976969718933 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 152 reward tensor([-1], device='cuda:0') loss 0.3010520339012146 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 153 reward tensor([-1], device='cuda:0') loss 0.21395790576934814 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 154 reward tensor([-1], device='cuda:0') loss 0.17363204061985016 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 155 reward tensor([-1], device='cuda:0') loss 0.2857791781425476 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 156 reward tensor([-1], device='cuda:0') loss 0.346983939409256 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 157 reward tensor([-1], device='cuda:0') loss 0.34050822257995605 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 158 reward tensor([-1], device='cuda:0') loss 0.2271147072315216 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 159 reward tensor([-1], device='cuda:0') loss 0.21282508969306946 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 160 reward tensor([-1], device='cuda:0') loss 0.31596487760543823 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 161 reward tensor([-1], device='cuda:0') loss 0.48743435740470886 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 162 reward tensor([-1], device='cuda:0') loss 0.21871058642864227 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 163 reward tensor([-1], device='cuda:0') loss 0.33645981550216675 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 164 reward tensor([-1], device='cuda:0') loss 0.22396960854530334 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 165 reward tensor([-1], device='cuda:0') loss 0.16774269938468933 epsilon 0.650684809722428
26-Feb-25 13:32:01 - agent.DQN.DQN - INFO - episode 8 step 166 reward tensor([-1], device='cuda:0') loss 0.29684877395629883 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 167 reward tensor([-1], device='cuda:0') loss 0.2238718867301941 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 168 reward tensor([-1], device='cuda:0') loss 0.2848018407821655 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 169 reward tensor([-1], device='cuda:0') loss 0.2134387493133545 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 170 reward tensor([-1], device='cuda:0') loss 0.2603526711463928 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 171 reward tensor([-1], device='cuda:0') loss 0.48057183623313904 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 172 reward tensor([-1], device='cuda:0') loss 0.13947129249572754 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 173 reward tensor([-1], device='cuda:0') loss 0.20348796248435974 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 174 reward tensor([-1], device='cuda:0') loss 0.33242931962013245 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 175 reward tensor([-1], device='cuda:0') loss 0.2180853933095932 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 176 reward tensor([-1], device='cuda:0') loss 0.22268372774124146 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 177 reward tensor([-1], device='cuda:0') loss 0.30668169260025024 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 178 reward tensor([-1], device='cuda:0') loss 0.23107776045799255 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 179 reward tensor([-1], device='cuda:0') loss 0.35998013615608215 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 180 reward tensor([-1], device='cuda:0') loss 0.19992074370384216 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 181 reward tensor([-1], device='cuda:0') loss 0.2765439748764038 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 182 reward tensor([-1], device='cuda:0') loss 0.33718377351760864 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 183 reward tensor([-1], device='cuda:0') loss 0.22188109159469604 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 184 reward tensor([-1], device='cuda:0') loss 0.41424560546875 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 185 reward tensor([-1], device='cuda:0') loss 0.34739774465560913 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 186 reward tensor([-1], device='cuda:0') loss 0.20847360789775848 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 187 reward tensor([-1], device='cuda:0') loss 0.20207396149635315 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 188 reward tensor([-1], device='cuda:0') loss 0.25385767221450806 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 189 reward tensor([-1], device='cuda:0') loss 0.2057628482580185 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 190 reward tensor([-1], device='cuda:0') loss 0.19638027250766754 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 191 reward tensor([-1], device='cuda:0') loss 0.1895321011543274 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 192 reward tensor([-1], device='cuda:0') loss 0.33019715547561646 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 193 reward tensor([-1], device='cuda:0') loss 0.1910921186208725 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 194 reward tensor([-1], device='cuda:0') loss 0.5196059346199036 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 195 reward tensor([-1], device='cuda:0') loss 0.48783037066459656 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 196 reward tensor([-1], device='cuda:0') loss 0.45544928312301636 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 197 reward tensor([-1], device='cuda:0') loss 0.30397310853004456 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 198 reward tensor([-1], device='cuda:0') loss 0.4368543326854706 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 199 reward tensor([-1], device='cuda:0') loss 0.18337851762771606 epsilon 0.650684809722428
26-Feb-25 13:32:02 - agent.DQN.DQN - INFO - episode 8 step 200 reward tensor([-1], device='cuda:0') loss 0.20171311497688293 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 201 reward tensor([-1], device='cuda:0') loss 0.18778672814369202 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 202 reward tensor([-1], device='cuda:0') loss 0.31513309478759766 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 203 reward tensor([-1], device='cuda:0') loss 0.19660291075706482 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 204 reward tensor([-1], device='cuda:0') loss 0.24393978714942932 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 205 reward tensor([-1], device='cuda:0') loss 0.433432400226593 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 206 reward tensor([-1], device='cuda:0') loss 0.3330480456352234 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 207 reward tensor([-1], device='cuda:0') loss 0.43736669421195984 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 208 reward tensor([-1], device='cuda:0') loss 0.23280566930770874 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 209 reward tensor([-1], device='cuda:0') loss 0.5530505180358887 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 210 reward tensor([-1], device='cuda:0') loss 0.4149209260940552 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 211 reward tensor([-1], device='cuda:0') loss 0.4633033275604248 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 212 reward tensor([-1], device='cuda:0') loss 0.34695035219192505 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 213 reward tensor([-1], device='cuda:0') loss 0.31412094831466675 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 214 reward tensor([-1], device='cuda:0') loss 0.22169026732444763 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 215 reward tensor([-1], device='cuda:0') loss 0.22588294744491577 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 216 reward tensor([-1], device='cuda:0') loss 0.3632868528366089 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 217 reward tensor([-1], device='cuda:0') loss 0.1639794558286667 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 218 reward tensor([-1], device='cuda:0') loss 0.26747819781303406 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 219 reward tensor([-1], device='cuda:0') loss 0.3358660340309143 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 220 reward tensor([-1], device='cuda:0') loss 0.2610747516155243 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 221 reward tensor([-1], device='cuda:0') loss 0.46916159987449646 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 222 reward tensor([-1], device='cuda:0') loss 0.16854090988636017 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 223 reward tensor([-1], device='cuda:0') loss 0.27979445457458496 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 224 reward tensor([-1], device='cuda:0') loss 0.24771487712860107 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 225 reward tensor([-1], device='cuda:0') loss 0.27926236391067505 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 226 reward tensor([-1], device='cuda:0') loss 0.3170826733112335 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 227 reward tensor([-1], device='cuda:0') loss 0.27731800079345703 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 228 reward tensor([-1], device='cuda:0') loss 0.4161354899406433 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 229 reward tensor([-1], device='cuda:0') loss 0.39063531160354614 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 230 reward tensor([-1], device='cuda:0') loss 0.18269076943397522 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 231 reward tensor([-1], device='cuda:0') loss 0.21424269676208496 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 232 reward tensor([-1], device='cuda:0') loss 0.35997918248176575 epsilon 0.650684809722428
26-Feb-25 13:32:03 - agent.DQN.DQN - INFO - episode 8 step 233 reward tensor([-1], device='cuda:0') loss 0.27059489488601685 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 234 reward tensor([-1], device='cuda:0') loss 0.6729106307029724 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 235 reward tensor([-1], device='cuda:0') loss 0.3033532500267029 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 236 reward tensor([-1], device='cuda:0') loss 0.2778798043727875 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 237 reward tensor([-1], device='cuda:0') loss 0.3272077739238739 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 238 reward tensor([-1], device='cuda:0') loss 0.23693153262138367 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 239 reward tensor([-1], device='cuda:0') loss 0.20415957272052765 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 240 reward tensor([-1], device='cuda:0') loss 0.1730315387248993 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 241 reward tensor([-1], device='cuda:0') loss 0.1841762661933899 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 242 reward tensor([-1], device='cuda:0') loss 0.2983039915561676 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 243 reward tensor([-1], device='cuda:0') loss 0.22755829989910126 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 244 reward tensor([-1], device='cuda:0') loss 0.3395180106163025 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 245 reward tensor([-1], device='cuda:0') loss 0.23164132237434387 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 246 reward tensor([-1], device='cuda:0') loss 0.16839317977428436 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 247 reward tensor([-1], device='cuda:0') loss 0.17866751551628113 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 248 reward tensor([-1], device='cuda:0') loss 0.20377305150032043 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 249 reward tensor([-1], device='cuda:0') loss 0.20201846957206726 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 250 reward tensor([-1], device='cuda:0') loss 0.8192168474197388 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 251 reward tensor([-1], device='cuda:0') loss 0.7000067234039307 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 252 reward tensor([-1], device='cuda:0') loss 0.19613274931907654 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 253 reward tensor([-1], device='cuda:0') loss 0.17250874638557434 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 254 reward tensor([-1], device='cuda:0') loss 0.16670197248458862 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 255 reward tensor([-1], device='cuda:0') loss 0.20417848229408264 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 256 reward tensor([-1], device='cuda:0') loss 0.2100294679403305 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 257 reward tensor([-1], device='cuda:0') loss 0.4070756137371063 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 258 reward tensor([-1], device='cuda:0') loss 0.32325685024261475 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 259 reward tensor([-1], device='cuda:0') loss 0.32621073722839355 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 260 reward tensor([-1], device='cuda:0') loss 0.2333764284849167 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 261 reward tensor([-1], device='cuda:0') loss 0.29240500926971436 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 262 reward tensor([-1], device='cuda:0') loss 0.3116905689239502 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 263 reward tensor([-1], device='cuda:0') loss 0.32359778881073 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 264 reward tensor([-1], device='cuda:0') loss 0.35548555850982666 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 265 reward tensor([-1], device='cuda:0') loss 0.26757481694221497 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 266 reward tensor([-1], device='cuda:0') loss 0.22949156165122986 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 267 reward tensor([-1], device='cuda:0') loss 0.20445120334625244 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 268 reward tensor([-1], device='cuda:0') loss 0.3030872344970703 epsilon 0.650684809722428
26-Feb-25 13:32:04 - agent.DQN.DQN - INFO - episode 8 step 269 reward tensor([-1], device='cuda:0') loss 0.3924833834171295 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 270 reward tensor([-1], device='cuda:0') loss 0.46850964426994324 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 271 reward tensor([-1], device='cuda:0') loss 0.38355857133865356 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 272 reward tensor([-1], device='cuda:0') loss 0.24498224258422852 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 273 reward tensor([-1], device='cuda:0') loss 0.22558492422103882 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 274 reward tensor([-1], device='cuda:0') loss 0.27044403553009033 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 275 reward tensor([-1], device='cuda:0') loss 0.2512539029121399 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 276 reward tensor([-1], device='cuda:0') loss 0.21710139513015747 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 277 reward tensor([-1], device='cuda:0') loss 0.16568900644779205 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 278 reward tensor([-1], device='cuda:0') loss 0.4253964424133301 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 279 reward tensor([-1], device='cuda:0') loss 0.23461535573005676 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 280 reward tensor([-1], device='cuda:0') loss 0.3595097064971924 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 281 reward tensor([-1], device='cuda:0') loss 0.2445976436138153 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 282 reward tensor([-1], device='cuda:0') loss 0.19859924912452698 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 283 reward tensor([-1], device='cuda:0') loss 0.26375365257263184 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 284 reward tensor([-1], device='cuda:0') loss 0.21205633878707886 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 285 reward tensor([-1], device='cuda:0') loss 0.2471267133951187 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 286 reward tensor([-1], device='cuda:0') loss 0.22384142875671387 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 287 reward tensor([-1], device='cuda:0') loss 0.3079286813735962 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 288 reward tensor([-1], device='cuda:0') loss 0.3821340799331665 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 289 reward tensor([-1], device='cuda:0') loss 0.2755887508392334 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 290 reward tensor([-1], device='cuda:0') loss 0.35514211654663086 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 291 reward tensor([-1], device='cuda:0') loss 0.25379425287246704 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 292 reward tensor([-1], device='cuda:0') loss 0.18855957686901093 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 293 reward tensor([-1], device='cuda:0') loss 0.3164314925670624 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 294 reward tensor([-1], device='cuda:0') loss 0.25103795528411865 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 295 reward tensor([-1], device='cuda:0') loss 0.1847747266292572 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 296 reward tensor([-1], device='cuda:0') loss 0.1876498907804489 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 297 reward tensor([-1], device='cuda:0') loss 0.17361322045326233 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 298 reward tensor([-1], device='cuda:0') loss 0.23851564526557922 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 299 reward tensor([-1], device='cuda:0') loss 0.1755293309688568 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 300 reward tensor([-1], device='cuda:0') loss 0.2796190679073334 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 8 step 301 reward tensor([-1], device='cuda:0') loss 0.24819794297218323 epsilon 0.650684809722428
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 9 step 0 reward tensor([-1], device='cuda:0') loss 0.2080249935388565 epsilon 0.6295375534064491
26-Feb-25 13:32:05 - agent.DQN.DQN - INFO - episode 9 step 1 reward tensor([-1], device='cuda:0') loss 0.17449410259723663 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 2 reward tensor([-1], device='cuda:0') loss 0.8316187858581543 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 3 reward tensor([-1], device='cuda:0') loss 0.4447610378265381 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 4 reward tensor([-1], device='cuda:0') loss 0.17619770765304565 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 5 reward tensor([-1], device='cuda:0') loss 0.3600984811782837 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 6 reward tensor([-1], device='cuda:0') loss 0.4665062725543976 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 7 reward tensor([-1], device='cuda:0') loss 0.2888692021369934 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 8 reward tensor([-1], device='cuda:0') loss 0.19815924763679504 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 9 reward tensor([-1], device='cuda:0') loss 0.1998036503791809 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 10 reward tensor([-1], device='cuda:0') loss 3.1058475971221924 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 11 reward tensor([-1], device='cuda:0') loss 0.4032847285270691 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 12 reward tensor([-1], device='cuda:0') loss 0.549142062664032 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 13 reward tensor([-1], device='cuda:0') loss 0.7198620438575745 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 14 reward tensor([-1], device='cuda:0') loss 0.8166750073432922 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 15 reward tensor([-1], device='cuda:0') loss 0.3145485520362854 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 16 reward tensor([-1], device='cuda:0') loss 0.27308931946754456 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 17 reward tensor([-1], device='cuda:0') loss 0.29914751648902893 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 18 reward tensor([-1], device='cuda:0') loss 0.30487295985221863 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 19 reward tensor([-1], device='cuda:0') loss 0.4435640275478363 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 20 reward tensor([-1], device='cuda:0') loss 0.21767330169677734 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 21 reward tensor([-1], device='cuda:0') loss 0.45376521348953247 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 22 reward tensor([-1], device='cuda:0') loss 0.4278942048549652 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 23 reward tensor([-1], device='cuda:0') loss 0.24929189682006836 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 24 reward tensor([-1], device='cuda:0') loss 0.27965718507766724 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 25 reward tensor([-1], device='cuda:0') loss 0.23794761300086975 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 26 reward tensor([-1], device='cuda:0') loss 0.23066630959510803 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 27 reward tensor([-1], device='cuda:0') loss 0.20073391497135162 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 28 reward tensor([-1], device='cuda:0') loss 2.178520441055298 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 29 reward tensor([-1], device='cuda:0') loss 0.24896806478500366 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 30 reward tensor([-1], device='cuda:0') loss 0.4606771469116211 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 31 reward tensor([-1], device='cuda:0') loss 0.4756850302219391 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 32 reward tensor([-1], device='cuda:0') loss 0.45632147789001465 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 33 reward tensor([-1], device='cuda:0') loss 0.3024615943431854 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 34 reward tensor([-1], device='cuda:0') loss 0.6364650726318359 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 35 reward tensor([-1], device='cuda:0') loss 0.25698310136795044 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 36 reward tensor([-1], device='cuda:0') loss 0.6195282936096191 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 37 reward tensor([-1], device='cuda:0') loss 0.4506431818008423 epsilon 0.6295375534064491
26-Feb-25 13:32:06 - agent.DQN.DQN - INFO - episode 9 step 38 reward tensor([-1], device='cuda:0') loss 0.2771146893501282 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 39 reward tensor([-1], device='cuda:0') loss 3.2563488483428955 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 40 reward tensor([-1], device='cuda:0') loss 0.2527441382408142 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 41 reward tensor([-1], device='cuda:0') loss 0.43376195430755615 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 42 reward tensor([-1], device='cuda:0') loss 0.9551441073417664 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 43 reward tensor([-1], device='cuda:0') loss 0.8994364738464355 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 44 reward tensor([-1], device='cuda:0') loss 0.6980266571044922 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 45 reward tensor([-1], device='cuda:0') loss 0.29005059599876404 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 46 reward tensor([-1], device='cuda:0') loss 0.2533760666847229 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 47 reward tensor([-1], device='cuda:0') loss 0.23765720427036285 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 48 reward tensor([-1], device='cuda:0') loss 1.062861442565918 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 49 reward tensor([-1], device='cuda:0') loss 0.22365260124206543 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 50 reward tensor([-1], device='cuda:0') loss 0.7552518844604492 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 51 reward tensor([-1], device='cuda:0') loss 2.809230327606201 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 52 reward tensor([-1], device='cuda:0') loss 0.26133689284324646 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 53 reward tensor([-1], device='cuda:0') loss 0.3433547914028168 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 54 reward tensor([-1], device='cuda:0') loss 0.4868849813938141 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 55 reward tensor([-1], device='cuda:0') loss 0.5247083902359009 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 56 reward tensor([-1], device='cuda:0') loss 0.7799052596092224 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 57 reward tensor([-1], device='cuda:0') loss 0.208514004945755 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 58 reward tensor([-1], device='cuda:0') loss 0.38208651542663574 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 59 reward tensor([-1], device='cuda:0') loss 0.24474400281906128 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 60 reward tensor([-1], device='cuda:0') loss 0.23361429572105408 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 61 reward tensor([-1], device='cuda:0') loss 0.3525954782962799 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 62 reward tensor([-1], device='cuda:0') loss 0.7327408194541931 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 63 reward tensor([-1], device='cuda:0') loss 0.30032214522361755 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 64 reward tensor([-1], device='cuda:0') loss 0.28712528944015503 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 65 reward tensor([-1], device='cuda:0') loss 0.38066697120666504 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 66 reward tensor([-1], device='cuda:0') loss 0.35813239216804504 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 67 reward tensor([-1], device='cuda:0') loss 0.5442965030670166 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 68 reward tensor([-1], device='cuda:0') loss 0.210563063621521 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 69 reward tensor([-1], device='cuda:0') loss 1.9083595275878906 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 70 reward tensor([-1], device='cuda:0') loss 0.4521317481994629 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 71 reward tensor([-1], device='cuda:0') loss 0.27782735228538513 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 72 reward tensor([-1], device='cuda:0') loss 1.5823192596435547 epsilon 0.6295375534064491
26-Feb-25 13:32:07 - agent.DQN.DQN - INFO - episode 9 step 73 reward tensor([-1], device='cuda:0') loss 0.36088693141937256 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 74 reward tensor([-1], device='cuda:0') loss 1.357717514038086 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 75 reward tensor([-1], device='cuda:0') loss 0.546058177947998 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 76 reward tensor([-1], device='cuda:0') loss 0.24632316827774048 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 77 reward tensor([-1], device='cuda:0') loss 0.3506062924861908 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 78 reward tensor([-1], device='cuda:0') loss 0.18226036429405212 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 79 reward tensor([-1], device='cuda:0') loss 0.36764389276504517 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 80 reward tensor([-1], device='cuda:0') loss 0.7256577014923096 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 81 reward tensor([-1], device='cuda:0') loss 0.3493686318397522 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 82 reward tensor([-1], device='cuda:0') loss 0.42258232831954956 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 83 reward tensor([-1], device='cuda:0') loss 2.203131675720215 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 84 reward tensor([-1], device='cuda:0') loss 0.487110435962677 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 85 reward tensor([-1], device='cuda:0') loss 0.3826237916946411 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 86 reward tensor([-1], device='cuda:0') loss 0.5314691066741943 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 87 reward tensor([-1], device='cuda:0') loss 0.2933759093284607 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 88 reward tensor([-1], device='cuda:0') loss 0.4766073226928711 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 89 reward tensor([-1], device='cuda:0') loss 0.5442551374435425 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 90 reward tensor([-1], device='cuda:0') loss 0.6627577543258667 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 91 reward tensor([-1], device='cuda:0') loss 0.6681778430938721 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 92 reward tensor([-1], device='cuda:0') loss 0.32650452852249146 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 93 reward tensor([-1], device='cuda:0') loss 0.32564276456832886 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 94 reward tensor([-1], device='cuda:0') loss 0.38151976466178894 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 95 reward tensor([-1], device='cuda:0') loss 0.2912635803222656 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 96 reward tensor([-1], device='cuda:0') loss 0.38214510679244995 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 97 reward tensor([-1], device='cuda:0') loss 0.41281571984291077 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 98 reward tensor([-1], device='cuda:0') loss 0.22987180948257446 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 99 reward tensor([-1], device='cuda:0') loss 0.2870308756828308 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 100 reward tensor([-1], device='cuda:0') loss 0.3044432997703552 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 101 reward tensor([-1], device='cuda:0') loss 0.23905806243419647 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 102 reward tensor([-1], device='cuda:0') loss 0.5684462189674377 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 103 reward tensor([-1], device='cuda:0') loss 0.5061437487602234 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 104 reward tensor([-1], device='cuda:0') loss 0.17461439967155457 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 105 reward tensor([-1], device='cuda:0') loss 0.3520156443119049 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 106 reward tensor([-1], device='cuda:0') loss 0.3380570411682129 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 107 reward tensor([-1], device='cuda:0') loss 0.4394267797470093 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 108 reward tensor([-1], device='cuda:0') loss 0.15431873500347137 epsilon 0.6295375534064491
26-Feb-25 13:32:08 - agent.DQN.DQN - INFO - episode 9 step 109 reward tensor([-1], device='cuda:0') loss 0.49026980996131897 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 110 reward tensor([-1], device='cuda:0') loss 0.4017190933227539 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 111 reward tensor([-1], device='cuda:0') loss 0.22235217690467834 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 112 reward tensor([-1], device='cuda:0') loss 0.25238102674484253 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 113 reward tensor([-1], device='cuda:0') loss 0.4018704891204834 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 114 reward tensor([-1], device='cuda:0') loss 0.34894102811813354 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 115 reward tensor([-1], device='cuda:0') loss 0.32602477073669434 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 116 reward tensor([-1], device='cuda:0') loss 1.0920554399490356 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 117 reward tensor([-1], device='cuda:0') loss 0.38111430406570435 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 118 reward tensor([-1], device='cuda:0') loss 0.3348543047904968 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 119 reward tensor([-1], device='cuda:0') loss 0.6933759450912476 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 120 reward tensor([-1], device='cuda:0') loss 0.33317694067955017 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 121 reward tensor([-1], device='cuda:0') loss 0.467223584651947 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 122 reward tensor([-1], device='cuda:0') loss 0.33442211151123047 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 123 reward tensor([-1], device='cuda:0') loss 0.3794376850128174 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 124 reward tensor([-1], device='cuda:0') loss 0.37403547763824463 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 125 reward tensor([-1], device='cuda:0') loss 0.346760094165802 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 126 reward tensor([-1], device='cuda:0') loss 0.18544121086597443 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 127 reward tensor([-1], device='cuda:0') loss 0.597795307636261 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 128 reward tensor([-1], device='cuda:0') loss 0.24840067327022552 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 129 reward tensor([-1], device='cuda:0') loss 1.3325610160827637 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 130 reward tensor([-1], device='cuda:0') loss 0.42827263474464417 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 131 reward tensor([-1], device='cuda:0') loss 0.18820571899414062 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 132 reward tensor([-1], device='cuda:0') loss 0.5761958956718445 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 133 reward tensor([-1], device='cuda:0') loss 0.35327422618865967 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 134 reward tensor([-1], device='cuda:0') loss 0.42437365651130676 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 135 reward tensor([-1], device='cuda:0') loss 0.5529087781906128 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 136 reward tensor([-1], device='cuda:0') loss 0.2953065037727356 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 137 reward tensor([-1], device='cuda:0') loss 0.23767739534378052 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 138 reward tensor([-1], device='cuda:0') loss 0.31973201036453247 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 139 reward tensor([-1], device='cuda:0') loss 0.26739802956581116 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 140 reward tensor([-1], device='cuda:0') loss 0.287920206785202 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 141 reward tensor([-1], device='cuda:0') loss 0.3697546124458313 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 142 reward tensor([-1], device='cuda:0') loss 0.5321301221847534 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 143 reward tensor([-1], device='cuda:0') loss 0.31424739956855774 epsilon 0.6295375534064491
26-Feb-25 13:32:09 - agent.DQN.DQN - INFO - episode 9 step 144 reward tensor([-1], device='cuda:0') loss 0.19883473217487335 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 145 reward tensor([-1], device='cuda:0') loss 0.41634583473205566 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 146 reward tensor([-1], device='cuda:0') loss 0.5463714003562927 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 147 reward tensor([-1], device='cuda:0') loss 0.49821382761001587 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 148 reward tensor([-1], device='cuda:0') loss 0.21795441210269928 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 149 reward tensor([-1], device='cuda:0') loss 0.3027246594429016 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 150 reward tensor([-1], device='cuda:0') loss 0.5162930488586426 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 151 reward tensor([-1], device='cuda:0') loss 0.31513291597366333 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 152 reward tensor([-1], device='cuda:0') loss 0.4539702534675598 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 153 reward tensor([-1], device='cuda:0') loss 0.33946701884269714 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 154 reward tensor([-1], device='cuda:0') loss 0.2934361696243286 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 155 reward tensor([-1], device='cuda:0') loss 0.2506193518638611 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 156 reward tensor([-1], device='cuda:0') loss 0.2701948881149292 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 157 reward tensor([-1], device='cuda:0') loss 0.3902018368244171 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 158 reward tensor([-1], device='cuda:0') loss 0.8841001391410828 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 159 reward tensor([-1], device='cuda:0') loss 0.25024890899658203 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 160 reward tensor([-1], device='cuda:0') loss 0.22087346017360687 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 161 reward tensor([-1], device='cuda:0') loss 0.2126443088054657 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 162 reward tensor([-1], device='cuda:0') loss 0.2260298728942871 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 163 reward tensor([-1], device='cuda:0') loss 0.1897915005683899 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 164 reward tensor([-1], device='cuda:0') loss 0.23467470705509186 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 165 reward tensor([-1], device='cuda:0') loss 0.28429102897644043 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 166 reward tensor([-1], device='cuda:0') loss 0.27430489659309387 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 167 reward tensor([-1], device='cuda:0') loss 0.22885407507419586 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 168 reward tensor([-1], device='cuda:0') loss 0.8181735277175903 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 169 reward tensor([-1], device='cuda:0') loss 0.2764636278152466 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 170 reward tensor([-1], device='cuda:0') loss 0.5190029144287109 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 171 reward tensor([-1], device='cuda:0') loss 0.33066579699516296 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 172 reward tensor([-1], device='cuda:0') loss 0.4980262815952301 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 173 reward tensor([-1], device='cuda:0') loss 0.3323982059955597 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 174 reward tensor([-1], device='cuda:0') loss 0.36931025981903076 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 175 reward tensor([-1], device='cuda:0') loss 0.2652438282966614 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 176 reward tensor([-1], device='cuda:0') loss 0.3543776869773865 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 177 reward tensor([-1], device='cuda:0') loss 0.7468439936637878 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 178 reward tensor([-1], device='cuda:0') loss 0.38113167881965637 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 179 reward tensor([-1], device='cuda:0') loss 0.20867592096328735 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 180 reward tensor([-1], device='cuda:0') loss 0.6325749158859253 epsilon 0.6295375534064491
26-Feb-25 13:32:10 - agent.DQN.DQN - INFO - episode 9 step 181 reward tensor([-1], device='cuda:0') loss 0.3127497732639313 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 182 reward tensor([-1], device='cuda:0') loss 0.29606500267982483 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 183 reward tensor([-1], device='cuda:0') loss 0.2347751408815384 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 184 reward tensor([-1], device='cuda:0') loss 0.5453197956085205 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 185 reward tensor([-1], device='cuda:0') loss 0.38704052567481995 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 186 reward tensor([-1], device='cuda:0') loss 0.3204301595687866 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 187 reward tensor([-1], device='cuda:0') loss 0.27986228466033936 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 188 reward tensor([-1], device='cuda:0') loss 0.1868455857038498 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 189 reward tensor([-1], device='cuda:0') loss 0.6746334433555603 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 190 reward tensor([-1], device='cuda:0') loss 0.20500753819942474 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 191 reward tensor([-1], device='cuda:0') loss 0.19693328440189362 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 192 reward tensor([-1], device='cuda:0') loss 0.29685458540916443 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 193 reward tensor([-1], device='cuda:0') loss 0.18694764375686646 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 194 reward tensor([-1], device='cuda:0') loss 0.5102992653846741 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 195 reward tensor([-1], device='cuda:0') loss 0.30595719814300537 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 196 reward tensor([-1], device='cuda:0') loss 0.588080644607544 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 197 reward tensor([-1], device='cuda:0') loss 0.3455362915992737 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 198 reward tensor([-1], device='cuda:0') loss 0.48132872581481934 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 199 reward tensor([-1], device='cuda:0') loss 0.49052807688713074 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 200 reward tensor([-1], device='cuda:0') loss 0.1960214227437973 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 201 reward tensor([-1], device='cuda:0') loss 0.22828930616378784 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 202 reward tensor([-1], device='cuda:0') loss 0.5179460048675537 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 203 reward tensor([-1], device='cuda:0') loss 0.3245639503002167 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 204 reward tensor([-1], device='cuda:0') loss 0.28329339623451233 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 205 reward tensor([-1], device='cuda:0') loss 0.6498239040374756 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 206 reward tensor([-1], device='cuda:0') loss 0.3305271863937378 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 207 reward tensor([-1], device='cuda:0') loss 0.4356061816215515 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 208 reward tensor([-1], device='cuda:0') loss 0.2004081904888153 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 209 reward tensor([-1], device='cuda:0') loss 0.1977379024028778 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 210 reward tensor([-1], device='cuda:0') loss 0.2383381724357605 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 211 reward tensor([-1], device='cuda:0') loss 0.21581627428531647 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 212 reward tensor([-1], device='cuda:0') loss 0.2542293667793274 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 213 reward tensor([-1], device='cuda:0') loss 0.2811877131462097 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 214 reward tensor([-1], device='cuda:0') loss 0.23917481303215027 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 215 reward tensor([-1], device='cuda:0') loss 0.3825075328350067 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 216 reward tensor([-1], device='cuda:0') loss 0.19374904036521912 epsilon 0.6295375534064491
26-Feb-25 13:32:11 - agent.DQN.DQN - INFO - episode 9 step 217 reward tensor([-1], device='cuda:0') loss 0.1932581067085266 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 218 reward tensor([-1], device='cuda:0') loss 0.17163872718811035 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 219 reward tensor([-1], device='cuda:0') loss 0.26471367478370667 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 220 reward tensor([-1], device='cuda:0') loss 0.254494309425354 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 221 reward tensor([-1], device='cuda:0') loss 0.12418349087238312 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 222 reward tensor([-1], device='cuda:0') loss 0.6254327893257141 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 223 reward tensor([-1], device='cuda:0') loss 0.1635996550321579 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 224 reward tensor([-1], device='cuda:0') loss 0.8882957696914673 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 225 reward tensor([-1], device='cuda:0') loss 0.26450568437576294 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 226 reward tensor([-1], device='cuda:0') loss 0.3340771496295929 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 227 reward tensor([-1], device='cuda:0') loss 0.35074421763420105 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 228 reward tensor([-1], device='cuda:0') loss 0.376223623752594 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 229 reward tensor([-1], device='cuda:0') loss 0.24929821491241455 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 230 reward tensor([-1], device='cuda:0') loss 0.6147632002830505 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 231 reward tensor([-1], device='cuda:0') loss 0.24455270171165466 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 232 reward tensor([-1], device='cuda:0') loss 0.27681005001068115 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 233 reward tensor([-1], device='cuda:0') loss 0.2757343053817749 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 234 reward tensor([-1], device='cuda:0') loss 0.30173635482788086 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 235 reward tensor([-1], device='cuda:0') loss 0.24122174084186554 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 236 reward tensor([-1], device='cuda:0') loss 0.3702554702758789 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 237 reward tensor([-1], device='cuda:0') loss 0.31666216254234314 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 238 reward tensor([-1], device='cuda:0') loss 0.15074467658996582 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 239 reward tensor([-1], device='cuda:0') loss 0.32168179750442505 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 240 reward tensor([-1], device='cuda:0') loss 0.1964220553636551 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 241 reward tensor([-1], device='cuda:0') loss 0.25518715381622314 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 242 reward tensor([-1], device='cuda:0') loss 0.24833276867866516 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 243 reward tensor([-1], device='cuda:0') loss 0.458965927362442 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 244 reward tensor([-1], device='cuda:0') loss 0.6778022646903992 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 245 reward tensor([-1], device='cuda:0') loss 0.4520970582962036 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 246 reward tensor([-1], device='cuda:0') loss 0.2168898582458496 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 247 reward tensor([-1], device='cuda:0') loss 0.21960169076919556 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 248 reward tensor([-1], device='cuda:0') loss 0.24696098268032074 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 249 reward tensor([-1], device='cuda:0') loss 0.23189234733581543 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 250 reward tensor([-1], device='cuda:0') loss 0.1824263334274292 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 251 reward tensor([-1], device='cuda:0') loss 0.33154523372650146 epsilon 0.6295375534064491
26-Feb-25 13:32:12 - agent.DQN.DQN - INFO - episode 9 step 252 reward tensor([-1], device='cuda:0') loss 0.2190910279750824 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 253 reward tensor([-1], device='cuda:0') loss 0.33288443088531494 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 254 reward tensor([-1], device='cuda:0') loss 0.2224264144897461 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 255 reward tensor([-1], device='cuda:0') loss 0.4540713131427765 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 256 reward tensor([-1], device='cuda:0') loss 0.4354552626609802 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 257 reward tensor([-1], device='cuda:0') loss 0.19649213552474976 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 258 reward tensor([-1], device='cuda:0') loss 0.5448010563850403 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 259 reward tensor([-1], device='cuda:0') loss 0.4006752669811249 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 260 reward tensor([-1], device='cuda:0') loss 0.26919329166412354 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 261 reward tensor([-1], device='cuda:0') loss 0.29474788904190063 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 262 reward tensor([-1], device='cuda:0') loss 0.2173452079296112 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 263 reward tensor([-1], device='cuda:0') loss 0.3201875388622284 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 264 reward tensor([-1], device='cuda:0') loss 0.3027271032333374 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 265 reward tensor([-1], device='cuda:0') loss 0.33878612518310547 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 266 reward tensor([-1], device='cuda:0') loss 0.6438730955123901 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 267 reward tensor([-1], device='cuda:0') loss 0.34981292486190796 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 268 reward tensor([-1], device='cuda:0') loss 0.44559094309806824 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 269 reward tensor([-1], device='cuda:0') loss 0.2535972595214844 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 270 reward tensor([-1], device='cuda:0') loss 0.1392080932855606 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 271 reward tensor([-1], device='cuda:0') loss 0.2474854290485382 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 272 reward tensor([-1], device='cuda:0') loss 0.21089625358581543 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 273 reward tensor([-1], device='cuda:0') loss 0.3132939636707306 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 274 reward tensor([-1], device='cuda:0') loss 0.3266659677028656 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 275 reward tensor([-1], device='cuda:0') loss 0.3460681140422821 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 276 reward tensor([-1], device='cuda:0') loss 0.43525823950767517 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 277 reward tensor([-1], device='cuda:0') loss 0.5315929651260376 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 278 reward tensor([-1], device='cuda:0') loss 0.2980770170688629 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 279 reward tensor([-1], device='cuda:0') loss 0.41369113326072693 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 280 reward tensor([-1], device='cuda:0') loss 0.291927695274353 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 281 reward tensor([-1], device='cuda:0') loss 0.18887180089950562 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 282 reward tensor([-1], device='cuda:0') loss 0.20126575231552124 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 283 reward tensor([-1], device='cuda:0') loss 0.3518871068954468 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 284 reward tensor([-1], device='cuda:0') loss 0.40969011187553406 epsilon 0.6295375534064491
26-Feb-25 13:32:13 - agent.DQN.DQN - INFO - episode 9 step 285 reward tensor([-1], device='cuda:0') loss 0.2718624770641327 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 286 reward tensor([-1], device='cuda:0') loss 0.5800561904907227 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 287 reward tensor([-1], device='cuda:0') loss 0.19237610697746277 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 288 reward tensor([-1], device='cuda:0') loss 0.2125062644481659 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 289 reward tensor([-1], device='cuda:0') loss 0.47971683740615845 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 290 reward tensor([-1], device='cuda:0') loss 0.3470931053161621 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 291 reward tensor([-1], device='cuda:0') loss 0.38328611850738525 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 292 reward tensor([-1], device='cuda:0') loss 0.2877437174320221 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 293 reward tensor([-1], device='cuda:0') loss 0.2302001267671585 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 294 reward tensor([-1], device='cuda:0') loss 0.26590511202812195 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 295 reward tensor([-1], device='cuda:0') loss 0.21003657579421997 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 296 reward tensor([-1], device='cuda:0') loss 0.23113758862018585 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 297 reward tensor([-1], device='cuda:0') loss 0.35191047191619873 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 298 reward tensor([-1], device='cuda:0') loss 0.45303255319595337 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 299 reward tensor([-1], device='cuda:0') loss 0.3393547534942627 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 300 reward tensor([-1], device='cuda:0') loss 0.3343098759651184 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - agent.DQN.DQN - INFO - episode 9 step 301 reward tensor([-1], device='cuda:0') loss 0.185495063662529 epsilon 0.6295375534064491
26-Feb-25 13:32:14 - __main__ - INFO - Training finished , training time: 72.81 seconds
26-Feb-25 13:32:14 - __main__ - INFO - Training results converted to DataFrame
26-Feb-25 13:32:14 - __main__ - INFO - Trained Q-Network saved
