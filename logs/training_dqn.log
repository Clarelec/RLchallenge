27-Feb-25 11:45:48 - agent.DQN.DQN - INFO - device is cpu
27-Feb-25 11:45:48 - numexpr.utils - INFO - Note: NumExpr detected 22 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
27-Feb-25 11:45:48 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
27-Feb-25 11:45:48 - __main__ - INFO - Libraries imported
27-Feb-25 11:45:48 - __main__ - INFO - Device: cpu
27-Feb-25 11:45:48 - __main__ - INFO - Starting training of DQN2 agent
27-Feb-25 11:45:48 - __main__ - INFO - Environment initialized
27-Feb-25 11:45:48 - agent.DQN.DQN - INFO - QNetwork initialized with 10 observations and 9 actions
27-Feb-25 11:45:48 - agent.DQN.DQN - INFO - QNetwork initialized with 10 observations and 9 actions
27-Feb-25 11:45:48 - __main__ - INFO - Q-Networks initialized and synchronized
27-Feb-25 11:45:49 - __main__ - INFO - Optimizer, LR scheduler, and loss function initialized
27-Feb-25 11:45:49 - __main__ - INFO - Epsilon-greedy strategy initialized
27-Feb-25 11:45:49 - __main__ - INFO - Replay buffer initialized
27-Feb-25 11:45:49 - __main__ - INFO - Training DQN2 agent
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 128 reward tensor([-1]) loss 831.0576171875 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 129 reward tensor([-1]) loss 297.93670654296875 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 130 reward tensor([-1]) loss 71.53599548339844 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 131 reward tensor([-1]) loss 52.54444885253906 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 132 reward tensor([-1]) loss 115.41055297851562 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 133 reward tensor([-1]) loss 85.08280944824219 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 134 reward tensor([-1]) loss 47.16259002685547 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 135 reward tensor([-1]) loss 28.626087188720703 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 136 reward tensor([-1]) loss 29.300079345703125 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 137 reward tensor([-1]) loss 31.899831771850586 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 138 reward tensor([-1]) loss 29.845962524414062 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 139 reward tensor([-1]) loss 26.45358657836914 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 140 reward tensor([-1]) loss 19.07880973815918 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 141 reward tensor([-1]) loss 14.384910583496094 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 142 reward tensor([-1]) loss 14.119882583618164 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 143 reward tensor([-1]) loss 12.902074813842773 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 144 reward tensor([-1]) loss 13.47861099243164 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 145 reward tensor([-1]) loss 12.834497451782227 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 146 reward tensor([-1]) loss 11.680994033813477 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 147 reward tensor([-1]) loss 9.227117538452148 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 148 reward tensor([-1]) loss 7.360812187194824 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 149 reward tensor([-1]) loss 7.105279445648193 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 150 reward tensor([-1]) loss 6.534749984741211 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 151 reward tensor([-1]) loss 5.402472496032715 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 152 reward tensor([-1]) loss 4.677910327911377 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 153 reward tensor([-1]) loss 4.3232340812683105 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 154 reward tensor([-1]) loss 4.3850507736206055 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 155 reward tensor([-1]) loss 3.9115824699401855 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 156 reward tensor([-1]) loss 3.543792963027954 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 157 reward tensor([-1]) loss 3.321193218231201 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 158 reward tensor([-1]) loss 2.864906072616577 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 159 reward tensor([-1]) loss 2.400353193283081 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 160 reward tensor([-1]) loss 2.6588330268859863 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 161 reward tensor([-1]) loss 2.720172643661499 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 162 reward tensor([-1]) loss 2.5689196586608887 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 163 reward tensor([-1]) loss 2.5103745460510254 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 164 reward tensor([-1]) loss 2.540407180786133 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 165 reward tensor([-1]) loss 1.9693607091903687 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 166 reward tensor([-1]) loss 1.4159724712371826 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 167 reward tensor([-1]) loss 1.0711705684661865 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 168 reward tensor([-1]) loss 0.78486168384552 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 169 reward tensor([-1]) loss 0.7868644595146179 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 170 reward tensor([-1]) loss 0.82477205991745 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 171 reward tensor([-1]) loss 0.9750490188598633 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 172 reward tensor([-1]) loss 0.9640138149261475 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 173 reward tensor([-1]) loss 1.122786045074463 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 174 reward tensor([-1]) loss 1.206855297088623 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 175 reward tensor([-1]) loss 1.390618085861206 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 176 reward tensor([-1]) loss 1.154166579246521 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 177 reward tensor([-1]) loss 1.1206797361373901 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 178 reward tensor([-1]) loss 0.9103378653526306 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 179 reward tensor([-1]) loss 0.8046317100524902 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 180 reward tensor([-1]) loss 0.6077256202697754 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 181 reward tensor([-1]) loss 7.604077339172363 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 182 reward tensor([-1]) loss 5.9792327880859375 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 183 reward tensor([-1]) loss 4.593472957611084 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 184 reward tensor([-1]) loss 2.9762630462646484 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 185 reward tensor([-1]) loss 2.782599925994873 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 186 reward tensor([-1]) loss 2.3900551795959473 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 187 reward tensor([-1]) loss 2.1464552879333496 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 188 reward tensor([-1]) loss 1.9719548225402832 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 189 reward tensor([-1]) loss 2.986551284790039 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 190 reward tensor([-1]) loss 2.5684561729431152 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 191 reward tensor([-1]) loss 2.3634629249572754 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 192 reward tensor([-1]) loss 3.7220585346221924 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 193 reward tensor([-1]) loss 4.568528652191162 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 194 reward tensor([-1]) loss 2.951601266860962 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 195 reward tensor([-1]) loss 3.6551458835601807 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 196 reward tensor([-1]) loss 2.4750919342041016 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 197 reward tensor([-1]) loss 1.9841923713684082 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 198 reward tensor([-1]) loss 1.7021217346191406 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 199 reward tensor([-1]) loss 1.5947966575622559 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 200 reward tensor([-1]) loss 1.631262183189392 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 201 reward tensor([-1]) loss 1.3639219999313354 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 202 reward tensor([-1]) loss 4.133913040161133 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 203 reward tensor([-1]) loss 3.4629640579223633 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 204 reward tensor([-1]) loss 3.825695514678955 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 205 reward tensor([-1]) loss 3.3425893783569336 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 206 reward tensor([-1]) loss 2.9359099864959717 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 207 reward tensor([-1]) loss 1.7762112617492676 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 208 reward tensor([-1]) loss 2.280358076095581 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 209 reward tensor([-1]) loss 1.404194712638855 epsilon 0.82
27-Feb-25 11:45:49 - agent.DQN.DQN - INFO - episode 1 step 210 reward tensor([-1]) loss 2.013195037841797 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 211 reward tensor([-1]) loss 7.241063117980957 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 212 reward tensor([-1]) loss 7.595578670501709 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 213 reward tensor([-1]) loss 5.675088882446289 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 214 reward tensor([-1]) loss 3.7143895626068115 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 215 reward tensor([-1]) loss 3.0245957374572754 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 216 reward tensor([-1]) loss 2.3346848487854004 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 217 reward tensor([-1]) loss 2.0484530925750732 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 218 reward tensor([-1]) loss 1.5672216415405273 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 219 reward tensor([-1]) loss 1.8065541982650757 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 220 reward tensor([-1]) loss 2.0058960914611816 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 221 reward tensor([-1]) loss 1.6960110664367676 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 222 reward tensor([-1]) loss 2.100794792175293 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 223 reward tensor([-1]) loss 1.4513059854507446 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 224 reward tensor([-1]) loss 2.0074472427368164 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 225 reward tensor([-1]) loss 1.5430936813354492 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 226 reward tensor([-1]) loss 1.449353814125061 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 227 reward tensor([-1]) loss 1.6274418830871582 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 228 reward tensor([-1]) loss 1.636361002922058 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 229 reward tensor([-1]) loss 1.2421337366104126 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 230 reward tensor([-1]) loss 1.3853579759597778 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 231 reward tensor([-1]) loss 1.1847715377807617 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 232 reward tensor([-1]) loss 0.9305780529975891 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 233 reward tensor([-1]) loss 1.1407840251922607 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 234 reward tensor([-1]) loss 1.1998380422592163 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 235 reward tensor([-1]) loss 0.6728469133377075 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 236 reward tensor([-1]) loss 0.7255657315254211 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 237 reward tensor([-1]) loss 0.7923047542572021 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 238 reward tensor([-1]) loss 0.8636852502822876 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 239 reward tensor([-1]) loss 0.5976049304008484 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 240 reward tensor([-1]) loss 0.6361247897148132 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 241 reward tensor([-1]) loss 8.300458908081055 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 242 reward tensor([-1]) loss 7.2163825035095215 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 243 reward tensor([-1]) loss 6.836365699768066 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 244 reward tensor([-1]) loss 6.432218551635742 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 245 reward tensor([-1]) loss 5.24129581451416 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 246 reward tensor([-1]) loss 4.106967926025391 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 247 reward tensor([-1]) loss 3.2144103050231934 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 248 reward tensor([-1]) loss 2.5392630100250244 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 249 reward tensor([-1]) loss 1.973703145980835 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 250 reward tensor([-1]) loss 1.9528065919876099 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 251 reward tensor([-1]) loss 1.390316128730774 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 252 reward tensor([-1]) loss 1.3770549297332764 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 253 reward tensor([-1]) loss 1.460521936416626 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 254 reward tensor([-1]) loss 1.6142570972442627 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 255 reward tensor([-1]) loss 1.7883507013320923 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 256 reward tensor([-1]) loss 1.4467424154281616 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 257 reward tensor([-1]) loss 1.5634201765060425 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 258 reward tensor([-1]) loss 1.293532133102417 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 259 reward tensor([-1]) loss 1.3128336668014526 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 260 reward tensor([-1]) loss 1.0800870656967163 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 261 reward tensor([-1]) loss 1.2495718002319336 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 262 reward tensor([-1]) loss 1.0939598083496094 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 263 reward tensor([-1]) loss 0.8557211756706238 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 264 reward tensor([-1]) loss 0.7441457509994507 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 265 reward tensor([-1]) loss 0.8591949939727783 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 266 reward tensor([-1]) loss 1.2534444332122803 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 267 reward tensor([-1]) loss 0.7424291968345642 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 268 reward tensor([-1]) loss 0.8203510046005249 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 269 reward tensor([-1]) loss 0.7885711193084717 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 270 reward tensor([-1]) loss 0.6514379978179932 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 271 reward tensor([-1]) loss 9.139110565185547 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 272 reward tensor([-1]) loss 8.10319709777832 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 273 reward tensor([-1]) loss 7.415551662445068 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 274 reward tensor([-1]) loss 5.972191333770752 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 275 reward tensor([-1]) loss 4.556631565093994 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 276 reward tensor([-1]) loss 4.013692855834961 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 277 reward tensor([-1]) loss 3.4508187770843506 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 278 reward tensor([-1]) loss 2.5611040592193604 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 279 reward tensor([-1]) loss 1.9821200370788574 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 280 reward tensor([-1]) loss 1.6840323209762573 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 281 reward tensor([-1]) loss 1.6507471799850464 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 282 reward tensor([-1]) loss 1.5769240856170654 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 283 reward tensor([-1]) loss 1.6692092418670654 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 284 reward tensor([-1]) loss 1.5757875442504883 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 285 reward tensor([-1]) loss 1.7651466131210327 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 286 reward tensor([-1]) loss 1.524417519569397 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 287 reward tensor([-1]) loss 1.4577858448028564 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 288 reward tensor([-1]) loss 1.6286284923553467 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 289 reward tensor([-1]) loss 1.3616846799850464 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 290 reward tensor([-1]) loss 1.5011820793151855 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 291 reward tensor([-1]) loss 1.434622883796692 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 292 reward tensor([-1]) loss 1.4248511791229248 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 293 reward tensor([-1]) loss 0.999846875667572 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 294 reward tensor([-1]) loss 0.9127259254455566 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 295 reward tensor([-1]) loss 0.9017691016197205 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 296 reward tensor([-1]) loss 0.9679901003837585 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 297 reward tensor([-1]) loss 0.8310273289680481 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 298 reward tensor([-1]) loss 0.8584007024765015 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 299 reward tensor([-1]) loss 0.796245813369751 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 300 reward tensor([-1]) loss 0.6700544357299805 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 1 step 301 reward tensor([-1]) loss 8.058736801147461 epsilon 0.82
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 0 reward tensor([-1]) loss 27.101192474365234 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 1 reward tensor([-1]) loss 6.2791032791137695 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 2 reward tensor([-1]) loss 5.496129989624023 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 3 reward tensor([-1]) loss 3.8724894523620605 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 4 reward tensor([-1]) loss 3.346269369125366 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 5 reward tensor([-1]) loss 2.3131837844848633 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 6 reward tensor([-1]) loss 2.2157113552093506 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 7 reward tensor([-1]) loss 1.7109954357147217 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 8 reward tensor([-1]) loss 1.7565300464630127 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 9 reward tensor([-1]) loss 15.872923851013184 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 10 reward tensor([-1]) loss 1.922407865524292 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 11 reward tensor([-1]) loss 2.0180234909057617 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 12 reward tensor([-1]) loss 2.0481462478637695 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 13 reward tensor([-1]) loss 2.0614006519317627 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 14 reward tensor([-1]) loss 1.8389368057250977 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 15 reward tensor([-1]) loss 14.812076568603516 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 16 reward tensor([-1]) loss 2.033386707305908 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 17 reward tensor([-1]) loss 2.0021603107452393 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 18 reward tensor([-1]) loss 15.016897201538086 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 19 reward tensor([-1]) loss 1.718307614326477 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 20 reward tensor([-1]) loss 14.558055877685547 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 21 reward tensor([-1]) loss 14.48593521118164 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 22 reward tensor([-1]) loss 1.1597177982330322 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 23 reward tensor([-1]) loss 1.284212350845337 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 24 reward tensor([-1]) loss 1.1824629306793213 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 25 reward tensor([-1]) loss 0.9268844723701477 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 26 reward tensor([-1]) loss 14.039920806884766 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 27 reward tensor([-1]) loss 1.4976847171783447 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 28 reward tensor([-1]) loss 13.564725875854492 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 29 reward tensor([-1]) loss 18.06403350830078 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 30 reward tensor([-1]) loss 17.563947677612305 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 31 reward tensor([-1]) loss 5.009578227996826 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 32 reward tensor([-1]) loss 4.553902626037598 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 33 reward tensor([-1]) loss 4.003041744232178 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 34 reward tensor([-1]) loss 3.034336805343628 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 35 reward tensor([-1]) loss 12.830198287963867 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 36 reward tensor([-1]) loss 2.1269617080688477 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 37 reward tensor([-1]) loss 1.9084221124649048 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 38 reward tensor([-1]) loss 10.984634399414062 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 39 reward tensor([-1]) loss 2.6810433864593506 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 40 reward tensor([-1]) loss 10.699263572692871 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 41 reward tensor([-1]) loss 2.036180019378662 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 42 reward tensor([-1]) loss 10.872440338134766 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 43 reward tensor([-1]) loss 2.711240530014038 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 44 reward tensor([-1]) loss 3.0227532386779785 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 45 reward tensor([-1]) loss 2.7989513874053955 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 46 reward tensor([-1]) loss 1.7251086235046387 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 47 reward tensor([-1]) loss 10.120685577392578 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 48 reward tensor([-1]) loss 9.478958129882812 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 49 reward tensor([-1]) loss 2.7353930473327637 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 50 reward tensor([-1]) loss 2.3356895446777344 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 51 reward tensor([-1]) loss 10.144505500793457 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 52 reward tensor([-1]) loss 1.9166861772537231 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 53 reward tensor([-1]) loss 10.294425964355469 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 54 reward tensor([-1]) loss 1.3547253608703613 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 55 reward tensor([-1]) loss 10.183642387390137 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 56 reward tensor([-1]) loss 10.438876152038574 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 57 reward tensor([-1]) loss 1.277738332748413 epsilon 0.79335
27-Feb-25 11:45:50 - agent.DQN.DQN - INFO - episode 2 step 58 reward tensor([-1]) loss 1.423197627067566 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 59 reward tensor([-1]) loss 4.6655049324035645 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 60 reward tensor([-1]) loss 4.850788593292236 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 61 reward tensor([-1]) loss 13.924713134765625 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 62 reward tensor([-1]) loss 3.6914706230163574 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 63 reward tensor([-1]) loss 3.7380211353302 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 64 reward tensor([-1]) loss 3.3392465114593506 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 65 reward tensor([-1]) loss 2.49336838722229 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 66 reward tensor([-1]) loss 9.613097190856934 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 67 reward tensor([-1]) loss 9.039825439453125 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 68 reward tensor([-1]) loss 1.5927927494049072 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 69 reward tensor([-1]) loss 8.151040077209473 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 70 reward tensor([-1]) loss 8.146684646606445 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 71 reward tensor([-1]) loss 1.5656760931015015 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 72 reward tensor([-1]) loss 1.6403071880340576 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 73 reward tensor([-1]) loss 7.920867443084717 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 74 reward tensor([-1]) loss 1.814603567123413 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 75 reward tensor([-1]) loss 2.4705092906951904 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 76 reward tensor([-1]) loss 1.8060051202774048 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 77 reward tensor([-1]) loss 8.386591911315918 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 78 reward tensor([-1]) loss 1.4566032886505127 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 79 reward tensor([-1]) loss 0.8522403836250305 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 80 reward tensor([-1]) loss 6.509626865386963 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 81 reward tensor([-1]) loss 1.940083622932434 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 82 reward tensor([-1]) loss 1.797031044960022 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 83 reward tensor([-1]) loss 7.64256477355957 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 84 reward tensor([-1]) loss 7.1572089195251465 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 85 reward tensor([-1]) loss 1.644260048866272 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 86 reward tensor([-1]) loss 7.709373950958252 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 87 reward tensor([-1]) loss 0.9528400897979736 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 88 reward tensor([-1]) loss 0.9368460178375244 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 89 reward tensor([-1]) loss 3.9046432971954346 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 90 reward tensor([-1]) loss 10.524075508117676 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 91 reward tensor([-1]) loss 5.227912425994873 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 92 reward tensor([-1]) loss 4.005725383758545 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 93 reward tensor([-1]) loss 3.336277723312378 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 94 reward tensor([-1]) loss 3.383107900619507 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 95 reward tensor([-1]) loss 2.758993625640869 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 96 reward tensor([-1]) loss 1.7444590330123901 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 97 reward tensor([-1]) loss 2.114433765411377 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 98 reward tensor([-1]) loss 1.7608963251113892 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 99 reward tensor([-1]) loss 1.0994282960891724 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 100 reward tensor([-1]) loss 1.4211437702178955 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 101 reward tensor([-1]) loss 1.303024172782898 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 102 reward tensor([-1]) loss 7.169092655181885 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 103 reward tensor([-1]) loss 1.0835380554199219 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 104 reward tensor([-1]) loss 6.904101848602295 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 105 reward tensor([-1]) loss 7.016050338745117 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 106 reward tensor([-1]) loss 1.7425495386123657 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 107 reward tensor([-1]) loss 1.706904649734497 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 108 reward tensor([-1]) loss 6.795231819152832 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 109 reward tensor([-1]) loss 1.8764288425445557 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 110 reward tensor([-1]) loss 1.0977141857147217 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 111 reward tensor([-1]) loss 6.584216594696045 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 112 reward tensor([-1]) loss 1.2624082565307617 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 113 reward tensor([-1]) loss 5.9914116859436035 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 114 reward tensor([-1]) loss 0.9712130427360535 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 115 reward tensor([-1]) loss 6.506913661956787 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 116 reward tensor([-1]) loss 1.3476026058197021 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 117 reward tensor([-1]) loss 1.1699802875518799 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 118 reward tensor([-1]) loss 1.444419264793396 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 119 reward tensor([-1]) loss 8.87935733795166 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 120 reward tensor([-1]) loss 3.642568826675415 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 121 reward tensor([-1]) loss 8.286835670471191 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 122 reward tensor([-1]) loss 2.8906705379486084 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 123 reward tensor([-1]) loss 7.495945453643799 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 124 reward tensor([-1]) loss 2.550602436065674 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 125 reward tensor([-1]) loss 6.128248691558838 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 126 reward tensor([-1]) loss 6.009214401245117 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 127 reward tensor([-1]) loss 1.950096607208252 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 128 reward tensor([-1]) loss 5.191269874572754 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 129 reward tensor([-1]) loss 5.641139507293701 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 130 reward tensor([-1]) loss 1.552966594696045 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 131 reward tensor([-1]) loss 4.885024547576904 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 132 reward tensor([-1]) loss 1.6175153255462646 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 133 reward tensor([-1]) loss 5.650232315063477 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 134 reward tensor([-1]) loss 3.2471911907196045 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 135 reward tensor([-1]) loss 4.537761688232422 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 136 reward tensor([-1]) loss 1.483324408531189 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 137 reward tensor([-1]) loss 2.17668080329895 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 138 reward tensor([-1]) loss 0.8114899396896362 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 139 reward tensor([-1]) loss 4.271061897277832 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 140 reward tensor([-1]) loss 4.839676856994629 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 141 reward tensor([-1]) loss 5.460999488830566 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 142 reward tensor([-1]) loss 4.377805233001709 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 143 reward tensor([-1]) loss 1.3095096349716187 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 144 reward tensor([-1]) loss 4.364110469818115 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 145 reward tensor([-1]) loss 1.8059227466583252 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 146 reward tensor([-1]) loss 1.959291696548462 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 147 reward tensor([-1]) loss 1.6235973834991455 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 148 reward tensor([-1]) loss 0.6166170835494995 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 149 reward tensor([-1]) loss 3.248328685760498 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 150 reward tensor([-1]) loss 2.445363759994507 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 151 reward tensor([-1]) loss 2.6744894981384277 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 152 reward tensor([-1]) loss 2.5279576778411865 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 153 reward tensor([-1]) loss 2.1418471336364746 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 154 reward tensor([-1]) loss 1.6645718812942505 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 155 reward tensor([-1]) loss 1.6576509475708008 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 156 reward tensor([-1]) loss 1.437244176864624 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 157 reward tensor([-1]) loss 5.183018207550049 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 158 reward tensor([-1]) loss 0.8792549967765808 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 159 reward tensor([-1]) loss 0.9109395146369934 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 160 reward tensor([-1]) loss 0.943812906742096 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 161 reward tensor([-1]) loss 4.8820390701293945 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 162 reward tensor([-1]) loss 0.9373389482498169 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 163 reward tensor([-1]) loss 1.1510570049285889 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 164 reward tensor([-1]) loss 4.517780780792236 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 165 reward tensor([-1]) loss 1.1754930019378662 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 166 reward tensor([-1]) loss 1.1460750102996826 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 167 reward tensor([-1]) loss 1.522291898727417 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 168 reward tensor([-1]) loss 4.723752975463867 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 169 reward tensor([-1]) loss 4.268352508544922 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 170 reward tensor([-1]) loss 0.9166971445083618 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 171 reward tensor([-1]) loss 4.827796459197998 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 172 reward tensor([-1]) loss 4.120185375213623 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 173 reward tensor([-1]) loss 4.110391616821289 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 174 reward tensor([-1]) loss 0.830804705619812 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 175 reward tensor([-1]) loss 0.7337432503700256 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 176 reward tensor([-1]) loss 1.6323944330215454 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 177 reward tensor([-1]) loss 4.250807285308838 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 178 reward tensor([-1]) loss 1.6272879838943481 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 179 reward tensor([-1]) loss 2.203535556793213 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 180 reward tensor([-1]) loss 1.9978350400924683 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 181 reward tensor([-1]) loss 1.9222795963287354 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 182 reward tensor([-1]) loss 2.035094738006592 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 183 reward tensor([-1]) loss 1.4031397104263306 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 184 reward tensor([-1]) loss 1.8158743381500244 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 185 reward tensor([-1]) loss 1.8484556674957275 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 186 reward tensor([-1]) loss 1.7649800777435303 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 187 reward tensor([-1]) loss 1.6931917667388916 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 188 reward tensor([-1]) loss 4.018953323364258 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 189 reward tensor([-1]) loss 1.2559313774108887 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 190 reward tensor([-1]) loss 3.828516721725464 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 191 reward tensor([-1]) loss 0.9103739261627197 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 192 reward tensor([-1]) loss 3.775855779647827 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 193 reward tensor([-1]) loss 1.3777999877929688 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 194 reward tensor([-1]) loss 3.672731637954712 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 195 reward tensor([-1]) loss 1.1486024856567383 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 196 reward tensor([-1]) loss 1.041575312614441 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 197 reward tensor([-1]) loss 0.8297091126441956 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 198 reward tensor([-1]) loss 0.9296526312828064 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 199 reward tensor([-1]) loss 4.006230354309082 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 200 reward tensor([-1]) loss 1.0116424560546875 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 201 reward tensor([-1]) loss 1.8532315492630005 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 202 reward tensor([-1]) loss 1.409686803817749 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 203 reward tensor([-1]) loss 3.4190046787261963 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 204 reward tensor([-1]) loss 0.8074705600738525 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 205 reward tensor([-1]) loss 3.5426032543182373 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 206 reward tensor([-1]) loss 1.0293617248535156 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 207 reward tensor([-1]) loss 3.595856189727783 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 208 reward tensor([-1]) loss 3.502565622329712 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 209 reward tensor([-1]) loss 1.6828604936599731 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 210 reward tensor([-1]) loss 1.6995596885681152 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 211 reward tensor([-1]) loss 1.7313474416732788 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 212 reward tensor([-1]) loss 1.3702163696289062 epsilon 0.79335
27-Feb-25 11:45:51 - agent.DQN.DQN - INFO - episode 2 step 213 reward tensor([-1]) loss 1.6565440893173218 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 214 reward tensor([-1]) loss 4.343929767608643 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 215 reward tensor([-1]) loss 1.2130730152130127 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 216 reward tensor([-1]) loss 1.1550827026367188 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 217 reward tensor([-1]) loss 1.015623688697815 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 218 reward tensor([-1]) loss 3.3703765869140625 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 219 reward tensor([-1]) loss 1.2215232849121094 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 220 reward tensor([-1]) loss 0.9457178711891174 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 221 reward tensor([-1]) loss 1.2833737134933472 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 222 reward tensor([-1]) loss 1.180111289024353 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 223 reward tensor([-1]) loss 2.7189548015594482 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 224 reward tensor([-1]) loss 1.1352955102920532 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 225 reward tensor([-1]) loss 0.6285795569419861 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 226 reward tensor([-1]) loss 1.2291980981826782 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 227 reward tensor([-1]) loss 0.9701667428016663 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 228 reward tensor([-1]) loss 0.8536730408668518 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 229 reward tensor([-1]) loss 3.00868558883667 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 230 reward tensor([-1]) loss 0.7735209465026855 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 231 reward tensor([-1]) loss 0.4456805884838104 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 232 reward tensor([-1]) loss 0.9404651522636414 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 233 reward tensor([-1]) loss 0.5853586792945862 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 234 reward tensor([-1]) loss 0.7190365791320801 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 235 reward tensor([-1]) loss 0.44343623518943787 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 236 reward tensor([-1]) loss 0.45117616653442383 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 237 reward tensor([-1]) loss 3.306765556335449 epsilon 0.79335
27-Feb-25 11:45:52 - agent.DQN.DQN - INFO - episode 2 step 238 reward tensor([-1]) loss 3.2978248596191406 epsilon 0.79335
